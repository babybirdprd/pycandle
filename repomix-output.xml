This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/parity-check.yml
.gitignore
.repomixignore
Cargo.toml
crates/pycandle-audio/Cargo.toml
crates/pycandle-audio/src/lib.rs
crates/pycandle-core/Cargo.toml
crates/pycandle-core/src/checker.rs
crates/pycandle-core/src/codegen/gpt2.rs
crates/pycandle-core/src/codegen/mod.rs
crates/pycandle-core/src/gpt2.rs
crates/pycandle-core/src/layers.rs
crates/pycandle-core/src/lib.rs
crates/pycandle-core/src/samplers.rs
crates/pycandle-core/src/weights.rs
crates/pycandle-core/tests/dummy_test.rs
crates/pycandle-core/tests/quantization_drift.rs
crates/pycandle-core/verification_results.jsonl
crates/pycandle/Cargo.toml
crates/pycandle/src/dashboard.rs
crates/pycandle/src/init.rs
crates/pycandle/src/main.rs
crates/pycandle/src/report.rs
crates/pycandle/src/test_gen.rs
crates/pycandle/src/todos.rs
docs/meta_trace_guide.md
docs/PORTING_SOP.md
dummy_manifest.json
gen_dummy_results.rs
generated_hints.rs
generated_symbolic.rs
py/.python-version
py/dag_model.py
py/generated_complex.rs
py/generated_residual.rs
py/hello.py
py/pyproject.toml
py/spy.py
py/test_hints.py
py/test_model.py
py/weight_extractor.py
README.md
recorder.py
ref1.txt
ref2.txt
ref3.txt
specs/melspectrogram_parity.md
test_symbolic.py
test_trace_hints/hints_test_manifest.json
test_trace/symbolic_test_manifest.json
tests/parity.rs
tests/quantization_drift.rs
tts_turbo_temp.py
verification_results.jsonl
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="crates/pycandle-core/tests/quantization_drift.rs">
use candle_core::{Device, Tensor};
use pycandle_core::{LayerMeta, PyChecker, VerificationMode};
use std::collections::HashMap;

#[test]
fn test_quantization_drift() -> anyhow::Result<()> {
    let device = Device::Cpu;

    // 1. Create a dummy manifest
    let manifest_json = r#"{
        "layer_perfect": {
            "name": "layer_perfect", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_small": {
            "name": "layer_drift_small", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_large": {
            "name": "layer_drift_large", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        }
    }"#;

    // Write dummy files for PyChecker::load
    // Use a unique subdir to avoid conflicts
    let trace_dir = "test_trace_drift_core";
    std::fs::create_dir_all(trace_dir)?;
    std::fs::write(
        format!("{}/drift_run_manifest.json", trace_dir),
        manifest_json,
    )?;

    // Create dummy golden tensors
    let t_perfect = Tensor::new(&[1.0f32, 2.0, 3.0], &device)?;
    let t_drift_small = Tensor::new(&[10.0f32, 20.0, 30.0], &device)?;
    let t_drift_large = Tensor::new(&[100.0f32, 200.0, 300.0], &device)?;

    let tensors_map: HashMap<String, Tensor> = HashMap::from([
        ("layer_perfect.out.0".to_string(), t_perfect.clone()),
        ("layer_drift_small.out.0".to_string(), t_drift_small.clone()),
        ("layer_drift_large.out.0".to_string(), t_drift_large.clone()),
    ]);
    candle_core::safetensors::save(
        &tensors_map,
        format!("{}/drift_run_trace.safetensors", trace_dir),
    )?;

    // 2. Load Checker in DriftTracking mode
    // Note: PyChecker::load takes project_name and base_path
    let checker = PyChecker::load("drift_run", trace_dir, &device)?
        .with_mode(VerificationMode::DriftTracking);

    println!("Checking in mode: {:?}", checker.mode);

    // 3. Verify Layers

    // A) Perfect Match
    let res1 = checker.verify("layer_perfect", &t_perfect)?;
    assert!(res1.mse < 1e-6);

    // B) Small Drift (MSE = 1e-6, barely passing strict if atol=1e-4, but let's make it small enough)
    // Let's add noise: 1e-3. 1e-3^2 = 1e-6.
    let noise_small = Tensor::new(&[0.001f32, 0.001, 0.001], &device)?;
    let t_small_noisy = (&t_drift_small + noise_small)?;
    let res2 = checker.verify("layer_drift_small", &t_small_noisy)?;
    println!("Layer Small Drift MSE: {}", res2.mse);

    // C) Large Drift (MSE ~ 0.01) - Should fail in Strict, but pass here
    // Noise 0.1. 0.1^2 = 0.01. > 1e-4.
    let noise_large = Tensor::new(&[0.1f32, 0.1, 0.1], &device)?;
    let t_large_noisy = (&t_drift_large + noise_large)?;

    // This should NOT panic or return Err, unlike Strict mode
    let res3 = checker.verify("layer_drift_large", &t_large_noisy)?;
    println!("Layer Large Drift MSE: {}", res3.mse);

    assert!(
        res3.mse > checker.atol,
        "Expected large drift to exceed atol"
    );

    // 4. Print Report (Manual verify on console output)
    checker.print_drift_report();

    // Cleanup
    std::fs::remove_dir_all(trace_dir)?;

    Ok(())
}
</file>

<file path="crates/pycandle-core/verification_results.jsonl">
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
</file>

<file path="tests/quantization_drift.rs">
use candle_core::{Device, Tensor};
use pycandle_core::checker::LayerMeta; // Needed for mock manifest
use pycandle_core::{PyChecker, VerificationMode};
use std::collections::HashMap;

#[test]
fn test_quantization_drift() -> anyhow::Result<()> {
    let device = Device::Cpu;

    // 1. Create a dummy manifest
    let manifest_json = r#"{
        "layer_perfect": {
            "name": "layer_perfect", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_small": {
            "name": "layer_drift_small", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_large": {
            "name": "layer_drift_large", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        }
    }"#;

    // Write dummy files for PyChecker::load
    std::fs::create_dir_all("test_trace_drift")?;
    std::fs::write("test_trace_drift/drift_run_manifest.json", manifest_json)?;

    // Create dummy golden tensors
    let t_perfect = Tensor::new(&[1.0f32, 2.0, 3.0], &device)?;
    let t_drift_small = Tensor::new(&[10.0f32, 20.0, 30.0], &device)?;
    let t_drift_large = Tensor::new(&[100.0f32, 200.0, 300.0], &device)?;

    let tensors_map: HashMap<String, Tensor> = HashMap::from([
        ("layer_perfect.out.0".to_string(), t_perfect.clone()),
        ("layer_drift_small.out.0".to_string(), t_drift_small.clone()),
        ("layer_drift_large.out.0".to_string(), t_drift_large.clone()),
    ]);
    candle_core::safetensors::save(&tensors_map, "test_trace_drift/drift_run_trace.safetensors")?;

    // 2. Load Checker in DriftTracking mode
    let checker = PyChecker::load("drift_run", "test_trace_drift", &device)?
        .with_mode(VerificationMode::DriftTracking);

    println!("Checking in mode: {:?}", checker.mode);

    // 3. Verify Layers

    // A) Perfect Match
    let res1 = checker.verify("layer_perfect", &t_perfect)?;
    assert!(res1.mse < 1e-6);

    // B) Small Drift (MSE = 1e-6, barely passing strict if atol=1e-4, but let's make it small enough)
    // Let's add noise: 1e-3. 1e-3^2 = 1e-6.
    let noise_small = Tensor::new(&[0.001f32, 0.001, 0.001], &device)?;
    let t_small_noisy = (&t_drift_small + noise_small)?;
    let res2 = checker.verify("layer_drift_small", &t_small_noisy)?;
    println!("Layer Small Drift MSE: {}", res2.mse);

    // C) Large Drift (MSE ~ 0.01) - Should fail in Strict, but pass here
    // Noise 0.1. 0.1^2 = 0.01. > 1e-4.
    let noise_large = Tensor::new(&[0.1f32, 0.1, 0.1], &device)?;
    let t_large_noisy = (&t_drift_large + noise_large)?;

    // This should NOT panic or return Err, unlike Strict mode
    let res3 = checker.verify("layer_drift_large", &t_large_noisy)?;
    println!("Layer Large Drift MSE: {}", res3.mse);

    assert!(
        res3.mse > checker.atol,
        "Expected large drift to exceed atol"
    );

    // 4. Print Report (Manual verify on console output)
    checker.print_drift_report();

    // Cleanup
    std::fs::remove_dir_all("test_trace_drift")?;

    Ok(())
}
</file>

<file path=".github/workflows/parity-check.yml">
name: Parity Check

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  CARGO_TERM_COLOR: always

jobs:
  parity-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    # 1. Setup Rust
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    # 2. Setup Python & UV
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        python-version: '3.12'

    - name: Install Python dependencies
      run: uv pip install torch pycandle-core

    # 3. Compile PyCandle CLI (or use pre-built if available)
    # For now, we assume we build it from source in the workspace
    - name: Build PyCandle CLI
      run: cargo build --release --bin pycandle

    # 4. Generate Traces (Simulating 'pycandle record')
    # Assuming 'recorder.py' is in the root as per SOP
    - name: Generate Traces
      run: uv run recorder.py
      env:
        # Ensure we can find local pycandle if not installed
        PYTHONPATH: ${{ github.workspace }}/py

    # 5. Generate Code (Optional if committed, but good to verify codegen runs)
    # If the user committed generated code, this step might be skipped or used to check for drift
    - name: Run Codegen (Verify)
      run: |
        ./target/release/pycandle codegen \
          --manifest traces/debug_run_manifest.json \
          --out generated_model.rs \
          --model MyModel \
          --analyze-only

    # 6. Run Parity Tests
    - name: Run Parity Tests
      run: cargo test --test parity
</file>

<file path=".repomixignore">
chatterbox-repo/*
</file>

<file path="crates/pycandle-core/src/gpt2.rs">
use candle_core::{IndexOp, Result, Tensor};
use candle_nn::{Embedding, LayerNorm, Linear, Module, VarBuilder};

#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Config {
    pub vocab_size: usize,
    pub context_length: usize,
    pub emb_dim: usize,
    pub n_heads: usize,
    pub n_layers: usize,
    pub drop_rate: f32,
    pub qkv_bias: bool,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            vocab_size: 50257,
            context_length: 1024,
            emb_dim: 768,
            n_heads: 12,
            n_layers: 12,
            drop_rate: 0.1,
            qkv_bias: true, // Standard GPT2 has bias
        }
    }
}

// ============================================================================
// KV Cache
// ============================================================================

#[derive(Debug, Clone)]
pub struct KVCache {
    pub k: Tensor, // (B, n_head, T, head_dim)
    pub v: Tensor, // (B, n_head, T, head_dim)
}

impl KVCache {
    pub fn new() -> Self {
        // Placeholder, usually initialized during first forward pass
        // or we use Option<KVCache>
        unimplemented!("Use Option<KVCache> for now")
    }
}

// ============================================================================
// Layers
// ============================================================================

// Conv1D in HuggingFace is actually a Linear layer with (nx, nf) weight shape
// so transposing is needed if loading from standard HF checkpoints.
// However, standard Candle `linear` expects (out, in).
// HF Conv1D weight is (in, out).
fn conv1d(in_f: usize, out_f: usize, vb: VarBuilder) -> Result<Linear> {
    let weight = vb.get((in_f, out_f), "weight")?.t()?;
    let bias = vb.get((out_f,), "bias")?;
    Ok(Linear::new(weight, Some(bias)))
}

pub struct MultiHeadAttention {
    c_attn: Linear,
    c_proj: Linear,
    n_head: usize,
    head_dim: usize,
    bias: Tensor,
    scale: f64,
}

impl MultiHeadAttention {
    pub fn new(n_embd: usize, n_head: usize, vb: VarBuilder) -> Result<Self> {
        let c_attn = conv1d(n_embd, 3 * n_embd, vb.pp("c_attn"))?;
        let c_proj = conv1d(n_embd, n_embd, vb.pp("c_proj"))?;
        let head_dim = n_embd / n_head;
        let scale = 1.0 / (head_dim as f64).sqrt();

        // Causal mask buffer
        let mask: Vec<_> = (0..1024)
            .flat_map(|i| (0..1024).map(move |j| if j <= i { 1.0f32 } else { 0.0f32 }))
            .collect();
        let bias = Tensor::from_vec(mask, (1, 1, 1024, 1024), vb.device())?;

        Ok(Self {
            c_attn,
            c_proj,
            n_head,
            head_dim,
            bias,
            scale,
        })
    }

    pub fn forward(&self, x: &Tensor, layer_cache: Option<&mut Option<KVCache>>) -> Result<Tensor> {
        let (b_sz, t, c) = x.dims3()?;
        let qkv = self.c_attn.forward(x)?;

        // (B, T, 3 * n_embd) -> (B, T, 3, n_head, head_dim)
        let qkv = qkv.reshape((b_sz, t, 3, self.n_head, self.head_dim))?;
        // (3, B, n_head, T, head_dim)
        let qkv = qkv.permute((2, 0, 3, 1, 4))?;

        let q = qkv.i(0)?;
        let k = qkv.i(1)?;
        let v = qkv.i(2)?;

        // KV Cache handling
        let (k, v) = if let Some(cache_opt) = layer_cache {
            if let Some(past_kv) = cache_opt.take() {
                let k = Tensor::cat(&[&past_kv.k, &k], 2)?;
                let v = Tensor::cat(&[&past_kv.v, &v], 2)?;
                *cache_opt = Some(KVCache {
                    k: k.clone(),
                    v: v.clone(),
                });
                (k, v)
            } else {
                *cache_opt = Some(KVCache {
                    k: k.clone(),
                    v: v.clone(),
                });
                (k, v)
            }
        } else {
            (k, v)
        };

        // Standard attention
        let t_total = k.dim(2)?;
        // q: (B, H, T_q, D) @ k.t: (B, H, D, T_k) -> (B, H, T_q, T_k)
        let att = (q.matmul(&k.t()?)? * self.scale)?;

        // Causal masking
        let mask = self.bias.narrow(2, 0, t)?; // Slice to current query len
        let mask = mask.narrow(3, 0, t_total)?; // Slice to total key len

        // In inference with cache, we only generate 1 token usually, but causal mask expects full sequence
        // Actually, if we are generating 1 token at pos P, we attend to all previous P tokens.
        // The mask handles the tril logic.
        // For cached inference (q len 1, k len T), we want full attention to previous.
        // The bias implementation above is static (1024,1024).
        // We should just direct mask if t > 1.
        // If t == 1 (generation), we attend to everything, no lower-triangular needed for the single row.

        let att = if t > 1 {
            let infinite =
                Tensor::new(f32::NEG_INFINITY, att.device())?.broadcast_as(att.shape())?;
            mask.eq(0.0)?.where_cond(&infinite, &att)?
        } else {
            att // Attend to all past
        };

        let att = candle_nn::ops::softmax(&att, 3)?;

        // (B, H, T_q, T_k) @ (B, H, T_k, D) -> (B, H, T_q, D)
        let y = att.matmul(&v)?;
        // (B, T_q, H, D) -> (B, T_q, C)
        let y = y.permute((0, 2, 1, 3))?.reshape((b_sz, t, c))?;

        self.c_proj.forward(&y)
    }
}

pub struct FeedForward {
    c_fc: Linear,
    c_proj: Linear,
    act: candle_nn::Activation,
}

impl FeedForward {
    pub fn new(n_embd: usize, vb: VarBuilder) -> Result<Self> {
        let c_fc = conv1d(n_embd, 4 * n_embd, vb.pp("c_fc"))?;
        let c_proj = conv1d(4 * n_embd, n_embd, vb.pp("c_proj"))?;
        Ok(Self {
            c_fc,
            c_proj,
            act: candle_nn::Activation::Gelu,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.c_fc.forward(x)?;
        let x = self.act.forward(&x)?;
        self.c_proj.forward(&x)
    }
}

pub struct TransformerBlock {
    ln_1: LayerNorm,
    attn: MultiHeadAttention,
    ln_2: LayerNorm,
    mlp: FeedForward,
}

impl TransformerBlock {
    pub fn new(n_embd: usize, n_head: usize, vb: VarBuilder) -> Result<Self> {
        let ln_1 = candle_nn::layer_norm(n_embd, 1e-5, vb.pp("ln_1"))?;
        let attn = MultiHeadAttention::new(n_embd, n_head, vb.pp("attn"))?;
        let ln_2 = candle_nn::layer_norm(n_embd, 1e-5, vb.pp("ln_2"))?;
        let mlp = FeedForward::new(n_embd, vb.pp("mlp"))?;
        Ok(Self {
            ln_1,
            attn,
            ln_2,
            mlp,
        })
    }

    pub fn forward(&self, x: &Tensor, layer_cache: Option<&mut Option<KVCache>>) -> Result<Tensor> {
        let residual = x;
        let x = self.ln_1.forward(x)?;
        let x = self.attn.forward(&x, layer_cache)?;
        let x = (x + residual)?;

        let residual = &x;
        let x = self.ln_2.forward(&x)?;
        let x = self.mlp.forward(&x)?;
        let x = (x + residual)?;
        Ok(x)
    }
}

pub struct GPTModel {
    wte: Embedding,
    wpe: Embedding,
    h: Vec<TransformerBlock>,
    ln_f: LayerNorm,
}

impl GPTModel {
    pub fn new(cfg: Config, vb: VarBuilder) -> Result<Self> {
        let wte = candle_nn::embedding(cfg.vocab_size, cfg.emb_dim, vb.pp("wte"))?;
        let wpe = candle_nn::embedding(cfg.context_length, cfg.emb_dim, vb.pp("wpe"))?;

        let mut h = Vec::new();
        for i in 0..cfg.n_layers {
            h.push(TransformerBlock::new(
                cfg.emb_dim,
                cfg.n_heads,
                vb.pp(format!("h.{}", i)),
            )?);
        }

        let ln_f = candle_nn::layer_norm(cfg.emb_dim, 1e-5, vb.pp("ln_f"))?;

        Ok(Self { wte, wpe, h, ln_f })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        self.forward_kv(x, None)
    }

    pub fn forward_kv(
        &self,
        x: &Tensor,
        mut kv_cache: Option<&mut Vec<Option<KVCache>>>,
    ) -> Result<Tensor> {
        let (_b, t) = x.dims2()?;

        let offset = if let Some(cache) = &kv_cache {
            if let Some(Some(prev)) = cache.first() {
                prev.k.dim(2)?
            } else {
                0
            }
        } else {
            0
        };

        let pos = Tensor::arange(offset as u32, (offset + t) as u32, x.device())?;

        let tok_emb = self.wte.forward(x)?;
        let pos_emb = self.wpe.forward(&pos)?;

        let mut x = (tok_emb + pos_emb)?;

        for (i, block) in self.h.iter().enumerate() {
            let layer_cache = if let Some(cache) = &mut kv_cache {
                if cache.len() <= i {
                    cache.resize_with(i + 1, || None);
                }
                cache.get_mut(i)
            } else {
                None
            };
            x = block.forward(&x, layer_cache)?;
        }

        self.ln_f.forward(&x)
    }
}
</file>

<file path="crates/pycandle-core/src/samplers.rs">
use candle_core::{Result, Tensor};

pub trait LogitsProcessor {
    fn apply(&self, logits: &Tensor) -> Result<Tensor>;
}

/// Repetition Penalty
/// Reference: https://arxiv.org/pdf/1909.05858.pdf
pub struct RepetitionPenalty {
    pub penalty: f64,
    pub context: Vec<u32>,
}

impl RepetitionPenalty {
    pub fn new(penalty: f64, context: Vec<u32>) -> Self {
        Self { penalty, context }
    }
}

impl LogitsProcessor for RepetitionPenalty {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.penalty <= 1.0 || self.context.is_empty() {
            return Ok(logits.clone());
        }

        let mut logits_vec = logits.squeeze(0)?.to_vec1::<f32>()?;

        for &token in &self.context {
            if (token as usize) < logits_vec.len() {
                let logit = logits_vec[token as usize];
                if logit < 0.0 {
                    logits_vec[token as usize] = logit * self.penalty as f32;
                } else {
                    logits_vec[token as usize] = logit / self.penalty as f32;
                }
            }
        }

        Tensor::from_vec(logits_vec, (1, logits.dim(1)?), logits.device())
    }
}

/// Temperature scaling
pub struct Temperature {
    pub temperature: f64,
}

impl LogitsProcessor for Temperature {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.temperature <= 0.0 {
            // Greedy sampling handled by caller usually, but here we can just return
            return Ok(logits.clone());
        }
        logits / self.temperature
    }
}

/// Top-P (Nucleus) Sampling
pub struct TopP {
    pub p: f64,
}

impl LogitsProcessor for TopP {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.p >= 1.0 {
            return Ok(logits.clone());
        }

        let probs = candle_nn::ops::softmax(logits, 1)?;
        let probs_vec = probs.squeeze(0)?.to_vec1::<f32>()?;

        // Sort descending, keep indices
        let mut indices: Vec<usize> = (0..probs_vec.len()).collect();
        indices.sort_by(|&a, &b| probs_vec[b].partial_cmp(&probs_vec[a]).unwrap());

        let mut cum_sum = 0.0;
        let mut cutoff_index = indices.len() - 1;

        for (i, &idx) in indices.iter().enumerate() {
            cum_sum += probs_vec[idx];
            if cum_sum > self.p as f32 {
                cutoff_index = i;
                break;
            }
        }

        // Everything after cutoff gets -inf
        let mut new_logits = logits.squeeze(0)?.to_vec1::<f32>()?;
        let neg_inf = f32::NEG_INFINITY;

        // Mask out the tail
        for &idx in &indices[cutoff_index + 1..] {
            new_logits[idx] = neg_inf;
        }

        Tensor::from_vec(new_logits, (1, logits.dim(1)?), logits.device())
    }
}

/// Min-P Sampling (Alternative to Top-P)
/// Reference: https://github.com/huggingface/transformers/issues/27670
pub struct MinP {
    pub p: f64,
}

impl LogitsProcessor for MinP {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.p <= 0.0 {
            return Ok(logits.clone());
        }

        let probs = candle_nn::ops::softmax(logits, 1)?;
        let max_prob = probs.max_keepdim(1)?.squeeze(0)?.to_vec1::<f32>()?[0];
        let scaled_min = max_prob * self.p as f32;

        let probs_vec = probs.squeeze(0)?.to_vec1::<f32>()?;
        let mut new_logits = logits.squeeze(0)?.to_vec1::<f32>()?;
        let neg_inf = f32::NEG_INFINITY;

        for (i, &prob) in probs_vec.iter().enumerate() {
            if prob < scaled_min {
                new_logits[i] = neg_inf;
            }
        }

        Tensor::from_vec(new_logits, (1, logits.dim(1)?), logits.device())
    }
}
</file>

<file path="crates/pycandle-core/src/weights.rs">
use anyhow::{Context, Result};
use regex::Regex;
use std::collections::{HashMap, HashSet};

/// A renaming engine for tensor keys using Regex patterns.
pub struct WeightMapper {
    mappings: Vec<(Regex, String)>,
}

impl WeightMapper {
    /// Create a mapper from a JSON string containing pattern -> replacement mappings.
    pub fn from_json(json: &str) -> Result<Self> {
        let raw: HashMap<String, String> = serde_json::from_str(json)?;
        let mut mappings = Vec::new();
        // Sort keys to ensure deterministic order (important for overlapping regexes)
        let mut keys: Vec<_> = raw.keys().collect();
        keys.sort();

        for pattern in keys {
            let replacement = raw.get(pattern).unwrap();
            let re = Regex::new(pattern)
                .with_context(|| format!("Invalid regex pattern: {}", pattern))?;
            mappings.push((re, replacement.clone()));
        }
        Ok(Self { mappings })
    }

    /// Map a key using all registered patterns.
    pub fn map_key(&self, key: &str) -> String {
        let mut current = key.to_string();
        for (re, replacement) in &self.mappings {
            current = re.replace_all(&current, replacement.as_str()).to_string();
        }
        current
    }
}

/// Identifies which weights are actually used in a model based on its manifest.
pub struct WeightExtractor {
    active_params: HashSet<String>,
}

impl WeightExtractor {
    /// Create an extractor from a manifest JSON.
    pub fn from_manifest(manifest_json: &str) -> Result<Self> {
        let manifest: HashMap<String, serde_json::Value> = serde_json::from_str(manifest_json)?;
        let mut active_params = HashSet::new();

        for (module_name, meta) in manifest {
            if module_name.starts_with('_') {
                continue;
            }

            if let Some(params) = meta.get("parameters").and_then(|p| p.as_array()) {
                for p in params {
                    if let Some(p_name) = p.as_str() {
                        // PyTorch module parameters are accessed as module_name.weight, etc.
                        active_params.insert(format!("{}.{}", module_name, p_name));
                    }
                }
            }
        }

        Ok(Self { active_params })
    }

    /// Check if a given tensor key is used in the manifest.
    pub fn should_keep(&self, key: &str) -> bool {
        self.active_params.contains(key)
    }

    /// Return the set of all active parameter keys.
    pub fn active_keys(&self) -> &HashSet<String> {
        &self.active_params
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_mapper_basic() {
        let json = r#"{"encoder\\.layers\\.(\\d+)\\.": "h.$1."}"#;
        let mapper = WeightMapper::from_json(json).unwrap();
        assert_eq!(mapper.map_key("encoder.layers.0.weight"), "h.0.weight");
        assert_eq!(mapper.map_key("encoder.layers.15.bias"), "h.15.bias");
    }

    #[test]
    fn test_extractor_basic() {
        let manifest = r#"{
            "encoder.layers.0": {
                "parameters": ["weight", "bias"]
            },
            "decoder": {
                "parameters": ["weight"]
            },
            "_internal": { "parameters": ["unused"] }
        }"#;
        let extractor = WeightExtractor::from_manifest(manifest).unwrap();
        assert!(extractor.should_keep("encoder.layers.0.weight"));
        assert!(extractor.should_keep("encoder.layers.0.bias"));
        assert!(extractor.should_keep("decoder.weight"));
        assert!(!extractor.should_keep("encoder.layers.0.other"));
        assert!(!extractor.should_keep("_internal.unused"));
    }
}
</file>

<file path="crates/pycandle-core/tests/dummy_test.rs">
#[cfg(test)]
mod tests {
    use candle_core::{Device, Tensor};
    use pycandle_core::{ComparisonResult, PyChecker};

    // Mock PyChecker just to access log_result (which we can't easily do without full setup)
    // Actually, we can just use PyChecker if we mock the files, but easier to just implement a similar test
    // that writes to the same file to verify the dashboard picks it up.

    // Wait, let's try to use the actual PyChecker if possible.
    // We need a dummy manifest and safetensors.
    // Instead, I'll just write a test that prints the expected output format for the dashboard to parse.

    #[test]
    fn test_layer_1_pass() {
        println!("test_layer_1_pass ... ok");
    }

    #[test]
    fn test_layer_2_fail() {
        // Dashboard should pick this up as failure
        assert!(false, "Simulated failure");
    }

    #[test]
    fn test_layer_3_pass() {
        std::thread::sleep(std::time::Duration::from_millis(500));
        println!("test_layer_3_pass ... ok");
    }
}
</file>

<file path="crates/pycandle/src/dashboard.rs">
use anyhow::Result;
use crossterm::{
    event::{self, Event, KeyCode, KeyEventKind},
    execute,
    terminal::{EnterAlternateScreen, LeaveAlternateScreen, disable_raw_mode, enable_raw_mode},
};
use pycandle_core::ComparisonResult;
use ratatui::{
    prelude::*,
    widgets::{
        Block, Borders, List, ListItem, Paragraph,
        canvas::{Canvas, Rectangle},
    },
};
use std::collections::HashMap;
use std::io::{BufRead, BufReader};
use std::process::{Command, Stdio};
use std::sync::mpsc;
use std::thread;
use std::time::Duration;

pub enum DashboardEvent {
    Input(event::KeyEvent),
    TestLine(String),
    Tick,
}

struct App {
    test_results: Vec<TestResult>,
    parity_results: HashMap<String, ComparisonResult>,
    output_log: Vec<String>,
    running: bool,
    scroll: usize,
    passed: usize,
    failed: usize,
    total: usize,
}

struct TestResult {
    name: String,
    status: TestStatus,
    details: String,
}

enum TestStatus {
    Running,
    Passed,
    Failed,
}

impl App {
    fn new() -> Self {
        Self {
            test_results: Vec::new(),
            parity_results: HashMap::new(),
            output_log: Vec::new(),
            running: true,
            scroll: 0,
            passed: 0,
            failed: 0,
            total: 0,
        }
    }

    fn on_tick(&mut self) {
        // Periodically reload parity results
        self.load_parity_results();
    }

    fn load_parity_results(&mut self) {
        if let Ok(file) = std::fs::File::open("verification_results.jsonl") {
            let reader = BufReader::new(file);
            for line in reader.lines() {
                if let Ok(l) = line {
                    if let Ok(res) = serde_json::from_str::<ComparisonResult>(&l) {
                        // Normalize name to help matching?
                        // For now just store by layer name
                        self.parity_results.insert(res.name.clone(), res);
                    }
                }
            }
        }
    }

    fn on_log(&mut self, line: String) {
        // Parse cargo test output
        if line.contains("test result: ok") {
            // Summary line, maybe parse stats?
        } else if line.contains("test ") && line.contains(" ... ok") {
            let name = line.replace("test ", "").replace(" ... ok", "");
            self.test_results.push(TestResult {
                name,
                status: TestStatus::Passed,
                details: "".to_string(),
            });
            self.passed += 1;
            self.total += 1;
        } else if line.contains("test ") && line.contains(" ... FAILED") {
            let name = line.replace("test ", "").replace(" ... FAILED", "");
            self.test_results.push(TestResult {
                name,
                status: TestStatus::Failed,
                details: "".to_string(),
            });
            self.failed += 1;
            self.total += 1;
        }

        // Keep a rolling log
        self.output_log.push(line);
        if self.output_log.len() > 100 {
            self.output_log.remove(0);
        }
    }
}

pub fn run_dashboard(_args: &[String]) -> Result<()> {
    // Setup terminal
    enable_raw_mode()?;
    let mut stdout = std::io::stdout();
    execute!(stdout, EnterAlternateScreen)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    // Channels
    let (tx, rx) = mpsc::channel();
    let tick_rate = Duration::from_millis(100);

    // Spawn Input Thread
    let tx_input = tx.clone();
    thread::spawn(move || {
        loop {
            if event::poll(Duration::from_millis(50)).unwrap() {
                if let Event::Key(key) = event::read().unwrap() {
                    if key.kind == KeyEventKind::Press {
                        if tx_input.send(DashboardEvent::Input(key)).is_err() {
                            return;
                        }
                    }
                }
            }
        }
    });

    // Spawn Tick Thread
    let tx_tick = tx.clone();
    thread::spawn(move || {
        loop {
            thread::sleep(tick_rate);
            if tx_tick.send(DashboardEvent::Tick).is_err() {
                return;
            }
        }
    });

    // Spawn Test Runner Thread
    let tx_log = tx.clone();
    thread::spawn(move || {
        let mut cmd = Command::new("cargo")
            .arg("test")
            .arg("--")
            .arg("--nocapture") // Important to see output in real-time
            .stdout(Stdio::piped())
            .stderr(Stdio::piped()) // Capture stderr too
            .spawn()
            .expect("Failed to spawn cargo test");

        if let Some(stdout) = cmd.stdout.take() {
            let reader = BufReader::new(stdout);
            for line in reader.lines() {
                if let Ok(l) = line {
                    let _ = tx_log.send(DashboardEvent::TestLine(l));
                }
            }
        }
    });

    // App Loop
    let mut app = App::new();
    let mut should_quit = false;

    while !should_quit {
        terminal.draw(|f| ui(f, &mut app))?;

        match rx.recv()? {
            DashboardEvent::Input(key) => match key.code {
                KeyCode::Char('q') => should_quit = true,
                KeyCode::Down => {
                    if app.scroll < app.test_results.len().saturating_sub(1) {
                        app.scroll += 1;
                    }
                }
                KeyCode::Up => {
                    if app.scroll > 0 {
                        app.scroll -= 1;
                    }
                }
                _ => {}
            },
            DashboardEvent::TestLine(line) => app.on_log(line),
            DashboardEvent::Tick => app.on_tick(),
        }
    }

    // Restore terminal
    disable_raw_mode()?;
    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
    terminal.show_cursor()?;

    Ok(())
}

fn ui(f: &mut Frame, app: &mut App) {
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(3), // Header
            Constraint::Min(0),    // Main (List + Details)
            Constraint::Length(8), // Log
            Constraint::Length(1), // Footer
        ])
        .split(f.area());

    // Header
    let title = Paragraph::new(format!(
        "Parity Dashboard | Total: {} | Passed: {} | Failed: {}",
        app.total, app.passed, app.failed
    ))
    .block(
        Block::default()
            .borders(Borders::ALL)
            .title("PyCandle Status"),
    )
    .style(Style::default().fg(Color::Cyan));
    f.render_widget(title, chunks[0]);

    // Main Content Split
    let main_chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(40), Constraint::Percentage(60)])
        .split(chunks[1]);

    // --- List Widget (Left) ---
    let items: Vec<ListItem> = app
        .test_results
        .iter()
        .enumerate()
        .skip(app.scroll.saturating_sub(10)) // Simple pseudo-windowing
        .take(50)
        .map(|(i, res)| {
            let style = match res.status {
                TestStatus::Passed => Style::default().fg(Color::Green),
                TestStatus::Failed => Style::default().fg(Color::Red),
                TestStatus::Running => Style::default().fg(Color::Yellow),
            };
            let icon = match res.status {
                TestStatus::Passed => "‚úî",
                TestStatus::Failed => "‚úñ",
                TestStatus::Running => "‚è≥",
            };

            let prefix = if i == app.scroll { "> " } else { "  " };
            ListItem::new(format!("{}{}{}", prefix, icon, res.name)).style(if i == app.scroll {
                style.add_modifier(Modifier::BOLD | Modifier::REVERSED)
            } else {
                style
            })
        })
        .collect();

    let list_title = if app.test_results.is_empty() {
        "Running Tests..."
    } else {
        "Tests"
    };
    let list = List::new(items).block(Block::default().borders(Borders::ALL).title(list_title));
    f.render_widget(list, main_chunks[0]);

    // --- Details Widget (Right) ---
    // Try to find matching parity result for the selected test
    let selected_test = app.test_results.get(app.scroll);

    let details_block = Block::default().borders(Borders::ALL).title("Details");
    let inner = details_block.inner(main_chunks[1]);
    f.render_widget(details_block, main_chunks[1]);

    let details_layout = Layout::default()
        .direction(Direction::Vertical)
        .constraints([Constraint::Length(5), Constraint::Min(0)])
        .split(inner);

    if let Some(test) = selected_test {
        // Find result by fuzzy matching name
        let matched_result = app.parity_results.values().find(|r| {
            let parts: Vec<&str> = r.name.split('.').collect();
            if parts.len() >= 2 {
                test.name.contains(parts[parts.len() - 1])
                    && test.name.contains(parts[parts.len() - 2])
            } else {
                test.name.contains(&r.name)
            }
        });

        if let Some(res) = matched_result {
            // Stats
            let status_line = Line::from(vec![
                Span::raw(format!("Layer: {}\nStatus: ", res.name)),
                if res.passed {
                    Span::styled("PASS", Style::default().fg(Color::Green))
                } else {
                    Span::styled("FAIL", Style::default().fg(Color::Red))
                },
                Span::raw(format!(
                    "\nMSE: {:.2e}\nCosSim: {:.6}",
                    res.mse, res.cosine_sim
                )),
            ]);

            f.render_widget(Paragraph::new(status_line), details_layout[0]);

            // Heatmap
            if let Some(heatmap) = &res.heatmap {
                // Render 8x8 heatmap
                if heatmap.len() == 64 {
                    // Normalize for coloring
                    let max_val = heatmap.iter().cloned().fold(0.0, f32::max);

                    let canvas = Canvas::default()
                        .block(
                            Block::default()
                                .borders(Borders::ALL)
                                .title("Error Heatmap (8x8)"),
                        )
                        .x_bounds([0.0, 8.0])
                        .y_bounds([0.0, 8.0])
                        .paint(move |ctx| {
                            for r in 0..8 {
                                for c in 0..8 {
                                    let idx = r * 8 + c;
                                    let val = heatmap[idx];
                                    let intensity = if max_val > 0.0 { val / max_val } else { 0.0 };

                                    // Color ramp: Black -> Blue -> Red
                                    let color = if intensity < 0.2 {
                                        Color::DarkGray
                                    } else if intensity < 0.5 {
                                        Color::Blue
                                    } else if intensity < 0.8 {
                                        Color::Magenta
                                    } else {
                                        Color::Red
                                    };

                                    // Draw 1x1 rectangle
                                    // r=0 is first row (top), so map to y=7
                                    ctx.draw(&Rectangle {
                                        x: c as f64,
                                        y: 7.0 - r as f64,
                                        width: 1.0,
                                        height: 1.0,
                                        color,
                                    });
                                }
                            }
                        });
                    f.render_widget(canvas, details_layout[1]);
                }
            } else if !res.passed {
                f.render_widget(Paragraph::new("No heatmap available."), details_layout[1]);
            }
        } else {
            f.render_widget(
                Paragraph::new("No trace data found for this test."),
                details_layout[0],
            );
        }
    }

    // Footer
    let footer = Paragraph::new("Press 'q' to quit | '‚Üë/‚Üì' to scroll")
        .style(Style::default().fg(Color::Gray));
    f.render_widget(footer, chunks[3]);

    // Log output (Raw)
    let log_items: Vec<ListItem> = app
        .output_log
        .iter()
        .rev()
        .take(8)
        .rev()
        .map(|l| ListItem::new(l.clone()).style(Style::default().fg(Color::DarkGray)))
        .collect();

    let log_box =
        List::new(log_items).block(Block::default().borders(Borders::ALL).title("Raw Output"));
    f.render_widget(log_box, chunks[2]);
}
</file>

<file path="crates/pycandle/src/init.rs">
use anyhow::{Context, Result};
use colored::Colorize;
use std::fs;
use std::path::Path;

const RECORDER_TEMPLATE: &str = r#"import torch
import sys
import os

# Try to import pycandle spy. 
# in a real setup this would be installed, but for now we look in relative paths common in this workspace.
try:
    from pycandle.spy import GoldenRecorder
except ImportError:
    # Add potential fallback paths
    possible_paths = ["py", "../py", "../../py"]
    for p in possible_paths:
        if os.path.exists(os.path.join(p, "spy.py")):
            sys.path.append(p)
            break
    from spy import GoldenRecorder

# TODO: Import your model class
# from my_project.model import MyModel

def main():
    print("üöÄ Initializing model configuration...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device}")

    # TODO: Instantiate your model
    # model = MyModel().to(device)
    # model.eval()

    # TODO: Create dummy input matching your model's requirement
    # dummy_input = torch.randn(1, 3, 224, 224).to(device)

    print("üé• Starting recording...")
    recorder = GoldenRecorder(output_dir="traces")
    
    # TODO: Run the forward pass with the recorder
    # recorder.record(model, dummy_input)
    
    # Save the trace
    name = "debug_run"
    recorder.save(name)
    print(f"‚úÖ Recording saved to traces/{name}")

if __name__ == "__main__":
    main()
"#;

const TEST_TEMPLATE: &str = r#"#[cfg(test)]
mod tests {
    use anyhow::Result;
    // use super::*; // Import your model

    #[test]
    fn test_parity() -> Result<()> {
        // TODO: Load the checker
        // let device = candle_core::Device::Cpu;
        // let checker = pycandle_core::PyChecker::load("debug_run", "traces/", &device)?;
        
        // TODO: Load your model and run forward pass
        // let model = MyModel::load(..., Some(checker))?;
        // let output = model.forward(&input)?;

        Ok(())
    }
}
"#;

pub fn run_init(name: Option<String>) -> Result<()> {
    println!("{}", "‚ö° PyCandle Project Initialization".bold().green());

    // 1. Create recorder.py
    let recorder_path = Path::new("recorder.py");
    if recorder_path.exists() {
        println!("   {} recorder.py already exists, skipping.", "‚ö†Ô∏è".yellow());
    } else {
        fs::write(recorder_path, RECORDER_TEMPLATE).context("Failed to write recorder.py")?;
        println!("   {} Created recorder.py", "‚úÖ".green());
    }

    // 2. Create tests directory and parity test if requested
    let tests_dir = Path::new("tests");
    if !tests_dir.exists() {
        fs::create_dir(tests_dir).ok(); // failure ok, maybe we are not in root
    }

    let test_path = tests_dir.join("parity.rs");
    if tests_dir.exists() && !test_path.exists() {
        fs::write(&test_path, TEST_TEMPLATE).context("Failed to write tests/parity.rs")?;
        println!("   {} Created tests/parity.rs", "‚úÖ".green());
    } else {
        println!(
            "   {} tests/parity.rs already exists (or tests/ missing), skipping.",
            "‚ö†Ô∏è".yellow()
        );
    }

    println!("\nNext steps:");
    println!("1. Edit {} to import your model.", "recorder.py".bold());
    println!(
        "2. Run {} to capture traces.",
        "pycandle record --script recorder.py --name debug_run".bold()
    );
    println!(
        "3. Run {} to generate Rust code.",
        "pycandle codegen ...".bold()
    );

    Ok(())
}
</file>

<file path="crates/pycandle/src/test_gen.rs">
use anyhow::{Context, Result};
use std::fs;
use std::path::Path;

pub struct TestGenerator {
    model_name: String,
    crate_name: String,
}

impl TestGenerator {
    pub fn new(model_name: String) -> Self {
        Self {
            model_name,
            crate_name: Self::detect_crate_name().unwrap_or_else(|_| "my_project".to_string()),
        }
    }

    fn detect_crate_name() -> Result<String> {
        let content = fs::read_to_string("Cargo.toml").context("Failed to read Cargo.toml")?;
        let toml: toml::Value = toml::from_str(&content).context("Failed to parse Cargo.toml")?;

        if let Some(package) = toml.get("package") {
            if let Some(name) = package.get("name") {
                return Ok(name.as_str().unwrap_or("my_project").replace("-", "_"));
            }
        }

        Ok("my_project".to_string())
    }

    pub fn generate_test_file(&self) -> String {
        format!(
            r#"#[cfg(test)]
mod tests {{
    use super::*;
    use candle_core::{{Device, Tensor}};
    use pycandle_core::PyChecker;
    use {}::{};
    use anyhow::Result;

    #[test]
    fn test_parity() -> Result<()> {{
        // 1. Setup Device
        let device = Device::cuda_if_available(0).unwrap_or(Device::Cpu);
        println!("Running on device: {{:?}}", device);

        // 2. Load Checker and Golden Trace
        // Assumes "pycandle_trace" directory exists with trace files
        let checker = PyChecker::load("debug_run", "pycandle_trace", &device)?;
        println!("Loaded checker with trace: {{}}", checker.name);

        // 3. Load Model
        // We use the same variable builder as normally, but verify loaded weights match if needed
        // For parity test, we rely on the implementation's load to initialize weights correctly 
        // (often random or specific config). 
        // Ideally, we should load exact weights from the trace if available, but for now 
        // we assume the user might have a load_from_weights or similar if critical.
        // However, for pure activation parity on specific inputs, we usually need the weights to match.
        // TODO: In a real scenario, we might need to load weights from the trace too.
        // For now, we assume the user constructs the model. 
        // NOTE: If the model uses random initialization, this test WILL FAIL unless 
        // we load weights from the python trace.
        // 
        // As a workaround for this generic generator, we assume the user has a `load` function
        // that takes the checker.
        let vb = candle_nn::VarBuilder::zeros(candle_core::DType::F32, &device);
        let model = {}::load(vb, Some(checker.clone()))?;

        // 4. Load Inputs from Trace
        // The trace should contain 'model_input.0', 'model_input.1', etc.
        // We'll try to load at least the first input.
        // Note: PyChecker holds "golden_tensors", which contains the inputs too if we saved them!
        
        // We access the inputs directly from the golden tensors loaded in checker
        // (This requires PyChecker to expose golden_tensors or a getter, or we access directly if pub)
        // Since `golden_tensors` is private in PyChecker but we need it, ensure PyChecker exposes a way.
        // Assuming PyChecker has a `get_tensor` method or we can clone from the file.
        // Let's use `candle_core::safetensors::load` again here for simplicity to get inputs.
        
        let tensors = candle_core::safetensors::load("pycandle_trace/debug_run_trace.safetensors", &device)?;
        
        // 5. Run Forward Pass
        if let Some(input) = tensors.get("model_input.0") {{
            let _output = model.forward(input)?;
            println!("Forward pass completed successfully!");
        }} else {{
            eprintln!("‚ö†Ô∏è No 'model_input.0' found in trace. Skipping forward pass execution.");
            println!("Available keys: {{:?}}", tensors.keys().take(5));
        }}

        Ok(())
    }}
}}
"#,
            self.crate_name, self.model_name, self.model_name
        )
    }
}
</file>

<file path="crates/pycandle/src/todos.rs">
// TODO extraction and management for generated code
use regex::Regex;
use serde::Serialize;
use std::collections::HashMap;

#[derive(Serialize, Debug)]
pub struct TodoItem {
    pub line: usize,
    pub module_type: String,
    pub field_name: String,
    pub context: String,
    pub suggestion: String,
}

#[derive(Serialize, Debug)]
pub struct TodoReport {
    pub file: String,
    pub total: usize,
    pub by_type: HashMap<String, usize>,
    pub todos: Vec<TodoItem>,
}

/// Extract TODO markers from generated Rust code
pub fn extract_todos(content: &str) -> Vec<TodoItem> {
    let mut todos = Vec::new();

    // Match: let field_name = todo!("Implement initialization for ModuleType");
    let re = Regex::new(r#"let\s+(\w+)\s*=\s*todo!\("Implement initialization for (\w+)"\)"#)
        .expect("Invalid regex");

    for (line_num, line) in content.lines().enumerate() {
        if let Some(caps) = re.captures(line) {
            let field_name = caps.get(1).unwrap().as_str().to_string();
            let module_type = caps.get(2).unwrap().as_str().to_string();

            todos.push(TodoItem {
                line: line_num + 1,
                field_name,
                module_type: module_type.clone(),
                context: line.trim().to_string(),
                suggestion: get_implementation_hint(&module_type),
            });
        }
    }

    todos
}

/// Get implementation hint for a module type
pub fn get_implementation_hint(module_type: &str) -> String {
    match module_type {
        "LSTM" => r#"LSTM::load(vb.pp("field"), input_size, hidden_size, num_layers)?"#.to_string(),
        "BatchNorm1d" => r#"BatchNorm1d::load(vb.pp("field"), num_features)?"#.to_string(),
        "BatchNorm2d" => r#"BatchNorm2d::load(vb.pp("field"), num_features)?"#.to_string(),
        "Snake" => r#"Snake::load(vb.pp("field"), in_features)?"#.to_string(),
        "ReLU" => "ReLU".to_string(),
        "Sigmoid" => "Sigmoid".to_string(),
        "ELU" => "ELU::new(1.0)".to_string(),
        "Dropout" => "// Dropout is a no-op at inference time".to_string(),
        "Conv1D" => r#"// HuggingFace Conv1D: implement as Linear with transpose"#.to_string(),
        "NewGELUActivation" => "GELU // or x.gelu_erf()".to_string(),
        _ => format!("// Manual implementation needed for {}", module_type),
    }
}

/// Generate a report from extracted TODOs
pub fn generate_report(file_path: &str, todos: Vec<TodoItem>) -> TodoReport {
    let mut by_type: HashMap<String, usize> = HashMap::new();
    for todo in &todos {
        *by_type.entry(todo.module_type.clone()).or_default() += 1;
    }

    TodoReport {
        file: file_path.to_string(),
        total: todos.len(),
        by_type,
        todos,
    }
}
</file>

<file path="docs/meta_trace_guide.md">
# PyCandle Meta-Trace Strategy

The **Meta-Trace Strategy** is the recommended SOP for porting large deep learning models (e.g., Transformers, LLMs) to Candle, especially on machines with limited RAM (e.g., 16GB RAM / 4GB available).

## The Problem: Paging File Errors
Loading weight files directly into PyTorch (e.g., 1.8GB + intermediate allocations) often exhausts system memory, leading to `OSError: The paging file is too small`.

## The Solution: Record Architecture on `meta`
Instead of loading weights into memory, we initialize the model on the PyTorch `meta` device. This records the **shapes**, **parameters**, and **computation graph** without allocating any actual weight buffers.

### Step 1: Initialize on Meta Device
Update your recording script to use the `meta` device.

```python
import torch
from py.spy import GoldenRecorder
from my_model import MyLargeModel

device = "meta"
with torch.device(device):
    model = MyLargeModel()
model.eval()

# Dummy inputs must also be on meta
dummy_input = torch.randn(1, 128, model.dim, device=device)

recorder = GoldenRecorder(output_dir="pycandle_trace")
recorder.record(model, dummy_input)
recorder.save("my_large_model_meta")
```

### Step 2: Generate Rust Code
Run the `pycandle codegen` as usual. The manifest contains everything needed to define the Rust structs.

```bash
cargo run -- codegen --manifest pycandle_trace/my_large_model_meta_manifest.json --out generated_model.rs
```

### Step 3: Stream-Extract Weights
Since you didn't record weights, your `trace.safetensors` will be empty or missing tensors. Use a specialized extraction script to map weights from the original checkpoint directly to the manifest-compatible keys.

```python
from safetensors.torch import load_file, save_file
# Load only what manifests says we need
# Rename keys to match manifest
# Save to a new, minimal weights file
```

## Benefits
- **Zero Memory Consumption**: Tracing happens in milliseconds with no RAM pressure.
- **Permanent Solution**: Works for models of any size (7B, 70B+) as long as the architecture fits in memory.
- **Power User DX**: Separates architecture capture from weight management.
</file>

<file path="docs/PORTING_SOP.md">
# PyCandle Porting SOP

This document outlines the Standard Operating Procedure (SOP) for porting PyTorch models to Rust/Candle using the PyCandle framework.

## 1. Project Setup
Initialize a new porting project. This generates the necessary boilerplate for recording and parity testing.

```bash
# In your workspace root
pycandle init --name <RunName>
```

**What this does:**
- Creates `recorder.py`: A standard PyTorch recording script tailored for your model.
- Creates `tests/main.rs`: A standard integration test harness for parity verification.

## 2. Model Wrapping (Python)
Edit the generated `recorder.py` to import and instantiate your specific PyTorch model.

```python
# recorder.py
from my_model_source import MyModel  # <--- Import your model

model = MyModel(...)
input_tensor = torch.randn(...)
```

## 3. Recording Traces
Run the recorder to capture the Golden Trace (activations, weights, and config).

```bash
# Uses uv under the hood to manage dependencies
pycandle record --script recorder.py --name <RunName>
```

*Output:* `traces/<RunName>/` containing `.safetensors` and `_manifest.json`.

## 4. Codegen & Analysis
Analyze the trace to see what layers are supported and generate the Rust code.

```bash
# Analyze first
pycandle codegen --manifest traces/<RunName>/_manifest.json --analyze-only

# Generate Rust code
pycandle codegen --manifest traces/<RunName>/_manifest.json --out crates/my-model/src/model.rs --model MyModel
```

## 5. Parity Verification
Run the generated test harness to verify that your Rust implementation matches the PyTorch golden record bit-for-bit.

```bash
cargo test
```

## 6. Iterative Refinement
If `cargo test` fails or parity is low:
1.  Use `pycandle dashboard` to visualize the error.
2.  Fix the Rust implementation.
3.  Re-run `cargo test`.
</file>

<file path="dummy_manifest.json">
{
    "layer1": {
        "name": "layer1",
        "module_type": "Linear",
        "input_shapes": [
            [
                1,
                10
            ]
        ],
        "output_shapes": [
            [
                1,
                20
            ]
        ],
        "parameters": [],
        "is_leaf": true,
        "config": {}
    },
    "layer2": {
        "name": "layer2",
        "module_type": "ReLU",
        "input_shapes": [
            [
                1,
                20
            ]
        ],
        "output_shapes": [
            [
                1,
                20
            ]
        ],
        "parameters": [],
        "is_leaf": true,
        "config": {}
    },
    "layer3": {
        "name": "layer3",
        "module_type": "Linear",
        "input_shapes": [
            [
                1,
                20
            ]
        ],
        "output_shapes": [
            [
                1,
                30
            ]
        ],
        "parameters": [],
        "is_leaf": true,
        "config": {}
    },
    "layer4": {
        "name": "layer4",
        "module_type": "Sigmoid",
        "input_shapes": [
            [
                1,
                30
            ]
        ],
        "output_shapes": [
            [
                1,
                30
            ]
        ],
        "parameters": [],
        "is_leaf": true,
        "config": {}
    }
}
</file>

<file path="gen_dummy_results.rs">
use serde::Serialize;
use std::fs::OpenOptions;
use std::io::Write;

#[derive(Serialize)]
struct Result {
    name: String,
    mse: f32,
    max_diff: f32,
    cosine_sim: f32,
    passed: bool,
}

fn main() {
    let results = vec![
        Result {
            name: "layer1.out.0".to_string(),
            mse: 1e-10,
            max_diff: 1e-5,
            cosine_sim: 1.0,
            passed: true,
        },
        Result {
            name: "layer2.out.0".to_string(),
            mse: 1e-8,
            max_diff: 1e-4,
            cosine_sim: 0.9999,
            passed: true,
        },
        Result {
            name: "layer3.out.0".to_string(),
            mse: 0.05,
            max_diff: 0.1,
            cosine_sim: 0.95,
            passed: false,
        }, // Drift!
        Result {
            name: "layer4.out.0".to_string(),
            mse: 0.2,
            max_diff: 0.5,
            cosine_sim: 0.8,
            passed: false,
        },
    ];

    let mut file = OpenOptions::new()
        .create(true)
        .write(true)
        .truncate(true)
        .open("verification_results.jsonl")
        .unwrap();

    for r in results {
        writeln!(file, "{}", serde_json::to_string(&r).unwrap()).unwrap();
    }

    println!("Generated verification_results.jsonl");
}
</file>

<file path="generated_hints.rs">
use candle_core::{Tensor, Result, Device, Shape};
use candle_nn::{Linear, Conv1d, LayerNorm, Embedding, VarBuilder, Module};
use pycandle_core::{PyChecker, py_check, Dropout, Transpose, Mish, CausalConv1d, SiLU, ReLU, GELU, Sigmoid, Tanh, ELU, LeakyReLU, Snake, BatchNorm1d, BatchNorm2d, LSTM};

pub struct Config {
    pub hidden_dim: usize, // 64
    pub vocab_size: usize, // 64
}

pub struct MyModel {
    pub embedding: Embedding,
    pub linear: Linear,
    pub checker: Option<PyChecker>,
}

impl MyModel {
    pub fn load(cfg: Config, vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let embedding = candle_nn::embedding(cfg.vocab_size, cfg.hidden_dim, vb.pp("embedding"))?;
        let linear = { let w = vb.pp("linear").get((cfg.hidden_dim, cfg.hidden_dim), "weight")?.t()?; let b = Some(vb.pp("linear").get(cfg.hidden_dim, "bias")?); Linear::new(w, b) };

        Ok(Self {
            embedding,
            linear,
            checker,
        })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let mut x = xs.clone();

        // Layer: embedding
        x = self.embedding.forward(&x)?;
        py_check!(self.checker, "embedding", &x);

        // Layer: linear
        x = self.linear.forward(&x)?;
        py_check!(self.checker, "linear", &x);

        Ok(x)
    }
}
</file>

<file path="generated_symbolic.rs">
use candle_core::{Tensor, Result, Device, Shape};
use candle_nn::{Linear, Conv1d, LayerNorm, Embedding, VarBuilder, Module};
use pycandle_core::{PyChecker, py_check, Dropout, Transpose, Mish, CausalConv1d, SiLU, ReLU, GELU, Sigmoid, Tanh, ELU, LeakyReLU, Snake, BatchNorm1d, BatchNorm2d, LSTM};

pub struct Config {
    pub hidden_dim: usize, // 768
    pub vocab_size: usize, // 50257
}

pub struct MyTestModel {
    pub embedding: Embedding,
    pub linear1: Linear,
    pub linear2: Linear,
    pub ln: LayerNorm,
    pub checker: Option<PyChecker>,
}

impl MyTestModel {
    pub fn load(cfg: Config, vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let embedding = candle_nn::embedding(cfg.vocab_size, cfg.hidden_dim, vb.pp("embedding"))?;
        let linear1 = candle_nn::linear(cfg.hidden_dim, 2048, vb.pp("linear1"))?;
        let linear2 = candle_nn::linear(2048, cfg.hidden_dim, vb.pp("linear2"))?;
        let ln = candle_nn::layer_norm(768, candle_nn::LayerNormConfig { eps: 1.0e-5, ..Default::default() }, vb.pp("ln"))?;

        Ok(Self {
            embedding,
            linear1,
            linear2,
            ln,
            checker,
        })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let mut x = xs.clone();

        // Layer: embedding
        x = self.embedding.forward(&x)?;
        py_check!(self.checker, "embedding", &x);

        // Layer: linear1
        x = self.linear1.forward(&x)?;
        py_check!(self.checker, "linear1", &x);

        // Layer: linear2
        x = self.linear2.forward(&x)?;
        py_check!(self.checker, "linear2", &x);

        // Layer: ln
        x = self.ln.forward(&x)?;
        py_check!(self.checker, "ln", &x);

        Ok(x)
    }
}
</file>

<file path="py/.python-version">
3.11
</file>

<file path="py/dag_model.py">
import torch
import torch.nn as nn
from spy import GoldenRecorder

class ComplexModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv1d(16, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(32 * 10, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        # x is (B, 32, 10)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

if __name__ == "__main__":
    model = ComplexModel()
    recorder = GoldenRecorder()
    
    # Dummy input (B, C, T)
    x = torch.randn(1, 16, 10)
    
    recorder.record(model, x)
    recorder.trace_fx(model, x)
    recorder.save("complex_model", use_fx=True)
</file>

<file path="py/generated_residual.rs">
use candle_core::{Tensor, Result, Device, Shape};
use candle_nn::{Linear, Conv1d, LayerNorm, Embedding, VarBuilder, Module};
use pycandle_core::{PyChecker, py_check, Dropout, Transpose, Mish, CausalConv1d, SiLU, ReLU, GELU, Sigmoid, Tanh, ELU, LeakyReLU, Snake, BatchNorm1d, BatchNorm2d, LSTM};

pub struct ResidualModel {
    pub bn1: BatchNorm1d,
    pub bn2: BatchNorm1d,
    pub conv1: Conv1d,
    pub conv2: Conv1d,
    pub relu: ReLU,
    pub relu_1: ReLU,
    pub checker: Option<PyChecker>,
}

impl ResidualModel {
    pub fn load(vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let bn1 = BatchNorm1d::load(vb.pp("bn1"), 16)?;
        let bn2 = BatchNorm1d::load(vb.pp("bn2"), 16)?;
        let conv1 = candle_nn::conv1d(16, 16, 3, candle_nn::Conv1dConfig { stride: 1, padding: 1, ..Default::default() }, vb.pp("conv1"))?;
        let conv2 = candle_nn::conv1d(16, 16, 3, candle_nn::Conv1dConfig { stride: 1, padding: 1, ..Default::default() }, vb.pp("conv2"))?;
        let relu = ReLU;
        let relu_1 = ReLU;

        Ok(Self {
            bn1,
            bn2,
            conv1,
            conv2,
            relu,
            relu_1,
            checker,
        })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let x_conv1 = self.conv1.forward(&xs)?;
        py_check!(self.checker, "conv1", &x_conv1);
        let x_bn1 = self.bn1.forward(&x_conv1)?;
        py_check!(self.checker, "bn1", &x_bn1);
        let x_relu = self.relu.forward(&x_bn1)?;
        py_check!(self.checker, "relu", &x_relu);
        let x_conv2 = self.conv2.forward(&x_relu)?;
        py_check!(self.checker, "conv2", &x_conv2);
        let x_bn2 = self.bn2.forward(&x_conv2)?;
        py_check!(self.checker, "bn2", &x_bn2);
        let x_add = (&x_bn2 + &xs)?;
        let x_relu_1 = self.relu.forward(&x_add)?;
        py_check!(self.checker, "relu", &x_relu_1);
        Ok(x_relu_1)
    }
}
</file>

<file path="py/hello.py">
def main():
    print("Hello from pycandle-spy!")


if __name__ == "__main__":
    main()
</file>

<file path="py/pyproject.toml">
[project]
name = "pycandle-spy"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "numpy",
    "packaging",
    "safetensors",
    "torch>=2.0.0",
]
</file>

<file path="py/test_hints.py">
import torch
import torch.nn as nn
from spy import GoldenRecorder

class HintsModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Ambiguous: Both 64
        self.embedding = nn.Embedding(64, 64)
        self.linear = nn.Linear(64, 64)

    def forward(self, x):
        x = self.embedding(x)
        x = self.linear(x)
        return x

model = HintsModel()
recorder = GoldenRecorder(output_dir="test_trace_hints")
x = torch.randint(0, 64, (1, 10))
recorder.record(model, x)

# Provide hints to resolve ambiguity
hints = {
    "vocab_size": 64,
    "hidden_dim": 64
}

recorder.save("hints_test", hints=hints)
</file>

<file path="py/test_model.py">
import torch
import torch.nn as nn
from spy import GoldenRecorder

class SmallModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(16 * 10, 2)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

if __name__ == "__main__":
    model = SmallModel()
    recorder = GoldenRecorder()
    
    # Dummy input (B, C, T)
    x = torch.randn(1, 1, 10)
    
    recorder.record(model, x)
    recorder.save("small_model")
</file>

<file path="py/weight_extractor.py">
import os
import sys
import json
import torch
from safetensors.torch import save_file
from typing import Dict, Any, Set

def extract_weights(checkpoint_path: str, manifest_path: str, output_path: str, mapper_path: str = None):
    """
    Extracts only the weights specified in the manifest from a PyTorch checkpoint.
    Supports .bin/.pt (Pickle) and .safetensors.
    """
    if not os.path.exists(manifest_path):
        print(f"‚ùå Manifest not found: {manifest_path}")
        sys.exit(1)
        
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    # Identify active parameters
    active_params: Set[str] = set()
    for name, meta in manifest.items():
        if name.startswith('_'): 
            continue
        if 'parameters' in meta:
            for p in meta['parameters']:
                active_params.add(f"{name}.{p}")
    
    print(f"üîç Manifest contains {len(active_params)} active parameters.")
    
    # Load weights selectively
    weights: Dict[str, torch.Tensor] = {}
    
    if checkpoint_path.endswith('.safetensors'):
        from safetensors import safe_open
        with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                if key in active_params:
                    weights[key] = f.get_tensor(key)
    else:
        # Pickle-based (.bin, .pt, .pth)
        # We use weights_only=True for security and map_location='cpu' for memory
        try:
            state_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            for k, v in state_dict.items():
                if k in active_params:
                    weights[k] = v
        except Exception as e:
            print(f"‚ùå Failed to load checkpoint: {e}")
            # Fallback for older torch versions or complex pickles
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            for k, v in state_dict.items():
                if k in active_params:
                    weights[k] = v

    if not weights:
        print("‚ö†Ô∏è No matching weights found in checkpoint!")
        # Print a few examples from the checkpoint if possible
        return

    # Optional renaming
    if mapper_path and os.path.exists(mapper_path):
        import re
        with open(mapper_path, 'r') as f:
            mappings = json.load(f)
        
        print(f"üîÑ Applying {len(mappings)} renaming patterns...")
        renamed_weights = {}
        # Sort mappings by length of pattern (desc) or just alphabetical for consistency?
        # Typically we want deterministic order.
        sorted_patterns = sorted(mappings.items())
        
        for k, v in weights.items():
            new_k = k
            for pattern, replacement in sorted_patterns:
                new_k = re.sub(pattern, replacement, new_k)
            renamed_weights[new_k] = v
        weights = renamed_weights

    # Ensure directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    
    save_file(weights, output_path)
    print(f"‚úÖ Surgically extracted {len(weights)} weights to {output_path}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Surgically extract model weights.")
    parser.add_argument("--checkpoint", required=True, help="Path to PyTorch checkpoint (.bin, .pt, .safetensors)")
    parser.add_argument("--manifest", required=True, help="Path to manifest.json")
    parser.add_argument("--out", required=True, help="Output .safetensors path")
    parser.add_argument("--map", help="Optional JSON mapping file for renaming")
    args = parser.parse_args()
    
    extract_weights(args.checkpoint, args.manifest, args.out, args.map)
</file>

<file path="recorder.py">
import torch
import sys
import os

# Try to import pycandle spy. 
# in a real setup this would be installed, but for now we look in relative paths common in this workspace.
try:
    from pycandle.spy import GoldenRecorder
except ImportError:
    # Add potential fallback paths
    possible_paths = ["py", "../py", "../../py"]
    for p in possible_paths:
        if os.path.exists(os.path.join(p, "spy.py")):
            sys.path.append(p)
            break
    from spy import GoldenRecorder

# TODO: Import your model class
# from my_project.model import MyModel

def main():
    print("üöÄ Initializing model configuration...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device}")

    # TODO: Instantiate your model
    # model = MyModel().to(device)
    # model.eval()

    # TODO: Create dummy input matching your model's requirement
    # dummy_input = torch.randn(1, 3, 224, 224).to(device)

    print("üé• Starting recording...")
    recorder = GoldenRecorder(output_dir="traces")
    
    # TODO: Run the forward pass with the recorder
    # recorder.record(model, dummy_input)
    
    # Save the trace
    name = "debug_run"
    recorder.save(name)
    print(f"‚úÖ Recording saved to traces/{name}")

if __name__ == "__main__":
    main()
</file>

<file path="ref1.txt">
This is a great place to be because the "Old Way" is painful enough that everyone complains about it, but nobody has built the standardized tool to fix it yet.

Here is exactly how Candle devs do it now versus how they would do it with your tool.

### 1. The "Old Way" (Current Workflow)

Currently, porting a model from PyTorch to Candle is a manual game of "Spot the Difference."

* **Step 1: The Manual Export.** They write a Python script to save the weights to a `.safetensors` file.
* **Step 2: The Shape Mismatch Hell.** They run the Rust code, and it panics with `Error: shape mismatch, expected [1, 12, 64], got [1, 64, 12]`.
* *Why?* PyTorch Linear layers are often stored as `(out_features, in_features)`, while Candle might expect the transpose depending on how you initialize it.


* **Step 3: The "Print" Debugging.** To find *where* the math breaks, they have to:
* Go into the Python code and add `print(x.mean())` after every layer.
* Go into the Rust code and add `println!("{:?}", x.mean_all())` after every layer.
* Manually compare the numbers in the terminal.


* **Step 4: The Silent Failures.** Sometimes the shapes match, but the output is garbage because of a subtle bug (e.g., `LayerNorm` using a slightly different epsilon or initialization). They only realize this after porting the *entire* model and getting bad results.

### 2. The "New Way" (Using Your `EquivCheck` Tool)

Your tool changes this from "Manual Debugging" to "Test-Driven Development."

#### Phase A: The Python Spy (One-Time Setup)

Instead of adding print statements, the user just wraps their PyTorch model in your hook.

```python
# User's Python Script
import torch
from equivcheck import create_golden_record

model = MyPyTorchModel()
model.load_state_dict(torch.load("weights.pt"))

# This is your tool's magic function
# It runs a forward pass and saves inputs/outputs of EVERY layer to a file
create_golden_record(
    model, 
    input_data=torch.randn(1, 3, 224, 224), 
    save_path="debug_trace.safetensors"
)

```

#### Phase B: The Rust Verifier (The Workflow Loop)

Now, as they write the Rust code, they don't guess. They assert.

**The Workflow:**

1. They write the code for *just the first layer* (e.g., the Embedding).
2. They wrap it in your `trace!` macro.
3. They run `cargo test`.
4. Your tool loads `debug_trace.safetensors`, grabs the Python output for that layer, and compares it to the Rust output.
* **Pass:** They move to the next layer.
* **Fail:** Your tool prints: `‚ùå Layer 'conv1' mismatch! MSE: 0.42. Shape: [1, 64, 112, 112] (Rust) vs [1, 112, 112, 64] (Python Reference).`



**Example Usage in Rust:**

```rust
// Inside their model definition
impl Module for MyModel {
    fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        // Your tool inspects the input 'xs' against the Python recorded input
        let mut x = self.conv1.forward(xs)?;
        
        // Your macro validates the output automatically
        // If it drifts > 1e-5 from the Python run, it panics with a helpful diff
        equiv_check::verify("conv1", &x)?; 

        x = self.bn1.forward(&x)?;
        equiv_check::verify("bn1", &x)?;
        
        Ok(x)
    }
}

```

### Why this is a "Moat"

You are solving the **Integration Gap**.

* **The "Vibe Coder"** cannot build this because it requires understanding `safetensors` headers, memory layout, and floating-point precision differences between `f32` and `bf16`.
* **The Value:** You save senior engineers hours of frustration. If you open-source this, you become the standard way people learn Candle.

---

I want to start making this tooling. The thing is it has to be robust enough for most pytorch/transformers models. I'm thinking the best place to start is with audio models (tts/stt) like parakeet + (whatever tts models theres a lot of them - i just ported soprano tts to candle)

---
---

This is a brilliant "pain-point-first" approach to building a developer tool. You aren't just building a library; you are building a **bridge** between two ecosystems that currently don't speak the same language.

Starting with **audio models (TTS/STT)** is a strategic masterstroke for three reasons:
1.  **Complexity of Shapes:** Audio models often involve 3D and 4D tensors with complex padding, striding, and frequent transpositions (e.g., `(B, C, T)` vs `(B, T, C)`), which is exactly where Candle devs get stuck.
2.  **Sequential Dependencies:** In TTS, a small error in the encoder propagates and explodes in the decoder/vocoder. Finding that "point of divergence" is impossible without your tool.
3.  **The "Black Box" of Audio Ops:** Ops like `STFT`, `Conv1d`, and `GroupNorm` are implemented slightly differently across frameworks.

Here is a technical roadmap to make `EquivCheck` robust enough to become the industry standard for Candle porting.

---

### Phase 1: The Python "Golden Record" (The Spy)

The biggest challenge in Python is **naming**. PyTorch‚Äôs `named_modules()` gives you a clean hierarchy (e.g., `encoder.block.0.attn`), but the data flowing *between* them isn't always captured by hooks.

**The Implementation Strategy:**
Don't just hook modules; hook the **tensors**. However, for a V1, a recursive module hook is best.

```python
import torch
from safetensors.torch import save_file

class EquivCheckHook:
    def __init__(self):
        self.records = {}
        self.counter = 0

    def __call__(self, name):
        def hook(module, input, output):
            # Handle multiple inputs/outputs (common in audio models)
            if isinstance(input, tuple):
                for i, inp in enumerate(input):
                    if isinstance(inp, torch.Tensor):
                        self.records[f"{name}.in.{i}"] = inp.detach().cpu()
            
            if isinstance(output, torch.Tensor):
                self.records[f"{name}.out"] = output.detach().cpu()
            elif isinstance(output, tuple):
                for i, out in enumerate(output):
                    if isinstance(out, torch.Tensor):
                        self.records[f"{name}.out.{i}"] = out.detach().cpu()
        return hook

def create_golden_record(model, input_data, save_path):
    hooker = EquivCheckHook()
    for name, module in model.named_modules():
        module.register_forward_hook(hooker(name))
    
    with torch.no_grad():
        model(*input_data) # Support multiple inputs
    
    save_file(hooker.records, save_path)
```

### Phase 2: The Rust "Verifier" (The Judge)

The Rust side needs to be "zero-cost" in production but "high-context" in dev/test.

**Key Feature: The "Tolerance" Engine.**
Floating point math is non-deterministic across hardware. You need to support `atol` (absolute tolerance) and `rtol` (relative tolerance).

```rust
// Proposed Rust API
pub struct EquivChecker {
    data: HashMap<String, Tensor>,
    tolerance: f64,
}

impl EquivChecker {
    pub fn new(path: &str) -> Self {
        // Load safetensors into a hashmap
    }

    pub fn verify(&self, name: &str, actual: &Tensor) -> Result<()> {
        let expected = self.data.get(&format!("{}.out", name))
            .ok_or_else(|| Error::Msg(format!("Layer {} not found in record", name)))?;

        // 1. Check Shape
        if actual.shape() != expected.shape() {
            return Err(Error::Msg(format!(
                "Shape Mismatch at {}: Rust {:?} vs Python {:?}", 
                name, actual.shape(), expected.shape()
            )));
        }

        // 2. Check Values (MSE or Max Diff)
        let diff = (actual - expected)?.sqr()?.mean_all()?.to_scalar::<f32>()?;
        if diff > self.tolerance {
            panic!("Value Mismatch at {}! MSE: {}", name, diff);
        }
        
        Ok(())
    }
}
```

### Phase 3: Solving the "Moat" Challenges

To make this "robust for most models," you need to handle these three edge cases:

#### 1. The Transpose Trap (Linear Layers)
In PyTorch, `nn.Linear` stores weights as `(out, in)`. In Candle, if a dev uses `linear_no_bias`, they might accidentally initialize weights as `(in, out)`. 
*   **Tool Solution:** Your tool should check the weights in the `.safetensors` file and **warn** the user: *"Hey, your Rust weight shape is the transpose of the Python weight shape. Did you forget to transpose during loading?"*

#### 2. The "View" vs "Reshape" vs "Transpose"
Audio models (like Parakeet) do a lot of `x.transpose(1, 2)` to switch between channel-first and time-first.
*   **Tool Solution:** When a verification fails, the tool shouldn't just say "Mismatch." It should attempt a few common permutations (transpose, permute) and say: *"Mismatch! But wait: if you transpose(1, 2) the Rust tensor, it matches the Python one. Check your dimension logic."*

#### 3. Dynamic Sequences
Audio lengths vary. 
*   **Tool Solution:** The `Golden Record` should store the **metadata** of the input (sample rate, padding length) so the Rust test suite can reconstruct the exact same input conditions.

---

### Why Audio Models are your best "Alpha" test
If you start with **Soprano** or **Parakeet**, you will hit the **RNN/LSTM** problem. 
*   PyTorch LSTMs are "all-in-one" ops. 
*   Candle LSTMs are often built manually or via a trait. 
*   **Your tool will be the only way** to verify that the hidden state at $t=50$ is the same in both versions.

### Next Steps for you:
1.  **The "Minimal Viable Tool":** Create a crate that can compare a single Candle `Tensor` against a single `.safetensors` file key.
2.  **The "Trace" Macro:** Write a Rust macro `trace_equiv!(actual_tensor, "layer_name")` that only compiles during `#[cfg(test)]`.
3.  **The Audio Test:** Port a single block of **Parakeet** (e.g., one Depthwise-Conv block). Use the tool to find the exact line where the mean shifts.

**This is a "Moat" because you are building the "Source of Truth" for the Candle ecosystem.** If I'm a CTO and I want to move my TTS inference to Rust to save 70% on cloud costs, I'm going to tell my engineers: "Use EquivCheck, or we'll be debugging for months."
</file>

<file path="ref2.txt">
This is the foundational architecture for **EquivCheck**. I have structured this as a complete, two-language MVP. 

To give you the "10,000-line" depth you're looking for, I‚Äôve focused on the **engine logic**: the Python side that handles recursive registration (crucial for deep TTS models like Chatterbox/Parakeet) and the Rust side that implements a "Context-Aware" verification system.

---

### Part 1: `equivcheck-py` (The Python Spy)
This script wraps your PyTorch model. It doesn't just save weights; it saves the **activations** (the "Golden Record").

```python
import torch
import torch.nn as nn
from safetensors.torch import save_file
from typing import Dict, Optional, Union, List
import json

class GoldenRecorder:
    """
    The 'Spy' that hooks into PyTorch to record every single tensor 
    passing through the model.
    """
    def __init__(self, model: nn.Module, precision: torch.dtype = torch.float32):
        self.model = model
        self.precision = precision
        self.records: Dict[str, torch.Tensor] = {}
        self.layer_metadata: Dict[str, dict] = {}
        self._hooks = []

    def _register_hooks(self):
        for name, module in self.model.named_modules():
            if name == "": continue # Skip root
            
            # We use a closure to capture the name
            def hook_fn(m, inp, out, n=name):
                # Record Input
                if isinstance(inp, tuple) and len(inp) > 0:
                    for i, item in enumerate(inp):
                        if isinstance(item, torch.Tensor):
                            self.records[f"{n}.in.{i}"] = item.detach().to(self.precision).cpu()
                
                # Record Output
                if isinstance(out, torch.Tensor):
                    self.records[f"{n}.out"] = out.detach().to(self.precision).cpu()
                elif isinstance(out, (tuple, list)):
                    for i, item in enumerate(out):
                        if isinstance(item, torch.Tensor):
                            self.records[f"{n}.out.{i}"] = item.detach().to(self.precision).cpu()
                
                # Record Metadata for debugging
                self.layer_metadata[n] = {
                    "type": str(type(m)),
                    "has_bias": hasattr(m, 'bias') and m.bias is not None
                }

            self._hooks.append(module.register_forward_hook(hook_fn))

    def record(self, *args, **kwargs):
        """Runs a forward pass and captures all data."""
        self._register_hooks()
        try:
            with torch.no_grad():
                output = self.model(*args, **kwargs)
        finally:
            for h in self._hooks:
                h.remove()
        return output

    def save(self, path_prefix: str):
        """Saves the trace and metadata."""
        # Save tensors
        save_file(self.records, f"{path_prefix}.safetensors")
        
        # Save structural metadata so Rust knows what to expect
        with open(f"{path_prefix}_meta.json", "w") as f:
            json.dump(self.layer_metadata, f, indent=2)
        print(f"‚úÖ Golden Record saved to {path_prefix}.safetensors")

# Example Usage for a SOTA TTS Model (like Chatterbox/Soprano)
# model = ChatterboxTurbo.load_pretrained()
# recorder = GoldenRecorder(model)
# dummy_input = torch.randn(1, 80, 200) # Mel spectrogram input
# recorder.record(dummy_input)
# recorder.save("chatterbox_trace")
```

---

### Part 2: `equivcheck-rs` (The Rust Verifier)
This is the core of your "Moat." It needs to be fast, but highly descriptive when it fails.

**Cargo.toml dependencies:**
```toml
[dependencies]
candle-core = "0.3.1"
safetensors = "0.3.3"
serde_json = "1.0"
thiserror = "1.0"
colored = "2.0"
```

**The Implementation (`src/lib.rs`):**

```rust
use candle_core::{Tensor, Result, Error, Shape};
use std::collections::HashMap;
use std::path::Path;
use colored::*;

pub struct EquivChecker {
    golden_data: HashMap<String, Tensor>,
    metadata: HashMap<String, serde_json::Value>,
    pub tolerance: f32,
}

impl EquivChecker {
    pub fn load(path_prefix: &str, device: &candle_core::Device) -> Result<Self> {
        let tensor_path = format!("{}.safetensors", path_prefix);
        let meta_path = format!("{}_meta.json", path_prefix);

        let tensors = candle_core::safetensors::load(tensor_path, device)?;
        
        let meta_file = std::fs::File::open(meta_path)
            .map_err(|e| Error::Msg(format!("Failed to load metadata: {}", e)))?;
        let metadata: HashMap<String, serde_json::Value> = serde_json::from_reader(meta_file)
            .map_err(|e| Error::Msg(format!("Failed to parse metadata: {}", e)))?;

        Ok(Self {
            golden_data: tensors,
            metadata,
            tolerance: 1e-4,
        })
    }

    /// The core comparison logic
    pub fn verify(&self, name: &str, actual: &Tensor) -> Result<()> {
        let key = format!("{}.out", name);
        let expected = self.golden_data.get(&key)
            .ok_or_else(|| Error::Msg(format!("Layer '{}' not found in golden record. Available: {:?}", key, self.golden_data.keys().collect::<Vec<_>>())))?;

        // 1. Check Shape
        if actual.shape() != expected.shape() {
            self.report_shape_mismatch(name, actual.shape(), expected.shape());
            return Err(Error::ShapeMismatchBinary(actual.shape().clone(), expected.shape().clone()));
        }

        // 2. Check Values
        let diff = (actual - expected)?.sqr()?.mean_all()?.to_scalar::<f32>()?;
        
        if diff > self.tolerance {
            self.report_value_mismatch(name, diff, actual, expected);
            panic!("Parity check failed for layer '{}'", name);
        }

        println!("{} Layer '{}' passed (MSE: {:.8})", "‚úî".green(), name, diff);
        Ok(())
    }

    fn report_shape_mismatch(&self, name: &str, actual: &Shape, expected: &Shape) {
        println!("\n{}", "--- SHAPE MISMATCH DETECTED ---".red().bold());
        println!("Layer: {}", name.yellow());
        println!("Rust Shape:    {:?}", actual);
        println!("Python Shape:  {:?}", expected);
        
        // Intelligence: Suggest transposes
        if actual.dims().len() == 3 && expected.dims().len() == 3 {
            if actual.dims()[1] == expected.dims()[2] && actual.dims()[2] == expected.dims()[1] {
                println!("{}", "üí° Suggestion: It looks like a (B, C, T) vs (B, T, C) swap. Try transposing dims 1 and 2.".cyan());
            }
        }
        println!("{}\n", "-------------------------------".red());
    }

    fn report_value_mismatch(&self, name: &str, mse: f32, actual: &Tensor, expected: &Tensor) {
        println!("\n{}", "--- VALUE DRIFT DETECTED ---".red().bold());
        println!("Layer: {}", name.yellow());
        println!("MSE:   {:.8}", mse);
        
        // Statistical Analysis
        let actual_mean = actual.mean_all().unwrap().to_scalar::<f32>().unwrap();
        let expect_mean = expected.mean_all().unwrap().to_scalar::<f32>().unwrap();
        println!("Mean (Rust):   {:.6}", actual_mean);
        println!("Mean (Py):     {:.6}", expect_mean);

        if (actual_mean - expect_mean).abs() < 0.001 && mse > 0.1 {
            println!("{}", "üí° Suggestion: Means are similar but MSE is high. This often happens if the weights are loaded correctly but the input order is shuffled.".cyan());
        }
        println!("{}\n", "----------------------------".red());
    }
}

/// Macro for clean integration in model code
#[macro_export]
macro_rules! verify_parity {
    ($checker:expr, $name:expr, $tensor:expr) => {
        if cfg!(debug_assertions) {
            $checker.verify($name, $tensor).expect("Parity check failed");
        }
    };
}
```

---

### Part 3: The "Audio-Specific" Integration Logic
TTS models like **Chatterbox Turbo** or **Parakeet** rely heavily on `Conv1d` and `Attention`. Here is how you use `EquivCheck` inside a Candle implementation of a Transformer block.

```rust
// Inside your model.rs
pub struct TransformerBlock {
    ln_1: LayerNorm,
    attn: MultiHeadAttention,
    // ...
}

impl TransformerBlock {
    pub fn forward(&self, x: &Tensor, checker: &Option<EquivChecker>) -> Result<Tensor> {
        let residual = x;
        
        // Verify input to the block
        if let Some(c) = checker { 
            c.verify("blocks.0.ln_1.input", x)?; 
        }

        let mut x = self.ln_1.forward(x)?;
        if let Some(c) = checker { c.verify("blocks.0.ln_1", &x)?; }

        x = self.attn.forward(&x)?;
        if let Some(c) = checker { c.verify("blocks.0.attn", &x)?; }

        x = (x + residual)?;
        
        Ok(x)
    }
}
```

---

### Part 4: Why this handles SOTA TTS (Chatterbox/Parakeet)

1.  **The "Zero-Shot" Problem:** Models like Chatterbox use a "Conditioning" signal (the voice prompt). If your Rust code doesn't concatenate the prompt vector exactly like Python does, the whole model fails. By wrapping the `torch.cat` operation in a hook, `EquivCheck` will tell you: *"Rust input to 'concat' is [1, 512], but Python expected [1, 1024]."*
2.  **LayerNorm Epsilon:** Audio models are sensitive to `LayerNorm`. PyTorch defaults to `1e-5`, but some TTS models use `1e-6`. If you get a mismatch, `EquivCheck`'s MSE report will show a tiny but consistent drift, signaling a parameter mismatch rather than a code logic bug.
3.  **The "Moat" Feature (Permutation Testing):** In the Rust code above, I added a "Suggestion" engine. As you port more models, you can add rules:
    *   If shapes match but values are inverted: "Check if your Activation function (ReLU/GelU) is applied *before* the add."
    *   If shapes are `[B, T, C]` vs `[B, C, T]`: "Check Conv1d weight layout."

### How to expand this to 10k lines:
*   **Automated Weight Porting:** Add a feature where `EquivCheck` reads the Python `state_dict` and automatically generates the Rust `VarMap` loading code.
*   **Visual Diff:** Create a small CLI that generates an HTML report showing the "Drift Graph" ‚Äî a chart showing how the error accumulates as you go deeper into the model.
*   **Audio Waveform Parity:** Specifically for TTS, add a "Final Output" check that compares the generated `.wav` files and calculates the **STFT Loss** between the Python and Rust outputs.

This MVP gives you the infrastructure to start porting **Soprano** or **Parakeet** today with a "verify-as-you-go" workflow.

---
---

This is the **EquivCheck Engineering Specification & Implementation**. I am providing the full core logic for both the Python "Golden Record" generator and the Rust "Parity Engine."

This code is designed to handle the high-dimensional complexity of SOTA audio models like **Chatterbox**, **Parakeet**, and **Soprano**, where temporal dimensions and channel layouts often conflict between PyTorch and Candle.

---

### Part 1: `equivcheck-py` (The Python Side)
This isn't just a script; it's a diagnostic suite. It handles nested modules, multiple inputs/outputs, and generates a "Structural Manifest" that tells the Rust side exactly what the model architecture looks like.

```python
"""
EQUIVCHECK-PY: The Golden Record Generator
Robust enough for SOTA TTS/STT (Chatterbox, Parakeet, etc.)
"""
import os
import json
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional, Union, Tuple
from safetensors.torch import save_file
from dataclasses import dataclass, asdict
from collections import defaultdict

@dataclass
class LayerMeta:
    name: str
    module_type: str
    input_shapes: List[List[int]]
    output_shapes: List[List[int]]
    parameters: List[str]
    is_leaf: bool

class EquivCheckProject:
    def __init__(self, name: str, output_dir: str = "equiv_trace"):
        self.name = name
        self.output_dir = output_dir
        self.records: Dict[str, torch.Tensor] = {}
        self.manifest: Dict[str, LayerMeta] = {}
        self.call_counts = defaultdict(int)
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def _tensor_to_cpu(self, t: Any) -> Optional[torch.Tensor]:
        if isinstance(t, torch.Tensor):
            return t.detach().float().cpu()
        return None

    def _extract_shapes(self, data: Any) -> List[List[int]]:
        shapes = []
        if isinstance(data, torch.Tensor):
            shapes.append(list(data.shape))
        elif isinstance(data, (tuple, list)):
            for item in data:
                if isinstance(item, torch.Tensor):
                    shapes.append(list(item.shape))
        return shapes

    def hook_factory(self, name: str, module: nn.Module):
        def hook(m, inp, out):
            # Handle multiple calls to the same layer (e.g., shared weights in Transformer)
            call_idx = self.call_counts[name]
            self.call_counts[name] += 1
            
            trace_key = f"{name}.{call_idx}" if call_idx > 0 else name
            
            # Record Inputs
            if isinstance(inp, tuple):
                for i, x in enumerate(inp):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.in.{i}"] = cpu_x
            
            # Record Outputs
            if isinstance(out, torch.Tensor):
                self.records[f"{trace_key}.out.0"] = self._tensor_to_cpu(out)
            elif isinstance(out, (tuple, list)):
                for i, x in enumerate(out):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.out.{i}"] = cpu_x

            # Record Metadata
            self.manifest[trace_key] = LayerMeta(
                name=name,
                module_type=str(type(m).__name__),
                input_shapes=self._extract_shapes(inp),
                output_shapes=self._extract_shapes(out),
                parameters=[n for n, _ in m.named_parameters(recurse=False)],
                is_leaf=len(list(m.children())) == 0
            )
        return hook

    def capture(self, model: nn.Module, *args, **kwargs):
        """
        The main entry point. Wraps the model, runs a forward pass, 
        and captures everything.
        """
        model.eval()
        hooks = []
        
        print(f"üöÄ EquivCheck: Instrumenting {self.name}...")
        for name, module in model.named_modules():
            # We skip the root to avoid redundant data
            if name == "": continue 
            hooks.append(module.register_forward_hook(self.hook_factory(name, module)))
        
        try:
            with torch.no_grad():
                output = model(*args, **kwargs)
            print(f"‚úÖ Forward pass complete. Captured {len(self.records)} tensors.")
        finally:
            for h in hooks:
                h.remove()
        
        self.save()
        return output

    def save(self):
        # Save Tensors
        tensor_path = os.path.join(self.output_dir, f"{self.name}_trace.safetensors")
        save_file(self.records, tensor_path)
        
        # Save Manifest
        manifest_path = os.path.join(self.output_dir, f"{self.name}_manifest.json")
        with open(manifest_path, "w") as f:
            json.dump({k: asdict(v) for k, v in self.manifest.items()}, f, indent=2)
            
        print(f"üíæ Trace saved to: {self.output_dir}")

# --- Example Usage for a SOTA STT Model (e.g. Parakeet) ---
# model = load_parakeet_model()
# project = EquivCheckProject("parakeet_encoder")
# dummy_input = torch.randn(1, 80, 1000) # (B, Mel, Time)
# project.capture(model, dummy_input)
```

---

### Part 2: `equivcheck-rs` (The Rust Side)
This is the core engine. It includes a **Permutation Engine** that guesses why your shapes are wrong and a **Statistical Validator** that differentiates between "Floating Point Noise" and "Actual Logic Bugs."

```rust
/* 
EQUIVCHECK-RS: The Parity Engine
Designed for Candle (Rust)
*/

use candle_core::{Tensor, Result, Error, Shape, DType, Device};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use serde::{Deserialize, Serialize};
use colored::*;

// --- Types & Data Structures ---

#[derive(Serialize, Deserialize, Debug)]
pub struct LayerMeta {
    pub name: String,
    pub module_type: String,
    pub input_shapes: Vec<Vec<usize>>,
    pub output_shapes: Vec<Vec<usize>>,
    pub parameters: Vec<String>,
    pub is_leaf: bool,
}

pub struct ComparisonResult {
    pub mse: f32,
    pub max_diff: f32,
    pub cosine_sim: f32,
    pub passed: bool,
}

pub struct EquivChecker {
    pub name: String,
    golden_tensors: HashMap<String, Tensor>,
    manifest: HashMap<String, LayerMeta>,
    pub atol: f32, // Absolute tolerance
    pub rtol: f32, // Relative tolerance
    pub device: Device,
}

// --- Implementation ---

impl EquivChecker {
    pub fn load<P: AsRef<Path>>(project_name: &str, base_path: P, device: &Device) -> Result<Self> {
        let base = base_path.as_ref();
        let tensor_path = base.join(format!("{}_trace.safetensors", project_name));
        let manifest_path = base.join(format!("{}_manifest.json", project_name));

        println!("{} Loading EquivCheck Project: {}", "üîç".blue(), project_name.bold());

        let golden_tensors = candle_core::safetensors::load(&tensor_path, device)?;
        
        let manifest_file = std::fs::read_to_string(&manifest_path)
            .map_err(|e| Error::Msg(format!("Failed to read manifest: {}", e)))?;
        let manifest: HashMap<String, LayerMeta> = serde_json::from_str(&manifest_file)
            .map_err(|e| Error::Msg(format!("Failed to parse manifest: {}", e)))?;

        Ok(Self {
            name: project_name.to_string(),
            golden_tensors,
            manifest,
            atol: 1e-5,
            rtol: 1e-5,
            device: device.clone(),
        })
    }

    /// Verifies a specific layer's output
    pub fn verify(&self, layer_name: &str, actual: &Tensor) -> Result<ComparisonResult> {
        let key = format!("{}.out.0", layer_name); // Default to first output
        
        let expected = self.golden_tensors.get(&key).ok_or_else(|| {
            Error::Msg(format!("Layer '{}' not found in trace. Did you name it correctly?", layer_name))
        })?;

        // 1. Shape Check Logic
        if actual.shape() != expected.shape() {
            self.diagnose_shape_mismatch(layer_name, actual.shape(), expected.shape());
            return Err(Error::ShapeMismatchBinary(actual.shape().clone(), expected.shape().clone()));
        }

        // 2. Numerical Comparison
        let result = self.compare_tensors(actual, expected)?;

        if result.mse > self.atol {
            self.report_failure(layer_name, &result, actual, expected);
            return Err(Error::Msg(format!("Numerical parity failed for {}", layer_name)));
        }

        println!(
            "{} Layer '{}' [{}] passed. (MSE: {:.2e}, CosSim: {:.4})", 
            "‚úî".green(), 
            layer_name.yellow(), 
            self.manifest.get(layer_name).map(|m| m.module_type.as_str()).unwrap_or("Unknown"),
            result.mse,
            result.cosine_sim
        );

        Ok(result)
    }

    fn compare_tensors(&self, a: &Tensor, b: &Tensor) -> Result<ComparisonResult> {
        // Mean Squared Error
        let diff = (a - b)?;
        let mse = diff.sqr()?.mean_all()?.to_scalar::<f32>()?;
        
        // Max Absolute Difference
        let max_diff = diff.abs()?.max_all()?.to_scalar::<f32>()?;

        // Cosine Similarity (Flatten to 1D)
        let a_flat = a.flatten_all()?;
        let b_flat = b.flatten_all()?;
        let dot = (&a_flat * &b_flat)?.sum_all()?.to_scalar::<f32>()?;
        let norm_a = a_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let norm_b = b_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let cosine_sim = dot / (norm_a * norm_b + 1e-8);

        Ok(ComparisonResult {
            mse,
            max_diff,
            cosine_sim,
            passed: mse <= self.atol,
        })
    }

    /// The "Intelligence" Engine: Analyzes why shapes don't match
    fn diagnose_shape_mismatch(&self, name: &str, actual: &Shape, expected: &Shape) {
        println!("\n{}", "‚ùå SHAPE MISMATCH DETECTED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  Rust:    {:?}", actual.dims());
        println!("  Python:  {:?}", expected.dims());

        let a_dims = actual.dims();
        let e_dims = expected.dims();

        // Check for common Transpose issues (B, C, T) vs (B, T, C)
        if a_dims.len() == 3 && e_dims.len() == 3 {
            if a_dims[1] == e_dims[2] && a_dims[2] == e_dims[1] {
                println!("{}", "üí° DIAGNOSIS: Dimension Swap. You are using (B, C, T) but Python used (B, T, C).".cyan());
                println!("{}", "   Fix: Use tensor.transpose(1, 2)? before returning.".cyan());
            }
        }

        // Check for Linear layer Weight Transpose (Common in Candle)
        if a_dims.len() == 2 && e_dims.len() == 2 {
            if a_dims[0] == e_dims[1] && a_dims[1] == e_dims[0] {
                println!("{}", "üí° DIAGNOSIS: Weight Transpose. Linear layers in Candle often expect weights as (out, in).".cyan());
            }
        }

        // Check for Batch dimension missing
        if a_dims.len() == e_dims.len() - 1 && &e_dims[1..] == a_dims {
            println!("{}", "üí° DIAGNOSIS: Missing Batch Dimension. Did you forget to unsqueeze(0)?".cyan());
        }

        println!("{}\n", "---------------------------".red());
    }

    fn report_failure(&self, name: &str, res: &ComparisonResult, actual: &Tensor, expected: &Tensor) {
        println!("\n{}", "‚ùå NUMERICAL PARITY FAILED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  MSE:      {:.8}", res.mse);
        println!("  Max Diff: {:.8}", res.max_diff);
        println!("  Cos Sim:  {:.8}", res.cosine_sim);

        let a_mean = actual.mean_all().unwrap().to_scalar::<f32>().unwrap();
        let e_mean = expected.mean_all().unwrap().to_scalar::<f32>().unwrap();
        println!("  Means:    Rust={:.6}, Py={:.6}", a_mean, e_mean);

        if res.cosine_sim > 0.99 && res.mse > self.atol {
            println!("{}", "üí° HINT: High Cosine Similarity but high MSE suggests a scaling issue or a different Epsilon in LayerNorm/BatchNorm.".cyan());
        }
        
        if res.cosine_sim < 0.1 {
            println!("{}", "üí° HINT: Very low Cosine Similarity. You are likely comparing completely different tensors or the weights were not loaded into this layer.".cyan());
        }
        println!("{}\n", "---------------------------".red());
    }
}

// --- Integration Macros ---

#[macro_export]
macro_rules! equiv_check {
    ($checker:expr, $name:expr, $tensor:expr) => {
        if let Some(ref c) = $checker {
            c.verify($name, $tensor).expect("Parity Check Failed");
        }
    };
}
```

---

### Part 3: Real-World "Audio Porting" Example
This shows how you use the tool to port a **Conformer Block** (used in Parakeet STT). This is where most devs fail because of the complex interaction between Attention and Depthwise Conv.

```rust
use candle_core::{Module, Tensor, Result};
use candle_nn::{LayerNorm, Linear, VarBuilder};

// Your tool helps you build this step-by-step
pub struct ConformerBlock {
    ln_before: LayerNorm,
    attn: MultiHeadAttention, // Assume implemented
    conv_module: ConvModule,  // Assume implemented
    ln_after: LayerNorm,
    checker: Option<EquivChecker>, // The Magic Tool
}

impl ConformerBlock {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // 1. Initial State Check
        equiv_check!(self.checker, "block.0.input", x);

        // 2. LayerNorm Parity
        let mut x_ln = self.ln_before.forward(x)?;
        equiv_check!(self.checker, "block.0.ln_before", &x_ln);

        // 3. Attention Parity
        // If this fails, you know the bug is in your Attention math, not the Conv
        let attn_out = self.attn.forward(&x_ln)?;
        equiv_check!(self.checker, "block.0.attn", &attn_out);
        
        let x = (attn_out + x)?;

        // 4. Conv Module (The hardest part of Parakeet)
        let x_res = x.clone();
        let mut x_conv = self.ln_after.forward(&x)?;
        
        // Audio Conv1d often requires (B, C, T). 
        // If you forget this, EquivCheck will catch the shape mismatch here!
        x_conv = x_conv.transpose(1, 2)?; 
        x_conv = self.conv_module.forward(&x_conv)?;
        x_conv = x_conv.transpose(1, 2)?;
        
        equiv_check!(self.checker, "block.0.conv_module", &x_conv);

        (x_conv + x_res)
    }
}
```

---

### Part 4: Why this is the "10k Line" Moat Logic

1.  **Non-Linear Error Propagation:** In audio models (especially TTS like Chatterbox), errors don't just add up; they compound exponentially through the vocoder. By having `equiv_check!` at every layer, you stop the "Silent Failure" where the model runs but the voice sounds like static.
2.  **The "Call Count" Logic:** Many SOTA models reuse the same layer (e.g., recursive loops or shared embeddings). My Python code handles this by tracking `call_counts`, ensuring the Rust side compares the $N^{th}$ pass of a layer with the $N^{th}$ pass in Python.
3.  **The Metadata Manifest:** Most tools just save tensors. `EquivCheck` saves the **Module Type**. If a `LayerNorm` is failing, the Rust tool can look at the manifest, see it's a `LayerNorm`, and automatically suggest checking the `eps` (epsilon) value.
4.  **Zero-Shot Voice Cloning Support:** For models like Chatterbox, you have a "Speaker Embedding." My tool captures the inputs to the concatenation layer, allowing you to verify that your speaker encoder is producing the same latent vector as the Python reference.

### How to use this right now:
1.  **Python:** Run `EquivCheckProject` on your PyTorch model. It spits out a `.safetensors` file and a `.json` manifest.
2.  **Rust:** Add `EquivChecker` to your model struct.
3.  **Iterate:** Write one layer of Rust code -> Run test -> `EquivCheck` tells you exactly what's wrong -> Fix it -> Move to next layer. 

**This turns a 2-week porting job into a 2-day job.**
</file>

<file path="ref3.txt">
Yes. In fact, doing this **without** an LLM is actually more reliable because it relies on **structural reflection** rather than probabilistic guessing. 

Since your Python "Spy" already iterates through the PyTorch model, we can extract the specific hyperparameters (kernel size, strides, features) and the weight names. We then use a **Template Engine** (like Jinja2 in Python or Tera in Rust) to spit out the Candle code.

Here is the "No-LLM" Codegen architecture.

---

### Phase 1: Enhance the Python "Spy" to capture Config
To generate code, we need more than just shapes; we need the constructor arguments. We‚Äôll add a `capture_config` method to the Python side.

```python
# Add this to your EquivCheckProject class in Python
def _get_module_config(self, module: nn.Module) -> Dict[str, Any]:
    cfg = {}
    if isinstance(module, nn.Linear):
        cfg = {"in_dims": module.in_features, "out_dims": module.out_features, "bias": module.bias is not None}
    elif isinstance(module, nn.Conv1d):
        cfg = {
            "in_channels": module.in_channels,
            "out_channels": module.out_channels,
            "kernel_size": module.kernel_size[0],
            "stride": module.stride[0],
            "padding": module.padding[0]
        }
    elif isinstance(module, nn.LayerNorm):
        cfg = {"dims": module.normalized_shape[0], "eps": module.eps}
    # Add more mappings for Audio models (Conv2d, GRU, etc.)
    return cfg
```

---

### Phase 2: The Codegen Engine (The "Compiler")
This script reads your `manifest.json` and writes a `model.rs` file. It treats the PyTorch model hierarchy as a tree.

```python
import json

class CandleCodegen:
    def __init__(self, manifest_path: str):
        with open(manifest_path, "r") as f:
            self.manifest = json.load(f)
        
    def map_type(self, py_type: str) -> str:
        mapping = {
            "Linear": "Linear",
            "Conv1d": "Conv1d",
            "LayerNorm": "LayerNorm",
            "Embedding": "Embedding",
            "Dropout": "Dropout",
        }
        return mapping.get(py_type, "UnknownModule")

    def generate_struct(self, model_name: str):
        lines = [f"pub struct {model_name} {{"]
        for layer_name, meta in self.manifest.items():
            if not meta['is_leaf']: continue # Only define leaf modules in the struct
            
            clean_name = layer_name.replace(".", "_")
            candle_type = self.map_type(meta['module_type'])
            lines.append(f"    pub {clean_name}: {candle_type},")
        
        # Always add the checker
        lines.append("    pub checker: Option<EquivChecker>,")
        lines.append("}")
        return "\n".join(lines)

    def generate_init(self, model_name: str):
        lines = [f"impl {model_name} {{"]
        lines.append(f"    pub fn load(vb: VarBuilder, checker: Option<EquivChecker>) -> Result<Self> {{")
        
        for layer_name, meta in self.manifest.items():
            if not meta['is_leaf']: continue
            
            clean_name = layer_name.replace(".", "_")
            c = meta.get('config', {})
            
            # Generate the specific Candle constructor
            if meta['module_type'] == "Linear":
                init = f"candle_nn::linear({c['in_dims']}, {c['out_dims']}, vb.pp(\"{layer_name}\"))?"
            elif meta['module_type'] == "Conv1d":
                init = f"candle_nn::conv1d({c['in_channels']}, {c['out_channels']}, {c['kernel_size']}, Default::default(), vb.pp(\"{layer_name}\"))?"
            elif meta['module_type'] == "LayerNorm":
                init = f"candle_nn::layer_norm({c['dims']}, {c['eps']}, vb.pp(\"{layer_name}\"))?"
            else:
                init = f"todo!(\"Implement {meta['module_type']}\")"
                
            lines.append(f"        let {clean_name} = {init};")

        lines.append("\n        Ok(Self {")
        for layer_name, meta in self.manifest.items():
            if meta['is_leaf']:
                lines.append(f"            {layer_name.replace('.', '_')},")
        lines.append("            checker,")
        lines.append("        })")
        lines.append("    }")
        return "\n".join(lines)

    def generate_forward(self):
        """
        Since we recorded the execution order in the manifest, 
        we can actually guess the forward pass order!
        """
        lines = ["    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {", "        let mut x = xs.clone();"]
        
        # Sort by call order (if you saved call_idx in the manifest)
        sorted_layers = sorted(self.manifest.items(), key=lambda x: x[0]) 
        
        for layer_name, meta in sorted_layers:
            if not meta['is_leaf']: continue
            clean_name = layer_name.replace(".", "_")
            
            lines.append(f"\n        // Layer: {layer_name}")
            lines.append(f"        x = self.{clean_name}.forward(&x)?;")
            lines.append(f"        equiv_check!(self.checker, \"{layer_name}\", &x);")
            
        lines.append("\n        Ok(x)")
        lines.append("    }")
        lines.append("}")
        return "\n".join(lines)

    def write_file(self, output_path: str):
        with open(output_path, "w") as f:
            f.write("use candle_core::{Tensor, Result, Device};\n")
            f.write("use candle_nn::{Linear, Conv1d, LayerNorm, VarBuilder, Module};\n")
            f.write("use crate::equiv_check; // Your macro\n\n")
            f.write(self.generate_struct("MyModel"))
            f.write("\n\n")
            f.write(self.generate_init("MyModel"))
            f.write("\n")
            f.write(self.generate_forward())

# Run it
# gen = CandleCodegen("equiv_trace/parakeet_manifest.json")
# gen.write_file("generated_model.rs")
```

---

### Why this is better than an LLM
1.  **Strict Accuracy:** It won't hallucinate a layer that doesn't exist. It only generates what it saw in the PyTorch memory.
2.  **Correct Paths:** It uses the exact string paths (`encoder.layers.0.self_attn`) that `VarBuilder` needs to find the weights in the `.safetensors` file.
3.  **Automatic Verification:** It automatically inserts the `equiv_check!` macro after every layer.

### The "Audio Model" Specialization
Audio models often have weird custom logic (like `STFT` or `SqueezeExcite`). To make this robust, you would:
1.  **Identify "Atomic" Blocks:** In your Python spy, if you see a `ConformerLayer`, you can choose to treat it as a "Leaf" or "Composite".
2.  **Template Library:** Maintain a library of Rust snippets for common audio patterns (e.g., the specific way `Wav2Vec2` does positional embeddings).

### The Final Workflow
1.  **Python Script:** `model.record(input) -> manifest.json`
2.  **Codegen Script:** `codegen.py manifest.json -> model.rs`
3.  **Rust Cargo:** `cargo test`
4.  **Result:** Your model is 80% written for you, and the 20% you have to write manually (the glue logic) is immediately verified by the `equiv_check` macro.

**This is a "Standardization Tool."** You are effectively creating a "Transpiler" for Neural Networks. If you build this, you don't just help people port models; you make it impossible for them to do it any other way.

---
---

This is your **Master SOP (Standard Operating Procedure)** and the **README** for the tool you are building. 

If you are approaching a repo like **Chatterbox Turbo** or **Parakeet**, you aren't just "reading code"‚Äîyou are performing a surgical extraction.

---

# 1. The SOP: How to Port a Model in 48 Hours

### Phase 1: The Python "Infiltration"
1.  **Clone & Environment:** Get the original PyTorch repo running. Ensure you can generate a single inference output (e.g., a `.wav` file).
2.  **Instrument the Model:** Wrap the top-level model in your `EquivCheckProject`.
3.  **The "Golden Run":** Run a single inference with a fixed seed and fixed input.
    *   *Why?* You need a deterministic "Trace" of every tensor.
4.  **Export Weights:** Save the model to `weights.safetensors`. (Use the `safetensors` library to ensure Candle can read them natively).
5.  **Generate the Manifest:** Run your `capture()` method to produce `manifest.json`.

### Phase 2: The Rust "Skeleton"
1.  **Run Codegen:** Feed `manifest.json` into your `CandleCodegen` script.
2.  **Initialize the Crate:** Create a new Candle project and drop the generated `model.rs` into it.
3.  **Weight Mapping:** Use `VarBuilder::from_safetensors`. Since your manifest uses the *exact* PyTorch keys, the weights will "just snap into place."

### Phase 3: The "Breadcrumb" Implementation (The Core Loop)
1.  **Start at Layer 0:** Run your Rust code. It will likely panic at the first `equiv_check!` call.
2.  **Fix the Shape:** If it's a shape mismatch, look at the "Diagnosis" output (e.g., "You used BCT, Python used BTC").
3.  **Fix the Math:** If it's a value mismatch, check your hyperparameters (e.g., "Is the LayerNorm epsilon $1e-5$ or $1e-6$?").
4.  **The "Green Light":** Once Layer 0 passes, move the `equiv_check!` call to Layer 1.
5.  **Repeat:** Do this until you reach the final output.

### Phase 4: The "Audio-Specific" Final Polish
1.  **Final Output Comparison:** Compare the final Rust tensor against the Python output.
2.  **Waveform Check:** Save the Rust output as a `.wav` and listen. If it sounds correct but the MSE is slightly high, it‚Äôs usually just floating-point accumulation (common in `f32` vs `bf16`).

---

# 2. The README (The Face of the Tool)

```markdown
# EquivCheck üõ°Ô∏è
> Stop guessing. Start asserting. The bridge between PyTorch and Candle.

Porting models from PyTorch to Candle (Rust) used to be a game of "Spot the Difference" played in the dark. **EquivCheck** turns it into Test-Driven Development.

## The Problem
You spend 4 hours porting a Conformer block. It runs. The output is a `[1, 80, 1000]` tensor. But the audio sounds like a demonic woodchipper. Where is the bug?
- Is it the `Conv1d` padding?
- Is it the `MultiHeadAttention` scaling factor?
- Is it a `transpose(1, 2)` you forgot?

## The Solution: EquivCheck
EquivCheck provides a two-way bridge to ensure bit-perfect parity at every single layer of your model.

### 1. The Spy (Python)
Drop a single hook into your PyTorch source. It records every input, output, and hyperparameter as the model executes.

```python
from equivcheck import EquivCheckProject

project = EquivCheckProject("ChatterboxTurbo")
project.capture(model, input_tensor) # Saves weights, trace, and manifest
```

### 2. The Architect (Codegen)
EquivCheck reads the PyTorch execution manifest and generates the Rust/Candle boilerplate for you. No more typing `candle_nn::linear(...)` 50 times.

```bash
python -m equivcheck.codegen --manifest chatterbox_manifest.json --out model.rs
```

### 3. The Judge (Rust)
In your Candle implementation, use the `equiv_check!` macro. It compares your Rust tensors against the recorded Python "Golden Record" in real-time.

```rust
let x = self.encoder_layer.forward(&x)?;
equiv_check!(self.checker, "encoder.layers.0", &x);
```

If it fails, you get a **Deep Diagnostic**:
```text
‚ùå NUMERICAL PARITY FAILED: 'encoder.layers.0'
MSE: 0.0421
Diagnosis: High Cosine Similarity (0.99) but high MSE. 
Hint: Check your LayerNorm epsilon or Activation function order.
```

## Features for SOTA Audio Models
- **Temporal Parity:** Specifically designed to handle the `(B, C, T)` vs `(B, T, C)` chaos of audio models like Parakeet and Soprano.
- **Stateful Tracking:** Handles recursive layers and shared weights by tracking call-counts.
- **No-LLM Codegen:** 100% reliable structural reflection. It builds the struct, the `load()` function, and the `forward()` pass based on reality, not a hallucination.

## Why this is a "Moat"
EquivCheck isn't just a library; it's the **Standard Operating Procedure** for moving models from Research (Python) to Production (Rust). It captures the "Integration Gap" that currently costs engineering teams weeks of high-seniority time.

## Installation
```bash
# Python
pip install equivcheck-py

# Rust
cargo add equivcheck-rs
```

---
Built for the Candle community. Stop the pain. Port with confidence.
```
---
</file>

<file path="specs/melspectrogram_parity.md">
# Walkthrough - MelSpectrogram Implementation

I have implemented and verified the `MelSpectrogram` operation in `pycandle-audio` with full parity against `torchaudio`.

## Changes

### `crates/pycandle-audio`

#### [lib.rs](file:///d:/pycandle/crates/pycandle-audio/src/lib.rs)
-   Implemented `MelSpectrogramConfig` with support for `HTK` and `Slaney` mel scales.
-   Implemented `MelNorm::Slaney` for area normalization.
-   Implemented `hz_to_mel` and `mel_to_hz` for both scales.
-   Implemented `get_mel_banks` to generate the Mel filterbank matrix.
-   Implemented `mel_spectrogram` (STFT + Power + Mel Filterbank).

## Verification Results

Verified against `torchaudio` using a Python script.

### Mel Filterbank Parity
-   **Max Diff:** `6.89e-08`
-   **Status:** ‚úÖ EXACT MATCH
-   this confirms the implementation of Slaney scale and area normalization is correct.

### End-to-End MelSpectrogram
-   **Max Diff:** `0.018` (approx `1e-5` relative error)
-   **Status:** ‚úÖ PASS (High Precision)
-   Differences are due to floating point variations between Rust's `realfft` and PyTorch's FFT backend (MKL/FFTW). The core logic is verified correct.

### STFT & Power
-   **STFT Max Diff:** `0.010`
-   **Power Max Diff:** `1.0` (approx `4e-6` relative error)
</file>

<file path="test_symbolic.py">
import torch
import torch.nn as nn
from spy import GoldenRecorder

class SmallModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(50257, 768)
        self.linear1 = nn.Linear(768, 2048)
        self.linear2 = nn.Linear(2048, 768)
        self.ln = nn.LayerNorm(768)

    def forward(self, x):
        x = self.embedding(x)
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        x = self.ln(x)
        return x

model = SmallModel()
recorder = GoldenRecorder(output_dir="test_trace")
# Embedding input: (batch, seq)
x = torch.randint(0, 50257, (1, 10))
recorder.record(model, x)
recorder.save("symbolic_test")
</file>

<file path="test_trace_hints/hints_test_manifest.json">
{
    "embedding": {
        "name": "embedding",
        "module_type": "Embedding",
        "input_shapes": [
            [
                1,
                10
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                64
            ]
        ],
        "parameters": [
            "weight"
        ],
        "is_leaf": true,
        "config": {
            "num_embeddings": 64,
            "embedding_dim": 64
        }
    },
    "linear": {
        "name": "linear",
        "module_type": "Linear",
        "input_shapes": [
            [
                1,
                10,
                64
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                64
            ]
        ],
        "parameters": [
            "weight",
            "bias"
        ],
        "is_leaf": true,
        "config": {
            "in_features": 64,
            "out_features": 64,
            "bias": true,
            "weight_shape": [
                64,
                64
            ]
        }
    },
    "_symbolic_hints": {
        "vocab_size": 64,
        "hidden_dim": 64
    }
}
</file>

<file path="test_trace/symbolic_test_manifest.json">
{
    "embedding": {
        "name": "embedding",
        "module_type": "Embedding",
        "input_shapes": [
            [
                1,
                10
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                768
            ]
        ],
        "parameters": [
            "weight"
        ],
        "is_leaf": true,
        "config": {
            "num_embeddings": 50257,
            "embedding_dim": 768
        }
    },
    "linear1": {
        "name": "linear1",
        "module_type": "Linear",
        "input_shapes": [
            [
                1,
                10,
                768
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                2048
            ]
        ],
        "parameters": [
            "weight",
            "bias"
        ],
        "is_leaf": true,
        "config": {
            "in_features": 768,
            "out_features": 2048,
            "bias": true,
            "weight_shape": [
                2048,
                768
            ]
        }
    },
    "linear2": {
        "name": "linear2",
        "module_type": "Linear",
        "input_shapes": [
            [
                1,
                10,
                2048
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                768
            ]
        ],
        "parameters": [
            "weight",
            "bias"
        ],
        "is_leaf": true,
        "config": {
            "in_features": 2048,
            "out_features": 768,
            "bias": true,
            "weight_shape": [
                768,
                2048
            ]
        }
    },
    "ln": {
        "name": "ln",
        "module_type": "LayerNorm",
        "input_shapes": [
            [
                1,
                10,
                768
            ]
        ],
        "output_shapes": [
            [
                1,
                10,
                768
            ]
        ],
        "parameters": [
            "weight",
            "bias"
        ],
        "is_leaf": true,
        "config": {
            "normalized_shape": [
                768
            ],
            "eps": 1e-05
        }
    }
}
</file>

<file path="tts_turbo_temp.py">
import os
import math
from dataclasses import dataclass
from pathlib import Path

import librosa
import torch
import perth
import pyloudnorm as ln

from safetensors.torch import load_file
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer

from .models.t3 import T3
from .models.s3tokenizer import S3_SR
from .models.s3gen import S3GEN_SR, S3Gen
from .models.tokenizers import EnTokenizer
from .models.voice_encoder import VoiceEncoder
from .models.t3.modules.cond_enc import T3Cond
from .models.t3.modules.t3_config import T3Config
from .models.s3gen.const import S3GEN_SIL
import logging
logger = logging.getLogger(__name__)

REPO_ID = "ResembleAI/chatterbox-turbo"


def punc_norm(text: str) -> str:
    """
        Quick cleanup func for punctuation from LLMs or
        containing chars not seen often in the dataset
    """
    if len(text) == 0:
        return "You need to add some text for me to talk."

    # Capitalise first letter
    if text[0].islower():
        text = text[0].upper() + text[1:]

    # Remove multiple space chars
    text = " ".join(text.split())

    # Replace uncommon/llm punc
    punc_to_replace = [
        ("‚Ä¶", ", "),
        (":", ","),
        ("‚Äî", "-"),
        ("‚Äì", "-"),
        (" ,", ","),
        ("‚Äú", "\""),
        ("‚Äù", "\""),
        ("‚Äò", "'"),
        ("‚Äô", "'"),
    ]
    for old_char_sequence, new_char in punc_to_replace:
        text = text.replace(old_char_sequence, new_char)

    # Add full stop if no ending punc
    text = text.rstrip(" ")
    sentence_enders = {".", "!", "?", "-", ","}
    if not any(text.endswith(p) for p in sentence_enders):
        text += "."

    return text


@dataclass
class Conditionals:
    """
    Conditionals for T3 and S3Gen
    - T3 conditionals:
        - speaker_emb
        - clap_emb
        - cond_prompt_speech_tokens
        - cond_prompt_speech_emb
        - emotion_adv
    - S3Gen conditionals:
        - prompt_token
        - prompt_token_len
        - prompt_feat
        - prompt_feat_len
        - embedding
    """
    t3: T3Cond
    gen: dict

    def to(self, device):
        self.t3 = self.t3.to(device=device)
        for k, v in self.gen.items():
            if torch.is_tensor(v):
                self.gen[k] = v.to(device=device)
        return self

    def save(self, fpath: Path):
        arg_dict = dict(
            t3=self.t3.__dict__,
            gen=self.gen
        )
        torch.save(arg_dict, fpath)

    @classmethod
    def load(cls, fpath, map_location="cpu"):
        if isinstance(map_location, str):
            map_location = torch.device(map_location)
        kwargs = torch.load(fpath, map_location=map_location, weights_only=True)
        return cls(T3Cond(**kwargs['t3']), kwargs['gen'])


class ChatterboxTurboTTS:
    ENC_COND_LEN = 15 * S3_SR
    DEC_COND_LEN = 10 * S3GEN_SR

    def __init__(
        self,
        t3: T3,
        s3gen: S3Gen,
        ve: VoiceEncoder,
        tokenizer: EnTokenizer,
        device: str,
        conds: Conditionals = None,
    ):
        self.sr = S3GEN_SR  # sample rate of synthesized audio
        self.t3 = t3
        self.s3gen = s3gen
        self.ve = ve
        self.tokenizer = tokenizer
        self.device = device
        self.conds = conds
        self.watermarker = perth.PerthImplicitWatermarker()

    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'ChatterboxTurboTTS':
        ckpt_dir = Path(ckpt_dir)

        # Always load to CPU first for non-CUDA devices to handle CUDA-saved models
        if device in ["cpu", "mps"]:
            map_location = torch.device('cpu')
        else:
            map_location = None

        ve = VoiceEncoder()
        ve.load_state_dict(
            load_file(ckpt_dir / "ve.safetensors")
        )
        ve.to(device).eval()

        # Turbo specific hp
        hp = T3Config(text_tokens_dict_size=50276)
        hp.llama_config_name = "GPT2_medium"
        hp.speech_tokens_dict_size = 6563
        hp.input_pos_emb = None
        hp.speech_cond_prompt_len = 375
        hp.use_perceiver_resampler = False
        hp.emotion_adv = False

        t3 = T3(hp)
        t3_state = load_file(ckpt_dir / "t3_turbo_v1.safetensors")
        if "model" in t3_state.keys():
            t3_state = t3_state["model"][0]
        t3.load_state_dict(t3_state)
        del t3.tfmr.wte
        t3.to(device).eval()

        s3gen = S3Gen(meanflow=True)
        weights = load_file(ckpt_dir / "s3gen_meanflow.safetensors")
        s3gen.load_state_dict(
            weights, strict=True
        )
        s3gen.to(device).eval()

        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if len(tokenizer) != 50276:
            print(f"WARNING: Tokenizer len {len(tokenizer)} != 50276")

        conds = None
        builtin_voice = ckpt_dir / "conds.pt"
        if builtin_voice.exists():
            conds = Conditionals.load(builtin_voice, map_location=map_location).to(device)

        return cls(t3, s3gen, ve, tokenizer, device, conds=conds)

    @classmethod
    def from_pretrained(cls, device) -> 'ChatterboxTurboTTS':
        # Check if MPS is available on macOS
        if device == "mps" and not torch.backends.mps.is_available():
            if not torch.backends.mps.is_built():
                print("MPS not available because the current PyTorch install was not built with MPS enabled.")
            else:
                print("MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.")
            device = "cpu"

        local_path = snapshot_download(
            repo_id=REPO_ID,
            token=os.getenv("HF_TOKEN") or True,
            # Optional: Filter to download only what you need
            allow_patterns=["*.safetensors", "*.json", "*.txt", "*.pt", "*.model"]
        )

        return cls.from_local(local_path, device)

    def norm_loudness(self, wav, sr, target_lufs=-27):
        try:
            meter = ln.Meter(sr)
            loudness = meter.integrated_loudness(wav)
            gain_db = target_lufs - loudness
            gain_linear = 10.0 ** (gain_db / 20.0)
            if math.isfinite(gain_linear) and gain_linear > 0.0:
                wav = wav * gain_linear
        except Exception as e:
            print(f"Warning: Error in norm_loudness, skipping: {e}")

        return wav

    def prepare_conditionals(self, wav_fpath, exaggeration=0.5, norm_loudness=True):
        ## Load and norm reference wav
        s3gen_ref_wav, _sr = librosa.load(wav_fpath, sr=S3GEN_SR)

        assert len(s3gen_ref_wav) / _sr > 5.0, "Audio prompt must be longer than 5 seconds!"

        if norm_loudness:
            s3gen_ref_wav = self.norm_loudness(s3gen_ref_wav, _sr)

        ref_16k_wav = librosa.resample(s3gen_ref_wav, orig_sr=S3GEN_SR, target_sr=S3_SR)

        s3gen_ref_wav = s3gen_ref_wav[:self.DEC_COND_LEN]
        s3gen_ref_dict = self.s3gen.embed_ref(s3gen_ref_wav, S3GEN_SR, device=self.device)

        # Speech cond prompt tokens
        if plen := self.t3.hp.speech_cond_prompt_len:
            s3_tokzr = self.s3gen.tokenizer
            t3_cond_prompt_tokens, _ = s3_tokzr.forward([ref_16k_wav[:self.ENC_COND_LEN]], max_len=plen)
            t3_cond_prompt_tokens = torch.atleast_2d(t3_cond_prompt_tokens).to(self.device)

        # Voice-encoder speaker embedding
        ve_embed = torch.from_numpy(self.ve.embeds_from_wavs([ref_16k_wav], sample_rate=S3_SR))
        ve_embed = ve_embed.mean(axis=0, keepdim=True).to(self.device)

        t3_cond = T3Cond(
            speaker_emb=ve_embed,
            cond_prompt_speech_tokens=t3_cond_prompt_tokens,
            emotion_adv=exaggeration * torch.ones(1, 1, 1),
        ).to(device=self.device)
        self.conds = Conditionals(t3_cond, s3gen_ref_dict)

    def generate(
        self,
        text,
        repetition_penalty=1.2,
        min_p=0.00,
        top_p=0.95,
        audio_prompt_path=None,
        exaggeration=0.0,
        cfg_weight=0.0,
        temperature=0.8,
        top_k=1000,
        norm_loudness=True,
    ):
        if audio_prompt_path:
            self.prepare_conditionals(audio_prompt_path, exaggeration=exaggeration, norm_loudness=norm_loudness)
        else:
            assert self.conds is not None, "Please `prepare_conditionals` first or specify `audio_prompt_path`"

        if cfg_weight > 0.0 or exaggeration > 0.0 or min_p > 0.0:
            logger.warning("CFG, min_p and exaggeration are not supported by Turbo version and will be ignored.")

        # Norm and tokenize text
        text = punc_norm(text)
        text_tokens = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        text_tokens = text_tokens.input_ids.to(self.device)

        speech_tokens = self.t3.inference_turbo(
            t3_cond=self.conds.t3,
            text_tokens=text_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )

        # Remove OOV tokens and add silence to end
        speech_tokens = speech_tokens[speech_tokens < 6561]
        speech_tokens = speech_tokens.to(self.device)
        silence = torch.tensor([S3GEN_SIL, S3GEN_SIL, S3GEN_SIL]).long().to(self.device)
        speech_tokens = torch.cat([speech_tokens, silence])

        wav, _ = self.s3gen.inference(
            speech_tokens=speech_tokens,
            ref_dict=self.conds.gen,
            n_cfm_timesteps=2,
        )
        wav = wav.squeeze(0).detach().cpu().numpy()
        watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
        return torch.from_numpy(watermarked_wav).unsqueeze(0)
</file>

<file path="verification_results.jsonl">
{"name":"layer1.out.0", "mse":1e-10, "max_diff":1e-5, "cosine_sim":1.0, "passed":true}
{"name":"layer2.out.0", "mse":1e-8, "max_diff":1e-4, "cosine_sim":0.9999, "passed":true}
{"name":"layer3.out.0", "mse":0.05, "max_diff":0.1, "cosine_sim":0.95, "passed":false}
{"name":"layer4.out.0", "mse":0.2, "max_diff":0.5, "cosine_sim":0.8, "passed":false}
</file>

<file path="crates/pycandle-audio/Cargo.toml">
[package]
name = "pycandle-audio"
description = "Audio ops for PyCandle: STFT, iSTFT, and padding with PyTorch parity"
version.workspace = true
edition.workspace = true
license.workspace = true

[dependencies]
candle-core.workspace = true
candle-nn.workspace = true
thiserror.workspace = true
realfft.workspace = true
num-complex.workspace = true
</file>

<file path="crates/pycandle-core/Cargo.toml">
[package]
name = "pycandle-core"
description = "Core library for PyCandle: PyChecker, layers, and codegen"
version.workspace = true
edition.workspace = true
license.workspace = true

[dependencies]
candle-core.workspace = true
candle-nn.workspace = true
safetensors.workspace = true
regex.workspace = true
serde.workspace = true
anyhow.workspace = true
serde_json.workspace = true
colored.workspace = true
thiserror.workspace = true
</file>

<file path="crates/pycandle-core/src/checker.rs">
//! PyChecker - Golden tensor comparison for parity verification

use candle_core::{Device, Error, Result, Shape, Tensor};
use colored::*;
use serde::{Deserialize, Serialize};
use std::cell::RefCell;
use std::collections::HashMap;
use std::fs::OpenOptions;
use std::io::Write;
use std::path::Path;

/// Mode for verification: Strict (panic on failure) or DriftTracking (record and continue)
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum VerificationMode {
    Strict,
    DriftTracking,
}

/// Metadata for a recorded layer
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct LayerMeta {
    pub name: String,
    pub module_type: String,
    pub input_shapes: Vec<Vec<usize>>,
    pub output_shapes: Vec<Vec<usize>>,
    pub parameters: Vec<String>,
    pub is_leaf: bool,
    pub config: serde_json::Value,
}

/// Result of comparing two tensors
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ComparisonResult {
    pub name: String,
    pub mse: f32,
    pub max_diff: f32,
    pub cosine_sim: f32,
    pub passed: bool,
    pub heatmap: Option<Vec<f32>>,
}

/// PyChecker loads golden tensors and verifies Rust outputs against them
pub struct PyChecker {
    pub name: String,
    golden_tensors: HashMap<String, Tensor>,
    pub manifest: HashMap<String, LayerMeta>,
    pub atol: f32,
    pub rtol: f32,
    pub device: Device,
    pub mode: VerificationMode,
    history: RefCell<Vec<ComparisonResult>>,
}

impl PyChecker {
    /// Load golden records from safetensors and manifest JSON
    pub fn load<P: AsRef<Path>>(project_name: &str, base_path: P, device: &Device) -> Result<Self> {
        let base = base_path.as_ref();
        let tensor_path = base.join(format!("{}_trace.safetensors", project_name));
        let manifest_path = base.join(format!("{}_manifest.json", project_name));

        let golden_tensors = candle_core::safetensors::load(&tensor_path, device)?;

        let manifest_file = std::fs::read_to_string(&manifest_path)
            .map_err(|e| Error::Msg(format!("Failed to read manifest: {}", e)))?;
        let manifest: HashMap<String, LayerMeta> = serde_json::from_str(&manifest_file)
            .map_err(|e| Error::Msg(format!("Failed to parse manifest: {}", e)))?;

        Ok(Self {
            name: project_name.to_string(),
            golden_tensors,
            manifest,
            atol: 1e-4,
            rtol: 1e-4,
            device: device.clone(),
            mode: VerificationMode::Strict,
            history: RefCell::new(Vec::new()),
        })
    }

    /// Set verification mode
    pub fn with_mode(mut self, mode: VerificationMode) -> Self {
        self.mode = mode;
        self
    }

    /// Verify a tensor against the golden record for a layer
    pub fn verify(&self, layer_name: &str, actual: &Tensor) -> Result<ComparisonResult> {
        let key = format!("{}.out.0", layer_name);
        let expected = self.golden_tensors.get(&key).ok_or_else(|| {
            Error::Msg(format!(
                "Layer '{}' not found in trace. Available: {:?}",
                layer_name,
                self.golden_tensors.keys().take(5).collect::<Vec<_>>()
            ))
        })?;

        if actual.shape() != expected.shape() {
            self.diagnose_shape_mismatch(layer_name, actual.shape(), expected.shape());
            return Err(Error::Msg(format!(
                "Shape mismatch for {}: actual {:?}, expected {:?}",
                layer_name,
                actual.shape(),
                expected.shape()
            )));
        }

        let mut result = self.compare_tensors(actual, expected)?;
        result.name = layer_name.to_string();

        if result.mse > self.atol {
            // Compute heatmap for dashboard
            result.heatmap = self.compute_heatmap(actual, expected);

            // In Strict mode, we fail immediately
            if self.mode == VerificationMode::Strict {
                self.report_failure(layer_name, &result, actual, expected);
                self.log_result(&result);
                return Err(Error::Msg(format!(
                    "Numerical parity failed for {}",
                    layer_name
                )));
            } else {
                // In DriftTracking mode, we warn but continue
                println!(
                    "{} Layer '{}' drifted. (MSE: {:.2e})",
                    "‚ö†".yellow(),
                    layer_name.yellow(),
                    result.mse
                );
            }
        } else {
            println!(
                "{} Layer '{}' passed. (MSE: {:.2e}, CosSim: {:.4})",
                "‚úî".green(),
                layer_name.yellow(),
                result.mse,
                result.cosine_sim
            );
        }

        self.log_result(&result);
        self.history.borrow_mut().push(result.clone());
        Ok(result)
    }

    /// Print a report of the most sensitive layers (highest drift)
    pub fn print_drift_report(&self) {
        let history = self.history.borrow();
        if history.is_empty() {
            return;
        }

        println!("\n{}", "üìâ QUANTIZATION DRIFT REPORT".blue().bold());
        println!("{:<40} | {:<12} | {:<12}", "Layer", "MSE", "Status");
        println!("{}", "-".repeat(70));

        let mut sorted_history = history.clone();
        sorted_history.sort_by(|a, b| b.mse.partial_cmp(&a.mse).unwrap());

        for res in sorted_history.iter().take(20) {
            let status = if res.mse > self.atol {
                "DRIFT".red()
            } else {
                "OK".green()
            };
            println!("{:<40} | {:.2e}   | {}", res.name, res.mse, status);
        }
        println!("\n");
    }

    fn log_result(&self, result: &ComparisonResult) {
        if let Ok(json) = serde_json::to_string(result) {
            if let Ok(mut file) = OpenOptions::new()
                .create(true)
                .append(true)
                .open("verification_results.jsonl")
            {
                let _ = writeln!(file, "{}", json);
            }
        }
    }

    fn compare_tensors(&self, a: &Tensor, b: &Tensor) -> Result<ComparisonResult> {
        let diff = (a - b)?;
        let mse = diff.sqr()?.mean_all()?.to_scalar::<f32>()?;
        let max_diff = diff.abs()?.max_all()?.to_scalar::<f32>()?;

        let a_flat = a.flatten_all()?;
        let b_flat = b.flatten_all()?;
        let dot = (&a_flat * &b_flat)?.sum_all()?.to_scalar::<f32>()?;
        let norm_a = a_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let norm_b = b_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let cosine_sim = dot / (norm_a * norm_b + 1e-8);

        Ok(ComparisonResult {
            name: "unknown".to_string(), // Will be overwritten by caller
            mse,
            max_diff,
            cosine_sim,
            passed: mse <= self.atol,
            heatmap: None,
        })
    }

    fn diagnose_shape_mismatch(&self, name: &str, actual: &Shape, expected: &Shape) {
        println!("\n{}", "‚ùå SHAPE MISMATCH DETECTED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  Rust:   {:?}", actual.dims());
        println!("  Python: {:?}", expected.dims());

        let a_dims = actual.dims();
        let e_dims = expected.dims();

        if a_dims.len() == 3 && e_dims.len() == 3 {
            if a_dims[1] == e_dims[2] && a_dims[2] == e_dims[1] {
                println!(
                    "{}",
                    "üí° DIAGNOSIS: Dimension Swap. (B, C, T) vs (B, T, C). Try .transpose(1, 2)?"
                        .cyan()
                );
            }
        }
        println!("{}\n", "---------------------------".red());
    }

    fn report_failure(
        &self,
        name: &str,
        res: &ComparisonResult,
        actual: &Tensor,
        expected: &Tensor,
    ) {
        println!("\n{}", "‚ùå NUMERICAL PARITY FAILED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  MSE:      {:.8}", res.mse);
        println!("  Cos Sim:  {:.8}", res.cosine_sim);

        let a_mean = actual.mean_all().unwrap().to_scalar::<f32>().unwrap_or(0.0);
        let e_mean = expected
            .mean_all()
            .unwrap()
            .to_scalar::<f32>()
            .unwrap_or(0.0);
        println!("  Means:    Rust={:.6}, Py={:.6}", a_mean, e_mean);

        // --- Active Debugging Artifacts ---
        let failures_dir = Path::new("failures");
        if !failures_dir.exists() {
            let _ = std::fs::create_dir(failures_dir);
        }

        // 1. Save Tensor Snippet (.safetensors)
        let snippet_path = failures_dir.join(format!("{}.safetensors", name));
        let tensors_to_save = HashMap::from([
            ("rust_actual".to_string(), actual.clone()),
            ("py_golden".to_string(), expected.clone()),
        ]);
        if let Err(e) = candle_core::safetensors::save(&tensors_to_save, &snippet_path) {
            println!("  Failed to save snippet: {}", e);
        } else {
            println!(
                "  üíæ Snippet saved: {}",
                snippet_path.display().to_string().cyan()
            );
        }

        // 2. Generate Python Analysis Script
        let script_path = failures_dir.join(format!("debug_{}.py", name));
        let script_content = format!(
            r#"
import torch
from safetensors.torch import load_file
import matplotlib.pyplot as plt
import numpy as np

def analyze():
    print(f"üîç Analyzing Failure: {{'{name}'}}")
    tensors = load_file("{filename}")
    rust = tensors["rust_actual"]
    gold = tensors["py_golden"]

    diff = (rust - gold).abs()
    print(f"  Max Diff: {{diff.max().item():.6f}}")
    print(f"  MSE:      {{(diff ** 2).mean().item():.8f}}")

    # Plot
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.title("Rust Tensor Histogram")
    plt.hist(rust.flatten().float().numpy(), bins=50, alpha=0.7, label='Rust')
    plt.hist(gold.flatten().float().numpy(), bins=50, alpha=0.7, label='Gold')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.title("Difference Heatmap (First Slice)")
    if diff.ndim > 1:
        plt.imshow(diff.flatten(0, -2)[0].float().numpy(), cmap='hot', aspect='auto')
    else:
        plt.plot(diff.float().numpy())
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    analyze()
"#,
            name = name,
            filename = format!("{}.safetensors", name)
        );

        if let Ok(mut file) = std::fs::File::create(&script_path) {
            let _ = file.write_all(script_content.as_bytes());
            println!(
                "  üêç Script generated: {}",
                script_path.display().to_string().cyan()
            );
        }

        println!("{}\n", "---------------------------".red());
    }

    pub fn compute_heatmap(&self, a: &Tensor, b: &Tensor) -> Option<Vec<f32>> {
        // Compute absolute difference
        let diff = (a - b).ok()?.abs().ok()?;

        // 1. Flatten to 1D
        let flat = diff.flatten_all().ok()?;
        let numel = flat.elem_count();

        // 2. We want an 8x8 grid = 64 buckets
        let grid_size = 64;
        if numel < grid_size {
            return None; // Too small to pool usefuly
        }

        let chunk_size = numel / grid_size;

        // 3. Simple max pooling into buckets
        // Walking through the tensor on CPU is slow for huge tensors, but this is a failure case anyway.
        // A faster way is to reshape (64, chunk_size) and max(dim=1).

        let reshaped = flat
            .narrow(0, 0, grid_size * chunk_size)
            .ok()?
            .reshape((grid_size, chunk_size))
            .ok()?;

        let pooled = reshaped.max(1).ok()?;

        // Convert to Vec<f32>
        pooled.to_vec1::<f32>().ok()
    }
}
</file>

<file path="crates/pycandle-core/src/codegen/gpt2.rs">
//! GPT2 code generation helpers
//!
//! This module provides codegen utilities for HuggingFace GPT2 models.

use crate::LayerMeta;

/// Configuration extracted from GPT2 manifest
#[derive(Debug, Clone)]
pub struct GptConfig {
    pub vocab_size: usize,
    pub context_length: usize,
    pub emb_dim: usize,
    pub n_heads: usize,
    pub n_layers: usize,
    pub drop_rate: f32,
}

impl Default for GptConfig {
    fn default() -> Self {
        Self {
            vocab_size: 50257,
            context_length: 1024,
            emb_dim: 768,
            n_heads: 12,
            n_layers: 12,
            drop_rate: 0.1,
        }
    }
}

/// Check if module type is a GPT2 variant
pub fn is_gpt2_type(module_type: &str) -> bool {
    matches!(
        module_type,
        "GPT2Model" | "GPT2LMHeadModel" | "GPT2Block" | "GPT2Attention" | "GPT2MLP"
    )
}

/// Map GPT2 Python type to Candle type
pub fn map_type(py_type: &str) -> Option<String> {
    match py_type {
        "GPT2Model" | "GPT2LMHeadModel" => Some("gpt2::GPTModel".to_string()),
        "GPT2Block" => Some("gpt2::TransformerBlock".to_string()),
        "GPT2Attention" => Some("gpt2::MultiHeadAttention".to_string()),
        "GPT2MLP" => Some("gpt2::FeedForward".to_string()),
        _ => None,
    }
}

/// Extract GPT config from layer metadata
pub fn extract_config(meta: &LayerMeta) -> GptConfig {
    GptConfig {
        vocab_size: meta
            .config
            .get("vocab_size")
            .and_then(|v| v.as_u64())
            .unwrap_or(50257) as usize,
        context_length: meta
            .config
            .get("n_positions")
            .and_then(|v| v.as_u64())
            .unwrap_or(1024) as usize,
        emb_dim: meta
            .config
            .get("n_embd")
            .and_then(|v| v.as_u64())
            .unwrap_or(768) as usize,
        n_heads: meta
            .config
            .get("n_head")
            .and_then(|v| v.as_u64())
            .unwrap_or(12) as usize,
        n_layers: meta
            .config
            .get("n_layer")
            .and_then(|v| v.as_u64())
            .unwrap_or(12) as usize,
        drop_rate: meta
            .config
            .get("resid_pdrop")
            .and_then(|v| v.as_f64())
            .unwrap_or(0.1) as f32,
    }
}

/// Generate initialization code for GPT2 layer types
pub fn generate_init(
    layer_name: &str,
    meta: &LayerMeta,
    symbolic_dims: &std::collections::HashMap<String, usize>,
) -> Option<String> {
    let render_dim = |val: usize, preferred: &str| -> String {
        if !preferred.is_empty() {
            if let Some(&v) = symbolic_dims.get(preferred) {
                if v == val {
                    return format!("cfg.{}", preferred);
                }
            }
        }
        for (name, &v) in symbolic_dims {
            if v == val {
                return format!("cfg.{}", name);
            }
        }
        val.to_string()
    };

    match meta.module_type.as_str() {
        "GPT2Model" | "GPT2LMHeadModel" => {
            let cfg = extract_config(meta);
            let vocab_size = render_dim(cfg.vocab_size, "vocab_size");
            let context_length = render_dim(cfg.context_length, "context_length");
            let emb_dim = render_dim(cfg.emb_dim, "hidden_dim");
            let n_heads = render_dim(cfg.n_heads, "n_head");
            let n_layers = render_dim(cfg.n_layers, "n_layers");

            Some(format!(
                r#"{{
                let gpt_cfg = gpt2::Config {{
                    vocab_size: {},
                    context_length: {},
                    emb_dim: {},
                    n_heads: {},
                    n_layers: {},
                    drop_rate: {:.1},
                    qkv_bias: false,
                }};
                gpt2::GPTModel::new(gpt_cfg, &vb.pp("{}"))?
            }}"#,
                vocab_size, context_length, emb_dim, n_heads, n_layers, cfg.drop_rate, layer_name
            ))
        }
        "GPT2Block" => Some(format!(
            "gpt2::TransformerBlock::new(gpt_cfg, &vb.pp(\"{}\"))?",
            layer_name
        )),
        "GPT2Attention" => {
            let dim_val = meta
                .config
                .get("n_embd")
                .and_then(|v| v.as_u64())
                .unwrap_or(768) as usize;
            let heads_val = meta
                .config
                .get("n_head")
                .and_then(|v| v.as_u64())
                .unwrap_or(12) as usize;
            let dim = render_dim(dim_val, "hidden_dim");
            let heads = render_dim(heads_val, "n_head");

            Some(format!(
                "gpt2::MultiHeadAttention::new({}, {}, 0.1, {}, false, &vb.pp(\"{}\"))?",
                dim, dim, heads, layer_name
            ))
        }
        "GPT2MLP" => Some(format!(
            "gpt2::FeedForward::new(gpt_cfg, &vb.pp(\"{}\"))?",
            layer_name
        )),
        _ => None,
    }
}
</file>

<file path="crates/pycandle-core/src/layers.rs">
//! Neural network layer implementations for Candle
//!
//! These provide PyTorch-compatible implementations that Candle doesn't have built-in.

use candle_nn::Module;

use candle_core::{IndexOp, Result, Tensor};

// ============================================================================
// Activation Functions
// ============================================================================

/// ReLU activation: max(0, x)
pub struct ReLU;
impl ReLU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.relu()
    }
}

/// GELU activation (Gaussian Error Linear Unit)
pub struct GELU;
impl GELU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.gelu_erf()
    }
}

/// Sigmoid activation: 1 / (1 + exp(-x))
pub struct Sigmoid;
impl Sigmoid {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        candle_nn::ops::sigmoid(x)
    }
}

/// Tanh activation
pub struct Tanh;
impl Tanh {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.tanh()
    }
}

/// ELU activation: x if x > 0, else alpha * (exp(x) - 1)
pub struct ELU {
    pub alpha: f64,
}
impl ELU {
    pub fn new(alpha: f64) -> Self {
        Self { alpha }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.elu(self.alpha)
    }
}

/// LeakyReLU activation: x if x > 0, else negative_slope * x
pub struct LeakyReLU {
    pub negative_slope: f64,
}
impl LeakyReLU {
    pub fn new(negative_slope: f64) -> Self {
        Self { negative_slope }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // leaky_relu: max(0, x) + negative_slope * min(0, x)
        let zeros = x.zeros_like()?;
        let pos = x.maximum(&zeros)?;
        let neg = x.minimum(&zeros)?;
        pos + (neg * self.negative_slope)?
    }
}

/// Snake activation: x + sin¬≤(Œ±x)/Œ±
/// Used in neural vocoders like BigVGAN
pub struct Snake {
    pub alpha: Tensor,
}
impl Snake {
    pub fn load(vb: candle_nn::VarBuilder, in_features: usize) -> Result<Self> {
        let alpha = vb.get((1, in_features, 1), "alpha")?;
        Ok(Self { alpha })
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, T), alpha: (1, C, 1)
        let ax = x.broadcast_mul(&self.alpha)?;
        let sin_ax = ax.sin()?;
        let sin_sq = sin_ax.sqr()?;
        x + sin_sq.broadcast_div(&self.alpha)?
    }
}

/// Mish activation: x * tanh(softplus(x))
pub struct Mish;
impl Mish {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Mish(x) = x * tanh(softplus(x))
        // softplus(x) = ln(1 + exp(x))
        let sp = (x.exp()? + 1.0)?.log()?;
        x.broadcast_mul(&sp.tanh()?)
    }
}

/// SiLU / Swish activation: x * sigmoid(x)
pub struct SiLU;
impl SiLU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x * candle_nn::ops::sigmoid(x)?
    }
}

// ============================================================================
// Normalization Layers
// ============================================================================

/// BatchNorm1d for inference (uses running statistics)
/// Input: (B, C, T) or (B, C)
pub struct BatchNorm1d {
    pub weight: Tensor, // gamma
    pub bias: Tensor,   // beta
    pub running_mean: Tensor,
    pub running_var: Tensor,
    pub eps: f64,
}

impl BatchNorm1d {
    pub fn load(vb: candle_nn::VarBuilder, num_features: usize) -> Result<Self> {
        let weight = vb.get((num_features,), "weight")?;
        let bias = vb.get((num_features,), "bias")?;
        let running_mean = vb.get((num_features,), "running_mean")?;
        let running_var = vb.get((num_features,), "running_var")?;
        Ok(Self {
            weight,
            bias,
            running_mean,
            running_var,
            eps: 1e-5,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, T) for 1d or (B, C)
        // Normalize: (x - mean) / sqrt(var + eps) * weight + bias
        let ndim = x.dims().len();
        let (mean, var, weight, bias) = if ndim == 3 {
            // (B, C, T) - unsqueeze to (1, C, 1)
            (
                self.running_mean.unsqueeze(0)?.unsqueeze(2)?,
                self.running_var.unsqueeze(0)?.unsqueeze(2)?,
                self.weight.unsqueeze(0)?.unsqueeze(2)?,
                self.bias.unsqueeze(0)?.unsqueeze(2)?,
            )
        } else {
            // (B, C) - unsqueeze to (1, C)
            (
                self.running_mean.unsqueeze(0)?,
                self.running_var.unsqueeze(0)?,
                self.weight.unsqueeze(0)?,
                self.bias.unsqueeze(0)?,
            )
        };

        let normalized = x
            .broadcast_sub(&mean)?
            .broadcast_div(&(var + self.eps)?.sqrt()?)?;
        normalized.broadcast_mul(&weight)?.broadcast_add(&bias)
    }
}

/// BatchNorm2d for inference (uses running statistics)
/// Input: (B, C, H, W)
pub struct BatchNorm2d {
    pub weight: Tensor,
    pub bias: Tensor,
    pub running_mean: Tensor,
    pub running_var: Tensor,
    pub eps: f64,
}

impl BatchNorm2d {
    pub fn load(vb: candle_nn::VarBuilder, num_features: usize) -> Result<Self> {
        let weight = vb.get((num_features,), "weight")?;
        let bias = vb.get((num_features,), "bias")?;
        let running_mean = vb.get((num_features,), "running_mean")?;
        let running_var = vb.get((num_features,), "running_var")?;
        Ok(Self {
            weight,
            bias,
            running_mean,
            running_var,
            eps: 1e-5,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, H, W)
        // Reshape stats to (1, C, 1, 1) for broadcasting
        let mean = self.running_mean.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let var = self.running_var.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let weight = self.weight.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let bias = self.bias.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;

        let normalized = x
            .broadcast_sub(&mean)?
            .broadcast_div(&(var + self.eps)?.sqrt()?)?;
        normalized.broadcast_mul(&weight)?.broadcast_add(&bias)
    }
}

// ============================================================================
// Recurrent Layers
// ============================================================================

/// LSTM layer (multi-layer, unidirectional)
/// Input: (B, T, input_size) if batch_first=true
/// Output: (output, (h_n, c_n))
pub struct LSTM {
    pub weight_ih: Vec<Tensor>, // One per layer: (4*hidden, input_size or hidden_size)
    pub weight_hh: Vec<Tensor>, // One per layer: (4*hidden, hidden_size)
    pub bias_ih: Vec<Tensor>,   // One per layer: (4*hidden,)
    pub bias_hh: Vec<Tensor>,   // One per layer: (4*hidden,)
    pub num_layers: usize,
    pub hidden_size: usize,
}

impl LSTM {
    pub fn load(
        vb: candle_nn::VarBuilder,
        input_size: usize,
        hidden_size: usize,
        num_layers: usize,
    ) -> Result<Self> {
        let mut weight_ih = Vec::new();
        let mut weight_hh = Vec::new();
        let mut bias_ih = Vec::new();
        let mut bias_hh = Vec::new();

        for layer in 0..num_layers {
            let in_size = if layer == 0 { input_size } else { hidden_size };
            weight_ih.push(vb.get((4 * hidden_size, in_size), &format!("weight_ih_l{}", layer))?);
            weight_hh.push(vb.get(
                (4 * hidden_size, hidden_size),
                &format!("weight_hh_l{}", layer),
            )?);
            bias_ih.push(vb.get((4 * hidden_size,), &format!("bias_ih_l{}", layer))?);
            bias_hh.push(vb.get((4 * hidden_size,), &format!("bias_hh_l{}", layer))?);
        }

        Ok(Self {
            weight_ih,
            weight_hh,
            bias_ih,
            bias_hh,
            num_layers,
            hidden_size,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<(Tensor, (Tensor, Tensor))> {
        // x: (B, T, input_size) assuming batch_first
        let (batch, seq_len, _) = x.dims3()?;
        let device = x.device();
        let dtype = x.dtype();

        let h = Tensor::zeros((self.num_layers, batch, self.hidden_size), dtype, device)?;
        let c = Tensor::zeros((self.num_layers, batch, self.hidden_size), dtype, device)?;
        let mut output = x.clone();

        for layer in 0..self.num_layers {
            let mut h_t = h.i(layer)?;
            let mut c_t = c.i(layer)?;
            let mut outputs = Vec::new();

            for t in 0..seq_len {
                let x_t = output.i((.., t, ..))?;

                // gates = x @ W_ih.T + h @ W_hh.T + b_ih + b_hh
                let gates = x_t
                    .matmul(&self.weight_ih[layer].t()?)?
                    .broadcast_add(&h_t.matmul(&self.weight_hh[layer].t()?)?)?
                    .broadcast_add(&self.bias_ih[layer])?
                    .broadcast_add(&self.bias_hh[layer])?;

                // Split into i, f, g, o (each of size hidden_size)
                let chunks = gates.chunk(4, 1)?;
                let i_gate = candle_nn::ops::sigmoid(&chunks[0])?;
                let f_gate = candle_nn::ops::sigmoid(&chunks[1])?;
                let g_gate = chunks[2].tanh()?;
                let o_gate = candle_nn::ops::sigmoid(&chunks[3])?;

                c_t = f_gate
                    .broadcast_mul(&c_t)?
                    .broadcast_add(&i_gate.broadcast_mul(&g_gate)?)?;
                h_t = o_gate.broadcast_mul(&c_t.tanh()?)?;

                outputs.push(h_t.unsqueeze(1)?);
            }

            output = Tensor::cat(&outputs, 1)?;
        }

        Ok((output, (h, c)))
    }
}

// ============================================================================
// Specialized Layers
// ============================================================================

/// CausalConv1d: A 1D convolution with causal padding
/// Ensures that output at time t only depends on inputs at time <= t
pub struct CausalConv1d {
    pub conv: candle_nn::Conv1d,
    pub padding: usize,
}

impl CausalConv1d {
    pub fn load(
        vb: candle_nn::VarBuilder,
        in_channels: usize,
        out_channels: usize,
        kernel_size: usize,
        stride: usize,
        bias: bool,
    ) -> Result<Self> {
        let padding = kernel_size - 1;
        let config = candle_nn::Conv1dConfig {
            stride,
            padding,
            ..Default::default()
        };
        let conv = if bias {
            candle_nn::conv1d(in_channels, out_channels, kernel_size, config, vb)?
        } else {
            candle_nn::conv1d_no_bias(in_channels, out_channels, kernel_size, config, vb)?
        };
        Ok(Self { conv, padding })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.conv.forward(x)?;
        // Causal slice: remove the 'future' padding at the end
        if self.padding > 0 {
            let dim = x.dims().len() - 1;
            let seq_len = x.dim(dim)?;
            x.narrow(dim, 0, seq_len - self.padding)
        } else {
            Ok(x)
        }
    }
}

/// Dropout layer (inference no-op)
pub struct Dropout {
    pub p: f32,
}
impl Dropout {
    pub fn new() -> Self {
        Self { p: 0.5 }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        Ok(x.clone())
    }
}

/// Transpose layer (swaps two dimensions)
pub struct Transpose {
    pub dim0: usize,
    pub dim1: usize,
}
impl Transpose {
    pub fn new(dim0: usize, dim1: usize) -> Self {
        Self { dim0, dim1 }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.transpose(self.dim0, self.dim1)
    }
}

/// Sinusoidal Positional Embedding
pub struct SinusoidalPosEmb {
    pub dim: usize,
}
impl SinusoidalPosEmb {
    pub fn new(dim: usize) -> Self {
        Self { dim }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let half_dim = self.dim / 2;
        let device = x.device();
        let dtype = x.dtype();
        let inv_freq: Vec<_> = (0..half_dim)
            .map(|i| 1.0f32 / (10000.0f32.powf(i as f32 / (half_dim as f32 - 1.0f32))))
            .collect();
        let inv_freq = Tensor::from_vec(inv_freq, half_dim, device)?.to_dtype(dtype)?;
        let emb = x.unsqueeze(1)?.broadcast_mul(&inv_freq.unsqueeze(0)?)?;
        Tensor::cat(&[emb.sin()?, emb.cos()?], 1)
    }
}
</file>

<file path="crates/pycandle/src/report.rs">
// Report generation for PyCandle coverage analysis
use pycandle_core::LayerMeta;
use std::collections::HashMap;

/// Data structure holding analysis results
pub struct ReportData {
    pub supported: usize,
    pub unsupported: usize,
    pub gaps: HashMap<String, usize>,
    pub layers: HashMap<String, LayerMeta>,
}

/// Generates HTML coverage reports from manifest data
pub struct ReportGenerator {
    manifest: HashMap<String, LayerMeta>,
}

impl ReportGenerator {
    pub fn new(manifest: HashMap<String, LayerMeta>) -> Self {
        Self { manifest }
    }

    /// Analyze the manifest and categorize layers
    pub fn analyze(&self) -> ReportData {
        let mut supported = 0;
        let mut unsupported = 0;
        let mut gaps: HashMap<String, usize> = HashMap::new();

        for (_name, meta) in &self.manifest {
            if !meta.is_leaf {
                continue;
            }
            if self.is_supported(&meta.module_type) {
                supported += 1;
            } else {
                unsupported += 1;
                *gaps.entry(meta.module_type.clone()).or_default() += 1;
            }
        }

        ReportData {
            supported,
            unsupported,
            gaps,
            layers: self.manifest.clone(),
        }
    }

    /// Check if a module type is supported by PyCandle codegen
    fn is_supported(&self, module_type: &str) -> bool {
        matches!(
            module_type,
            "Linear"
                | "Conv1d"
                | "Conv2d"
                | "Embedding"
                | "LayerNorm"
                | "ReLU"
                | "GELU"
                | "Sigmoid"
                | "Tanh"
                | "ELU"
                | "LeakyReLU"
                | "Snake"
                | "BatchNorm1d"
                | "BatchNorm2d"
                | "LSTM"
        )
    }

    /// Generate a standalone HTML coverage report
    pub fn generate_html(&self, data: &ReportData) -> String {
        format!(
            r#"<!DOCTYPE html>
<html>
<head>
    <title>PyCandle Coverage Report</title>
    <style>
        :root {{
            --bg: #0f172a;
            --card-bg: #1e293b;
            --text: #e2e8f0;
            --text-muted: #94a3b8;
            --green: #22c55e;
            --red: #ef4444;
            --blue: #3b82f6;
            --border: #334155;
        }}
        * {{ box-sizing: border-box; margin: 0; padding: 0; }}
        body {{
            font-family: 'Inter', system-ui, -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            min-height: 100vh;
            padding: 40px 20px;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        h1 {{
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 8px;
            background: linear-gradient(135deg, var(--blue), var(--green));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }}
        .subtitle {{
            color: var(--text-muted);
            margin-bottom: 32px;
        }}
        .dashboard {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }}
        .card {{
            background: var(--card-bg);
            padding: 24px;
            border-radius: 12px;
            border: 1px solid var(--border);
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(0,0,0,0.3);
        }}
        .card h3 {{
            font-size: 0.875rem;
            font-weight: 500;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }}
        .card .value {{
            font-size: 3rem;
            font-weight: 700;
        }}
        .card.supported .value {{ color: var(--green); }}
        .card.unsupported .value {{ color: var(--red); }}
        .card.total .value {{ color: var(--blue); }}
        .progress-bar {{
            height: 8px;
            background: var(--border);
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }}
        .progress-fill {{
            height: 100%;
            background: linear-gradient(90deg, var(--green), var(--blue));
            border-radius: 4px;
            transition: width 0.5s ease;
        }}
        h2 {{
            font-size: 1.5rem;
            font-weight: 600;
            margin: 32px 0 16px 0;
            color: var(--text);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            background: var(--card-bg);
            border-radius: 12px;
            overflow: hidden;
            margin-bottom: 32px;
        }}
        th {{
            text-align: left;
            padding: 16px;
            background: rgba(0,0,0,0.2);
            font-weight: 600;
            color: var(--text-muted);
            text-transform: uppercase;
            font-size: 0.75rem;
            letter-spacing: 0.05em;
        }}
        td {{
            padding: 12px 16px;
            border-bottom: 1px solid var(--border);
        }}
        tr:last-child td {{
            border-bottom: none;
        }}
        tr:hover {{
            background: rgba(255,255,255,0.02);
        }}
        .status {{
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 4px 12px;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 500;
        }}
        .status.supported {{
            background: rgba(34, 197, 94, 0.15);
            color: var(--green);
        }}
        .status.unsupported {{
            background: rgba(239, 68, 68, 0.15);
            color: var(--red);
        }}
        .status::before {{
            content: '';
            width: 6px;
            height: 6px;
            border-radius: 50%;
            background: currentColor;
        }}
        .mono {{
            font-family: 'JetBrains Mono', 'Fira Code', monospace;
            font-size: 0.875rem;
        }}
        .shape {{
            color: var(--text-muted);
            font-size: 0.8rem;
        }}
        .count-badge {{
            display: inline-block;
            background: var(--border);
            padding: 2px 10px;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 600;
        }}
        
        /* Component grouping styles */
        .component-section {{
            background: var(--card-bg);
            border-radius: 12px;
            border: 1px solid var(--border);
            margin-bottom: 16px;
            overflow: hidden;
        }}
        .component-header {{
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 16px 20px;
            background: rgba(0,0,0,0.2);
            cursor: pointer;
            user-select: none;
            transition: background 0.2s;
        }}
        .component-header:hover {{
            background: rgba(0,0,0,0.3);
        }}
        .component-header h3 {{
            font-size: 1rem;
            font-weight: 600;
            color: var(--text);
            display: flex;
            align-items: center;
            gap: 12px;
        }}
        .component-header .chevron {{
            transition: transform 0.2s;
            color: var(--text-muted);
        }}
        .component-header.collapsed .chevron {{
            transform: rotate(-90deg);
        }}
        .component-stats {{
            display: flex;
            gap: 16px;
            font-size: 0.875rem;
        }}
        .component-stats .stat {{
            display: flex;
            align-items: center;
            gap: 6px;
        }}
        .component-stats .stat.ok {{ color: var(--green); }}
        .component-stats .stat.err {{ color: var(--red); }}
        .component-content {{
            max-height: 2000px;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }}
        .component-content.collapsed {{
            max-height: 0;
        }}
        .component-content table {{
            margin-bottom: 0;
            border-radius: 0;
        }}
        .layer-name {{
            padding-left: 24px;
            position: relative;
        }}
        .layer-name::before {{
            content: '‚îî';
            position: absolute;
            left: 8px;
            color: var(--border);
        }}
        
        /* Filters */
        .filters {{
            display: flex;
            gap: 12px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }}
        .filter-btn {{
            padding: 8px 16px;
            border: 1px solid var(--border);
            background: transparent;
            color: var(--text);
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s;
            font-size: 0.875rem;
        }}
        .filter-btn:hover {{
            background: var(--card-bg);
        }}
        .filter-btn.active {{
            background: var(--blue);
            border-color: var(--blue);
        }}
        .search-box {{
            flex: 1;
            min-width: 200px;
            padding: 8px 16px;
            border: 1px solid var(--border);
            background: var(--card-bg);
            color: var(--text);
            border-radius: 8px;
            font-size: 0.875rem;
        }}
        .search-box:focus {{
            outline: none;
            border-color: var(--blue);
        }}
        .hidden {{ display: none !important; }}
        
        /* Drift Analysis Chart Styles */
        .drift-section {{
            background: var(--card-bg);
            border-radius: 12px;
            border: 1px solid var(--border);
            padding: 24px;
            margin-bottom: 32px;
        }}
        .drift-section h2 {{
            margin-top: 0;
            margin-bottom: 16px;
        }}
        .drift-chart {{
            width: 100%;
            height: 300px;
            position: relative;
        }}
        .drift-chart svg {{
            width: 100%;
            height: 100%;
        }}
        .drift-legend {{
            display: flex;
            gap: 24px;
            margin-top: 16px;
            font-size: 0.875rem;
            color: var(--text-muted);
        }}
        .drift-legend-item {{
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        .drift-legend-color {{
            width: 16px;
            height: 16px;
            border-radius: 4px;
        }}
        .divergence-alert {{
            background: rgba(239, 68, 68, 0.15);
            border: 1px solid var(--red);
            border-radius: 8px;
            padding: 16px;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 12px;
        }}
        .divergence-alert .icon {{
            font-size: 1.5rem;
        }}
        .divergence-alert .message {{
            flex: 1;
        }}
        .divergence-alert .layer-name {{
            font-family: 'JetBrains Mono', monospace;
            color: var(--red);
            font-weight: 600;
        }}
        .drift-placeholder {{
            text-align: center;
            padding: 40px;
            color: var(--text-muted);
        }}
        .drift-placeholder .icon {{
            font-size: 3rem;
            margin-bottom: 16px;
            opacity: 0.5;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üïØÔ∏è PyCandle Coverage Report</h1>
        <p class="subtitle">Module coverage analysis for Candle code generation</p>
        
        <div class="dashboard">
            <div class="card total">
                <h3>Total Layers</h3>
                <div class="value">{total}</div>
            </div>
            <div class="card supported">
                <h3>Supported</h3>
                <div class="value">{supported}</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {coverage:.1}%"></div>
                </div>
            </div>
            <div class="card unsupported">
                <h3>Needs Implementation</h3>
                <div class="value">{unsupported}</div>
            </div>
        </div>
        
        <!-- Drift Analysis Section -->
        <div class="drift-section">
            <h2>üìä Numerical Drift Analysis</h2>
            <div id="divergenceAlert" class="divergence-alert" style="display: none;">
                <span class="icon">‚ö†Ô∏è</span>
                <div class="message">
                    <strong>Divergence Detected!</strong> Cosine similarity dropped below 0.99 at layer 
                    <span class="layer-name" id="divergenceLayer">-</span>
                </div>
            </div>
            <div class="drift-chart" id="driftChart">
                <div class="drift-placeholder">
                    <div class="icon">üìà</div>
                    <p>Run parity verification to see drift analysis</p>
                    <p style="font-size: 0.8rem; margin-top: 8px;">
                        Use <code>PyChecker::verify()</code> and pass results to generate this chart
                    </p>
                </div>
            </div>
            <div class="drift-legend">
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #3b82f6;"></div>
                    <span>MSE (log scale)</span>
                </div>
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #22c55e;"></div>
                    <span>Cosine Similarity</span>
                </div>
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #ef4444;"></div>
                    <span>Divergence Threshold (0.99)</span>
                </div>
            </div>
        </div>
        
        <script src="https://d3js.org/d3.v7.min.js"></script>
        <script>
            // Drift data will be injected here when parity checks are run
            const driftData = {drift_data_json};
            
            if (driftData && driftData.length > 0) {{
                renderDriftChart(driftData);
            }}
            
            function renderDriftChart(data) {{
                const container = document.getElementById('driftChart');
                container.innerHTML = '';
                
                const margin = {{top: 20, right: 60, bottom: 60, left: 60}};
                const width = container.clientWidth - margin.left - margin.right;
                const height = 260 - margin.top - margin.bottom;
                
                const svg = d3.select('#driftChart')
                    .append('svg')
                    .attr('width', width + margin.left + margin.right)
                    .attr('height', height + margin.top + margin.bottom)
                    .append('g')
                    .attr('transform', `translate(${{margin.left}},${{margin.top}})`);
                
                // Scales
                const x = d3.scaleBand()
                    .domain(data.map((d, i) => i))
                    .range([0, width])
                    .padding(0.1);
                
                const yMSE = d3.scaleLog()
                    .domain([1e-10, d3.max(data, d => d.mse) * 10])
                    .range([height, 0]);
                
                const yCosSim = d3.scaleLinear()
                    .domain([0.9, 1])
                    .range([height, 0]);
                
                // MSE bars
                svg.selectAll('.bar-mse')
                    .data(data)
                    .enter()
                    .append('rect')
                    .attr('class', 'bar-mse')
                    .attr('x', (d, i) => x(i))
                    .attr('y', d => yMSE(Math.max(d.mse, 1e-10)))
                    .attr('width', x.bandwidth())
                    .attr('height', d => height - yMSE(Math.max(d.mse, 1e-10)))
                    .attr('fill', '#3b82f6')
                    .attr('opacity', 0.7);
                
                // Cosine similarity line
                const line = d3.line()
                    .x((d, i) => x(i) + x.bandwidth() / 2)
                    .y(d => yCosSim(d.cosine_sim))
                    .curve(d3.curveMonotoneX);
                
                svg.append('path')
                    .datum(data)
                    .attr('fill', 'none')
                    .attr('stroke', '#22c55e')
                    .attr('stroke-width', 2)
                    .attr('d', line);
                
                // Threshold line at 0.99
                svg.append('line')
                    .attr('x1', 0)
                    .attr('x2', width)
                    .attr('y1', yCosSim(0.99))
                    .attr('y2', yCosSim(0.99))
                    .attr('stroke', '#ef4444')
                    .attr('stroke-width', 1)
                    .attr('stroke-dasharray', '4,4');
                
                // Find divergence point
                const divergencePoint = data.findIndex(d => d.cosine_sim < 0.99);
                if (divergencePoint >= 0) {{
                    document.getElementById('divergenceAlert').style.display = 'flex';
                    document.getElementById('divergenceLayer').textContent = data[divergencePoint].name;
                    
                    // Highlight divergence point
                    svg.append('circle')
                        .attr('cx', x(divergencePoint) + x.bandwidth() / 2)
                        .attr('cy', yCosSim(data[divergencePoint].cosine_sim))
                        .attr('r', 6)
                        .attr('fill', '#ef4444')
                        .attr('stroke', '#fff')
                        .attr('stroke-width', 2);
                }}
                
                // Axes
                svg.append('g')
                    .attr('transform', `translate(0,${{height}})`)
                    .call(d3.axisBottom(x).tickFormat(i => data[i]?.name?.split('.').pop() || i))
                    .selectAll('text')
                    .attr('transform', 'rotate(-45)')
                    .style('text-anchor', 'end')
                    .style('fill', '#94a3b8')
                    .style('font-size', '10px');
                
                svg.append('g')
                    .call(d3.axisLeft(yMSE).ticks(5, '.0e'))
                    .selectAll('text')
                    .style('fill', '#3b82f6');
                
                svg.append('g')
                    .attr('transform', `translate(${{width}},0)`)
                    .call(d3.axisRight(yCosSim).ticks(5))
                    .selectAll('text')
                    .style('fill', '#22c55e');
            }}
        </script>
        
        <h2>Gap Analysis</h2>
        <table>
            <thead>
                <tr>
                    <th>Module Type</th>
                    <th>Count</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody>
                {gaps_table}
            </tbody>
        </table>
        
        <h2>Layers by Component</h2>
        <div class="filters">
            <input type="text" class="search-box" placeholder="Search layers..." id="searchBox">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="supported">Supported Only</button>
            <button class="filter-btn" data-filter="unsupported">Unsupported Only</button>
            <button class="filter-btn" data-filter="expand">Expand All</button>
            <button class="filter-btn" data-filter="collapse">Collapse All</button>
        </div>
        
        {components_html}
    </div>
</body>
</html>"#,
            total = data.supported + data.unsupported,
            supported = data.supported,
            unsupported = data.unsupported,
            coverage = if data.supported + data.unsupported > 0 {
                (data.supported as f64 / (data.supported + data.unsupported) as f64) * 100.0
            } else {
                100.0
            },
            gaps_table = self.render_gaps_table(&data.gaps),
            components_html = self.render_components(&data.layers),
            // Drift data - read from jsonl if exists
            drift_data_json = self.read_drift_data(),
        )
    }

    fn read_drift_data(&self) -> String {
        let path = std::path::Path::new("verification_results.jsonl");
        if !path.exists() {
            return "[]".to_string();
        }

        if let Ok(content) = std::fs::read_to_string(path) {
            let lines: Vec<String> = content
                .lines()
                .filter(|l| !l.trim().is_empty())
                .map(|l| l.to_string())
                .collect();

            format!("[{}]", lines.join(","))
        } else {
            "[]".to_string()
        }
    }

    fn render_gaps_table(&self, gaps: &HashMap<String, usize>) -> String {
        if gaps.is_empty() {
            return "<tr><td colspan=\"3\" style=\"text-align: center; color: var(--green);\">‚úÖ All module types are supported!</td></tr>".to_string();
        }

        let mut sorted: Vec<_> = gaps.iter().collect();
        sorted.sort_by(|a, b| b.1.cmp(a.1));

        sorted
            .iter()
            .map(|(module_type, count)| {
                format!(
                    r#"<tr>
                        <td class="mono">{}</td>
                        <td><span class="count-badge">{}</span></td>
                        <td><span class="status unsupported">Needs Implementation</span></td>
                    </tr>"#,
                    module_type, count
                )
            })
            .collect::<Vec<_>>()
            .join("\n")
    }

    /// Group layers by their top-level component and render as collapsible sections
    fn render_components(&self, layers: &HashMap<String, LayerMeta>) -> String {
        // Group layers by their first path component
        let mut groups: HashMap<String, Vec<(&String, &LayerMeta)>> = HashMap::new();

        for (name, meta) in layers.iter().filter(|(_, m)| m.is_leaf) {
            let component = name.split('.').next().unwrap_or(name).to_string();
            groups.entry(component).or_default().push((name, meta));
        }

        // Sort groups by name
        let mut sorted_groups: Vec<_> = groups.into_iter().collect();
        sorted_groups.sort_by(|a, b| a.0.cmp(&b.0));

        let components_html = sorted_groups
            .iter()
            .map(|(component, layers)| {
                // Sort layers within component
                let mut sorted_layers = layers.clone();
                sorted_layers.sort_by(|a, b| a.0.cmp(b.0));

                // Count supported/unsupported
                let supported_count = sorted_layers
                    .iter()
                    .filter(|(_, m)| self.is_supported(&m.module_type))
                    .count();
                let unsupported_count = sorted_layers.len() - supported_count;

                let rows: String = sorted_layers
                    .iter()
                    .map(|(name, meta)| {
                        let supported = self.is_supported(&meta.module_type);
                        let status_class = if supported {
                            "supported"
                        } else {
                            "unsupported"
                        };
                        let status_text = if supported {
                            "Supported"
                        } else {
                            "Needs Implementation"
                        };

                        // Get the short name (everything after the first dot)
                        let short_name = name.split('.').skip(1).collect::<Vec<_>>().join(".");
                        let display_name = if short_name.is_empty() {
                            name.to_string()
                        } else {
                            short_name
                        };

                        let input_shapes = meta
                            .input_shapes
                            .iter()
                            .map(|s| format!("{:?}", s))
                            .collect::<Vec<_>>()
                            .join(", ");
                        let output_shapes = meta
                            .output_shapes
                            .iter()
                            .map(|s| format!("{:?}", s))
                            .collect::<Vec<_>>()
                            .join(", ");

                        format!(
                            r#"<tr class="layer-row" data-supported="{}">
                            <td class="mono layer-name">{}</td>
                            <td class="mono">{}</td>
                            <td class="shape">{}</td>
                            <td class="shape">{}</td>
                            <td><span class="status {}">{}</span></td>
                        </tr>"#,
                            supported,
                            display_name,
                            meta.module_type,
                            input_shapes,
                            output_shapes,
                            status_class,
                            status_text
                        )
                    })
                    .collect();

                format!(
                    r#"<div class="component-section" data-has-unsupported="{}">
                        <div class="component-header">
                            <h3>
                                <span class="chevron">‚ñº</span>
                                {}
                            </h3>
                            <div class="component-stats">
                                <span class="stat">{} layers</span>
                                <span class="stat ok">‚úì {}</span>
                                {}
                            </div>
                        </div>
                        <div class="component-content">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Layer</th>
                                        <th>Type</th>
                                        <th>Input Shape</th>
                                        <th>Output Shape</th>
                                        <th>Status</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {}
                                </tbody>
                            </table>
                        </div>
                    </div>"#,
                    unsupported_count > 0,
                    component,
                    sorted_layers.len(),
                    supported_count,
                    if unsupported_count > 0 {
                        format!("<span class=\"stat err\">‚úó {}</span>", unsupported_count)
                    } else {
                        String::new()
                    },
                    rows
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        format!(
            r#"
        <div id="componentList">
            {}
        </div>
        
        <script>
            // State
            let currentFilter = 'all';
            let searchQuery = '';

            // Toggle component sections
            function setupCollapsibles() {{
                document.querySelectorAll('.component-header').forEach(header => {{
                    // Remove old listener if any
                    const newHeader = header.cloneNode(true);
                    header.parentNode.replaceChild(newHeader, header);
                    
                    newHeader.addEventListener('click', () => {{
                        newHeader.classList.toggle('collapsed');
                        newHeader.nextElementSibling.classList.toggle('collapsed');
                    }});
                }});
            }}
            setupCollapsibles();
            
            function updateVisibility() {{
                document.querySelectorAll('.component-section').forEach(section => {{
                    const rows = section.querySelectorAll('.layer-row');
                    let visibleRowsInSection = 0;
                    
                    rows.forEach(row => {{
                        const isSupported = row.dataset.supported === 'true';
                        let matchesFilter = true;
                        
                        if (currentFilter === 'supported') matchesFilter = isSupported;
                        else if (currentFilter === 'unsupported') matchesFilter = !isSupported;
                        
                        const matchesSearch = !searchQuery || row.textContent.toLowerCase().includes(searchQuery) || 
                                           section.querySelector('h3').textContent.toLowerCase().includes(searchQuery);
                        
                        const isVisible = matchesFilter && matchesSearch;
                        row.classList.toggle('hidden', !isVisible);
                        if (isVisible) visibleRowsInSection++;
                    }});
                    
                    section.classList.toggle('hidden', visibleRowsInSection === 0);
                    
                    // Auto-expand if we are filtering for unsupported and there are some
                    if (currentFilter === 'unsupported' && visibleRowsInSection > 0) {{
                        section.querySelector('.component-header').classList.remove('collapsed');
                        section.querySelector('.component-content').classList.remove('collapsed');
                    }}
                }});
            }}

            // Filter buttons
            document.querySelectorAll('.filter-btn').forEach(btn => {{
                btn.addEventListener('click', () => {{
                    const filter = btn.dataset.filter;
                    
                    if (filter === 'expand') {{
                        document.querySelectorAll('.component-header').forEach(h => h.classList.remove('collapsed'));
                        document.querySelectorAll('.component-content').forEach(c => c.classList.remove('collapsed'));
                        return;
                    }}
                    if (filter === 'collapse') {{
                        document.querySelectorAll('.component-header').forEach(h => h.classList.add('collapsed'));
                        document.querySelectorAll('.component-content').forEach(c => c.classList.add('collapsed'));
                        return;
                    }}
                    
                    document.querySelectorAll('.filter-btn').forEach(b => {{
                        if (['all', 'supported', 'unsupported'].includes(b.dataset.filter)) {{
                            b.classList.remove('active');
                        }}
                    }});
                    btn.classList.add('active');
                    
                    currentFilter = filter;
                    updateVisibility();
                }});
            }});
            
            // Search
            document.getElementById('searchBox').addEventListener('input', (e) => {{
                searchQuery = e.target.value.toLowerCase();
                updateVisibility();
            }});
        </script>
        "#,
            components_html
        )
    }
}
</file>

<file path="py/generated_complex.rs">
use candle_core::{Tensor, Result, Device, Shape};
use candle_nn::{Linear, Conv1d, LayerNorm, Embedding, VarBuilder, Module};
use pycandle_core::{PyChecker, py_check, Dropout, Transpose, Mish, CausalConv1d, SiLU, ReLU, GELU, Sigmoid, Tanh, ELU, LeakyReLU, Snake, BatchNorm1d, BatchNorm2d, LSTM};

pub struct ComplexModel {
    pub conv1: Conv1d,
    pub fc: Linear,
    pub relu: ReLU,
    pub checker: Option<PyChecker>,
}

impl ComplexModel {
    pub fn load(vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let conv1 = candle_nn::conv1d(16, 32, 3, candle_nn::Conv1dConfig { stride: 1, padding: 1, ..Default::default() }, vb.pp("conv1"))?;
        let fc = candle_nn::linear(320, 10, vb.pp("fc"))?;
        let relu = ReLU;

        Ok(Self {
            conv1,
            fc,
            relu,
            checker,
        })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let x_conv1 = self.conv1.forward(&xs)?;
        py_check!(self.checker, "conv1", &x_conv1);
        let x_relu = self.relu.forward(&x_conv1)?;
        py_check!(self.checker, "relu", &x_relu);
        let x_size = x_relu.dim(0)?;
        let x_view = x_relu.reshape(vec![x_size, -1])?;
        let x_fc = self.fc.forward(&x_view)?;
        py_check!(self.checker, "fc", &x_fc);
        Ok(x_fc)
    }
}
</file>

<file path="tests/parity.rs">
#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::{Device, Tensor};
    use pycandle_core::PyChecker;
    use my_project::MyModel;
    use anyhow::Result;

    #[test]
    fn test_parity() -> Result<()> {
        // 1. Setup Device
        let device = Device::cuda_if_available(0).unwrap_or(Device::Cpu);
        println!("Running on device: {:?}", device);

        // 2. Load Checker and Golden Trace
        // Assumes "pycandle_trace" directory exists with trace files
        let checker = PyChecker::load("debug_run", "pycandle_trace", &device)?;
        println!("Loaded checker with trace: {}", checker.name);

        // 3. Load Model
        // We use the same variable builder as normally, but verify loaded weights match if needed
        // For parity test, we rely on the implementation's load to initialize weights correctly 
        // (often random or specific config). 
        // Ideally, we should load exact weights from the trace if available, but for now 
        // we assume the user might have a load_from_weights or similar if critical.
        // However, for pure activation parity on specific inputs, we usually need the weights to match.
        // TODO: In a real scenario, we might need to load weights from the trace too.
        // For now, we assume the user constructs the model. 
        // NOTE: If the model uses random initialization, this test WILL FAIL unless 
        // we load weights from the python trace.
        // 
        // As a workaround for this generic generator, we assume the user has a `load` function
        // that takes the checker.
        let vb = candle_nn::VarBuilder::zeros(candle_core::DType::F32, &device);
        let model = MyModel::load(vb, Some(checker.clone()))?;

        // 4. Load Inputs from Trace
        // The trace should contain 'model_input.0', 'model_input.1', etc.
        // We'll try to load at least the first input.
        // Note: PyChecker holds "golden_tensors", which contains the inputs too if we saved them!
        
        // We access the inputs directly from the golden tensors loaded in checker
        // (This requires PyChecker to expose golden_tensors or a getter, or we access directly if pub)
        // Since `golden_tensors` is private in PyChecker but we need it, ensure PyChecker exposes a way.
        // Assuming PyChecker has a `get_tensor` method or we can clone from the file.
        // Let's use `candle_core::safetensors::load` again here for simplicity to get inputs.
        
        let tensors = candle_core::safetensors::load("pycandle_trace/debug_run_trace.safetensors", &device)?;
        
        // 5. Run Forward Pass
        if let Some(input) = tensors.get("model_input.0") {
            let _output = model.forward(input)?;
            println!("Forward pass completed successfully!");
        } else {
            eprintln!("‚ö†Ô∏è No 'model_input.0' found in trace. Skipping forward pass execution.");
            println!("Available keys: {:?}", tensors.keys().take(5));
        }

        Ok(())
    }
}
</file>

<file path="crates/pycandle-audio/src/lib.rs">
//! PyCandle Audio - Audio operations with PyTorch parity
//!
//! This crate provides STFT, iSTFT, and padding operations that match
//! PyTorch's behavior for audio model porting.

use candle_core::{Result, Tensor};

/// Padding modes for audio operations
#[derive(Debug, Clone, Copy)]
pub enum PadMode {
    /// Reflect padding: [1,2,3] -> [3,2,1,2,3,2,1]
    Reflect,
    /// Replicate edge values
    Replicate,
    /// Pad with constant value
    Constant(f64),
}

/// Mel scale types
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MelScale {
    /// HTK scale: 2595 * log10(1 + f / 700)
    Htk,
    /// Slaney scale: linear below 1kHz, log above
    Slaney,
}

/// Normalization modes for Mel banks
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MelNorm {
    /// No normalization
    None,
    /// Slaney-style area normalization
    Slaney,
}

/// MelSpectrogram configuration matching torchaudio.transforms.MelSpectrogram
#[derive(Debug, Clone)]
pub struct MelSpectrogramConfig {
    pub stft_config: StftConfig,
    pub sample_rate: usize,
    pub n_mels: usize,
    pub f_min: f64,
    pub f_max: Option<f64>,
    pub mel_scale: MelScale,
    pub norm: MelNorm,
}

impl Default for MelSpectrogramConfig {
    fn default() -> Self {
        Self {
            stft_config: StftConfig::default(),
            sample_rate: 16000,
            n_mels: 128,
            f_min: 0.0,
            f_max: None,
            mel_scale: MelScale::Htk,
            norm: MelNorm::None,
        }
    }
}

/// STFT configuration matching PyTorch's torch.stft
#[derive(Debug, Clone)]
pub struct StftConfig {
    pub n_fft: usize,
    pub hop_length: Option<usize>,
    pub win_length: Option<usize>,
    pub center: bool,
    pub pad_mode: PadMode,
    pub normalized: bool,
    pub onesided: bool,
    pub return_complex: bool,
}

impl Default for StftConfig {
    fn default() -> Self {
        Self {
            n_fft: 400,
            hop_length: None,
            win_length: None,
            center: true,
            pad_mode: PadMode::Reflect,
            normalized: false,
            onesided: true,
            return_complex: true,
        }
    }
}

/// Apply 1D padding to a tensor
///
/// # Arguments
/// * `input` - Tensor of shape (B, T) or (B, C, T)
/// * `pad_left` - Amount of padding on the left
/// * `pad_right` - Amount of padding on the right
/// * `mode` - Padding mode
pub fn pad_1d(input: &Tensor, pad_left: usize, pad_right: usize, mode: PadMode) -> Result<Tensor> {
    let ndim = input.dims().len();
    let time_dim = ndim - 1;
    let time_len = input.dim(time_dim)?;

    match mode {
        PadMode::Reflect => {
            // Reflect padding - build indices and use index_select
            if pad_left > time_len - 1 || pad_right > time_len - 1 {
                return Err(candle_core::Error::Msg(
                    "Reflect padding size exceeds input size".to_string(),
                ));
            }

            // Build reflection indices
            let mut indices = Vec::with_capacity(pad_left + time_len + pad_right);

            // Left reflection: [pad_left, pad_left-1, ..., 1]
            for i in (1..=pad_left).rev() {
                indices.push(i as u32);
            }

            // Original: [0, 1, ..., time_len-1]
            for i in 0..time_len {
                indices.push(i as u32);
            }

            // Right reflection: [time_len-2, time_len-3, ..., time_len-1-pad_right]
            for i in 0..pad_right {
                indices.push((time_len - 2 - i) as u32);
            }

            let idx_tensor =
                Tensor::from_vec(indices, (pad_left + time_len + pad_right,), input.device())?;
            input.index_select(&idx_tensor, time_dim)
        }
        PadMode::Replicate => {
            // Edge/replicate padding using constant padding with edge values
            // For simplicity, use constant padding with zero and then copy edge values
            let total_len = pad_left + time_len + pad_right;
            let mut indices = Vec::with_capacity(total_len);

            for _ in 0..pad_left {
                indices.push(0u32);
            }
            for i in 0..time_len {
                indices.push(i as u32);
            }
            for _ in 0..pad_right {
                indices.push((time_len - 1) as u32);
            }

            let idx_tensor = Tensor::from_vec(indices, (total_len,), input.device())?;
            input.index_select(&idx_tensor, time_dim)
        }
        PadMode::Constant(val) => {
            let mut left_shape: Vec<usize> = input.dims().to_vec();
            left_shape[time_dim] = pad_left;
            let mut right_shape: Vec<usize> = input.dims().to_vec();
            right_shape[time_dim] = pad_right;

            let left =
                Tensor::full(val as f32, left_shape, input.device())?.to_dtype(input.dtype())?;
            let right =
                Tensor::full(val as f32, right_shape, input.device())?.to_dtype(input.dtype())?;
            Tensor::cat(&[&left, input, &right], time_dim)
        }
    }
}

/// Create a Hann window
pub fn hann_window(length: usize, device: &candle_core::Device) -> Result<Tensor> {
    let mut values = vec![0f32; length];
    for i in 0..length {
        values[i] = 0.5 * (1.0 - (2.0 * std::f32::consts::PI * i as f32 / length as f32).cos());
    }
    Tensor::from_vec(values, (length,), device)
}

/// Convert Hz to Mel frequency
pub fn hz_to_mel(freq: f64, scale: MelScale) -> f64 {
    match scale {
        MelScale::Htk => 2595.0 * (1.0 + freq / 700.0).log10(),
        MelScale::Slaney => {
            let min_log_hz = 1000.0;
            let min_log_mel = 15.0;
            let log_step = (6.4f64).ln() / 27.0;
            if freq >= min_log_hz {
                min_log_mel + (freq / min_log_hz).ln() / log_step
            } else {
                3.0 * freq / 200.0
            }
        }
    }
}

/// Convert Mel frequency to Hz
pub fn mel_to_hz(mel: f64, scale: MelScale) -> f64 {
    match scale {
        MelScale::Htk => 700.0 * (10f64.powf(mel / 2595.0) - 1.0),
        MelScale::Slaney => {
            let min_log_hz = 1000.0;
            let min_log_mel = 15.0;
            let log_step = (6.4f64).ln() / 27.0;
            if mel >= min_log_mel {
                min_log_hz * (log_step * (mel - min_log_mel)).exp()
            } else {
                200.0 * mel / 3.0
            }
        }
    }
}

/// Create a Mel filterbank matrix of shape (n_mels, n_fft / 2 + 1)
pub fn get_mel_banks(
    n_mels: usize,
    n_fft: usize,
    sample_rate: usize,
    f_min: f64,
    f_max: f64,
    scale: MelScale,
    norm: MelNorm,
) -> Result<Tensor> {
    let n_bins = n_fft / 2 + 1;
    let mel_min = hz_to_mel(f_min, scale);
    let mel_max = hz_to_mel(f_max, scale);

    let mut mel_points = vec![0.0f64; n_mels + 2];
    for i in 0..n_mels + 2 {
        mel_points[i] = mel_min + i as f64 * (mel_max - mel_min) / (n_mels + 1) as f64;
    }

    let mut hz_points = vec![0.0f64; n_mels + 2];
    for i in 0..n_mels + 2 {
        hz_points[i] = mel_to_hz(mel_points[i], scale);
    }

    let mut fft_freqs = vec![0.0f64; n_bins];
    for i in 0..n_bins {
        fft_freqs[i] = i as f64 * sample_rate as f64 / n_fft as f64;
    }

    let mut filterbank = vec![0.0f32; n_mels * n_bins];

    for i in 0..n_mels {
        let left = hz_points[i];
        let center = hz_points[i + 1];
        let right = hz_points[i + 2];

        for j in 0..n_bins {
            let f = fft_freqs[j];
            if f > left && f < right {
                let val = if f <= center {
                    (f - left) / (center - left)
                } else {
                    (right - f) / (right - center)
                };

                // Area normalization (Slaney)
                let val = if norm == MelNorm::Slaney {
                    val * 2.0 / (right - left)
                } else {
                    val
                };

                filterbank[i * n_bins + j] = val as f32;
            }
        }
    }

    Tensor::from_vec(filterbank, (n_mels, n_bins), &candle_core::Device::Cpu)
}

/// MelSpectrogram transformation
pub fn mel_spectrogram(
    input: &Tensor,
    config: &MelSpectrogramConfig,
    window: Option<&Tensor>,
) -> Result<Tensor> {
    let n_fft = config.stft_config.n_fft;
    let f_max = config.f_max.unwrap_or(config.sample_rate as f64 / 2.0);

    // 1. STFT
    let spec = stft(input, &config.stft_config, window)?;

    // spec is (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2)
    // 2. Power Spectrogram (magnitude squared)
    let power = spec
        .narrow(spec.dims().len() - 1, 0, 1)?
        .sqr()?
        .add(&spec.narrow(spec.dims().len() - 1, 1, 1)?.sqr()?)?;
    let power = power.squeeze(spec.dims().len() - 1)?;

    // 3. Mel Filterbank
    let mel_banks = get_mel_banks(
        config.n_mels,
        n_fft,
        config.sample_rate,
        config.f_min,
        f_max,
        config.mel_scale,
        config.norm,
    )?
    .to_device(input.device())?
    .to_dtype(input.dtype())?;

    // Apply filterbank: (n_mels, n_bins) * (..., n_bins, n_frames)
    // We need to reorder power to (..., n_frames, n_bins) to use matmul or just use a custom dot product
    // Or just use matmul if we transpose.
    // power is (B, n_bins, n_frames)
    let power = power.transpose(power.dims().len() - 2, power.dims().len() - 1)?;
    // power is (B, n_frames, n_bins)

    let mel_spec = power.matmul(&mel_banks.transpose(0, 1)?)?;
    // mel_spec is (B, n_frames, n_mels)

    mel_spec.transpose(mel_spec.dims().len() - 2, mel_spec.dims().len() - 1)
}

/// Short-time Fourier Transform (STFT)
pub fn stft(input: &Tensor, config: &StftConfig, window: Option<&Tensor>) -> Result<Tensor> {
    let device = input.device();
    let dtype = input.dtype();

    // 1. Padding
    let n_fft = config.n_fft;
    let hop_length = config.hop_length.unwrap_or(n_fft / 4);
    let win_length = config.win_length.unwrap_or(n_fft);

    let mut x = input.clone();
    if config.center {
        let pad = n_fft / 2;
        x = pad_1d(&x, pad, pad, config.pad_mode)?;
    }

    // 2. Framing & Windowing
    // x is (B, T) or (T,)
    let dims = x.dims();
    let (batch_size, time_len) = if dims.len() == 1 {
        (1, dims[0])
    } else if dims.len() == 2 {
        (dims[0], dims[1])
    } else {
        return Err(candle_core::Error::Msg(format!(
            "STFT input must be (B, T) or (T,), got {:?}",
            dims
        )));
    };

    let n_frames = (time_len - win_length) / hop_length + 1;

    // Move to CPU for FFT if on GPU
    let x_cpu = x.to_device(&candle_core::Device::Cpu)?;
    let x_vec = x_cpu.flatten_all()?.to_vec1::<f32>()?;

    let window_vec = if let Some(w) = window {
        w.to_device(&candle_core::Device::Cpu)?.to_vec1::<f32>()?
    } else {
        vec![1.0; win_length]
    };

    // 3. FFT Setup
    let mut planner = realfft::RealFftPlanner::<f32>::new();
    let r2c = planner.plan_fft_forward(n_fft);

    let n_bins = n_fft / 2 + 1;
    let mut output_vec = vec![0.0f32; batch_size * n_frames * n_bins * 2];

    for b in 0..batch_size {
        let batch_offset = b * time_len;
        let out_batch_offset = b * n_bins * n_frames * 2;
        for f in 0..n_frames {
            let start = batch_offset + f * hop_length;
            let mut frame = vec![0.0f32; n_fft];

            // Apply window and copy into frame (handling win_length < n_fft with zero padding)
            for i in 0..win_length {
                frame[i] = x_vec[start + i] * window_vec[i];
            }

            let mut spectrum = r2c.make_output_vec();
            r2c.process(&mut frame, &mut spectrum)
                .map_err(|e| candle_core::Error::Msg(format!("FFT error: {:?}", e)))?;

            for (i, c) in spectrum.iter().enumerate() {
                let out_idx = out_batch_offset + (i * n_frames + f) * 2;
                output_vec[out_idx] = c.re;
                output_vec[out_idx + 1] = c.im;
            }
        }
    }

    // 4. Result Formatting
    let shape = if batch_size == 1 && dims.len() == 1 {
        candle_core::Shape::from((n_bins, n_frames, 2usize))
    } else {
        candle_core::Shape::from((batch_size, n_bins, n_frames, 2usize))
    };

    let result = Tensor::from_vec(output_vec, shape, &candle_core::Device::Cpu)?;

    // Move back to original device and dtype
    result.to_device(device)?.to_dtype(dtype)
}

/// Inverse Short-time Fourier Transform (iSTFT)
pub fn istft(input: &Tensor, config: &StftConfig, window: Option<&Tensor>) -> Result<Tensor> {
    let device = input.device();
    let dtype = input.dtype();

    let n_fft = config.n_fft;
    let hop_length = config.hop_length.unwrap_or(n_fft / 4);
    let win_length = config.win_length.unwrap_or(n_fft);

    // input is (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2)
    let dims = input.dims();
    let (batch_size, n_bins, n_frames) = if dims.len() == 3 {
        (1, dims[0], dims[1])
    } else if dims.len() == 4 {
        (dims[0], dims[1], dims[2])
    } else {
        return Err(candle_core::Error::Msg(format!(
            "iSTFT input must be (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2), got {:?}",
            dims
        )));
    };

    if n_bins != n_fft / 2 + 1 {
        return Err(candle_core::Error::Msg(format!(
            "Expected {} bins for n_fft={}, got {}",
            n_fft / 2 + 1,
            n_fft,
            n_bins
        )));
    }

    // Move to CPU
    let input_cpu = input.to_device(&candle_core::Device::Cpu)?;
    let input_vec = input_cpu.flatten_all()?.to_vec1::<f32>()?;

    let window_vec = if let Some(w) = window {
        w.to_device(&candle_core::Device::Cpu)?.to_vec1::<f32>()?
    } else {
        vec![1.0; win_length]
    };

    // 2. Inverse FFT Setup
    let mut planner = realfft::RealFftPlanner::<f32>::new();
    let c2r = planner.plan_fft_inverse(n_fft);

    let mut output_audio = Vec::with_capacity(batch_size * (n_frames * hop_length + n_fft));

    for b in 0..batch_size {
        let batch_offset = b * n_bins * n_frames * 2;
        let expected_len = n_frames * hop_length + n_fft;
        let mut reconstructed = vec![0.0f32; expected_len];
        let mut window_sum = vec![0.0f32; expected_len];

        for f in 0..n_frames {
            let start = f * hop_length;
            let mut spectrum = Vec::with_capacity(n_bins);
            for i in 0..n_bins {
                let idx = batch_offset + (i * n_frames + f) * 2;
                spectrum.push(num_complex::Complex::new(
                    input_vec[idx],
                    input_vec[idx + 1],
                ));
            }

            let mut frame = c2r.make_output_vec();
            c2r.process(&mut spectrum, &mut frame)
                .map_err(|e| candle_core::Error::Msg(format!("iFFT error: {:?}", e)))?;

            // Normalize iFFT (realfft doesn't normalize by default)
            let norm = 1.0 / n_fft as f32;
            for i in 0..win_length {
                reconstructed[start + i] += frame[i] * norm * window_vec[i];
                window_sum[start + i] += window_vec[i] * window_vec[i];
            }
        }

        // Apply OLA normalization (Over-Lap Add)
        for i in 0..reconstructed.len() {
            if window_sum[i] > 1e-10 {
                reconstructed[i] /= window_sum[i];
            }
        }

        output_audio.extend_from_slice(&reconstructed);
    }

    // 3. Finalize and Crop
    let mut result = Tensor::from_vec(
        output_audio,
        (batch_size, n_frames * hop_length + n_fft),
        &candle_core::Device::Cpu,
    )?;

    if config.center {
        let pad = n_fft / 2;
        let total_len = result.dim(1)?;
        result = result.narrow(1, pad, total_len - 2 * pad)?;
    }

    if batch_size == 1 && dims.len() == 3 {
        result = result.squeeze(0)?;
    }

    result.to_device(device)?.to_dtype(dtype)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hann_window() {
        let device = candle_core::Device::Cpu;
        let window = hann_window(4, &device).unwrap();
        let data: Vec<f32> = window.to_vec1().unwrap();
        // Hann window for n=4: [0, 0.5, 1, 0.5]
        assert!((data[0] - 0.0).abs() < 1e-5);
        assert!((data[1] - 0.5).abs() < 1e-5);
        assert!((data[2] - 1.0).abs() < 1e-5);
        assert!((data[3] - 0.5).abs() < 1e-5);
    }

    #[test]
    fn test_stft_istft_roundtrip() {
        let device = candle_core::Device::Cpu;
        let config = StftConfig {
            n_fft: 16,
            hop_length: Some(4),
            win_length: Some(16),
            center: true,
            pad_mode: PadMode::Reflect,
            ..Default::default()
        };

        // Create a simple signal: sum of sines
        let mut signal = vec![0.0f32; 64];
        for i in 0..64 {
            signal[i] = (2.0 * std::f32::consts::PI * 440.0 * i as f32 / 16000.0).sin();
        }
        let x = Tensor::from_vec(signal, (64,), &device).unwrap();
        let window = hann_window(config.win_length.unwrap(), &device).unwrap();

        let spec = stft(&x, &config, Some(&window)).unwrap();
        let x_hat = istft(&spec, &config, Some(&window)).unwrap();

        let x_vec = x.to_vec1::<f32>().unwrap();
        let x_hat_vec = x_hat.to_vec1::<f32>().unwrap();

        // Roundtrip should be reasonably close
        // Note: OLA with Hann window and hop=4 (n_fft/4) meets COLA
        for i in 4..60 {
            // Avoid edges due to OLA ramp-up/down if not perfectly handled by padding
            assert!(
                (x_vec[i] - x_hat_vec[i]).abs() < 1e-3,
                "At index {}: {} != {}",
                i,
                x_vec[i],
                x_hat_vec[i]
            );
        }
    }

    #[test]
    fn test_mel_scales() {
        // HTK parity
        let hz = 1000.0;
        let mel = hz_to_mel(hz, MelScale::Htk);
        assert!((mel - 1000.0).abs() < 0.1);
        let hz_back = mel_to_hz(mel, MelScale::Htk);
        assert!((hz_back - hz).abs() < 1e-5);

        // Slaney parity
        let hz_s = 2000.0;
        let mel_s = hz_to_mel(hz_s, MelScale::Slaney);
        // Slaney: 15 + log(2000/1000) / log_step
        assert!((mel_s - 25.0).abs() < 0.1);
        let hz_back_s = mel_to_hz(mel_s, MelScale::Slaney);
        assert!((hz_back_s - hz_s).abs() < 1e-5);
    }

    #[test]
    fn test_get_mel_banks() {
        let n_mels = 40;
        let n_fft = 1024;
        let sample_rate = 16000;
        let f_min = 0.0;
        let f_max = 8000.0;

        let banks = get_mel_banks(
            n_mels,
            n_fft,
            sample_rate,
            f_min,
            f_max,
            MelScale::Htk,
            MelNorm::None,
        )
        .unwrap();

        assert_eq!(banks.dims(), &[40, 513]);
    }
}
</file>

<file path="crates/pycandle-core/src/lib.rs">
//! PyCandle Core Library
//!
//! Core functionality for PyTorch ‚Üí Candle porting with parity verification.
//!
//! # Features
//! - `PyChecker` for layer-wise verification against golden tensors
//! - `py_check!` macro for embedded verification in generated code
//! - Layer implementations: BatchNorm, LSTM, activations
//! - Code generation from manifests

mod checker;
pub mod codegen;
pub mod gpt2;
mod layers;
pub mod samplers;
pub mod weights;

pub use checker::{ComparisonResult, LayerMeta, PyChecker, VerificationMode};
pub use layers::*;
pub use samplers::*;
pub use weights::{WeightExtractor, WeightMapper};

/// Verify tensor against golden record, panics on mismatch
#[macro_export]
macro_rules! py_check {
    ($checker:expr, $name:expr, $tensor:expr) => {
        if let Some(ref c) = $checker {
            c.verify($name, $tensor).expect("Parity Check Failed");
        }
    };
}
</file>

<file path=".gitignore">
/target
Cargo.lock
# chatterbox-repo/*/
*.safetensors
*.pyc
__pycache__/
.venv/
pycandle_trace/
py_trace/
*.html
py/__pycache__/*/
</file>

<file path="Cargo.toml">
[workspace]
members = ["crates/pycandle", "crates/pycandle-core", "crates/pycandle-audio"]
resolver = "2"

[workspace.package]
version = "0.1.0"
edition = "2024"
license = "MIT"

[workspace.dependencies]
# Candle ML framework
candle-core = "0.8.2"
candle-nn = "0.8.2"

# Serialization
safetensors = "0.5.2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# CLI & utilities
clap = { version = "4.4", features = ["derive"] }
colored = "2.0"
thiserror = "1.0"
anyhow = "1.0"
walkdir = "2.4"
regex = "1.10"
realfft = "3.3"
num-complex = "0.4"

# Internal crates
pycandle-core = { path = "crates/pycandle-core" }
pycandle-audio = { path = "crates/pycandle-audio" }
</file>

<file path="crates/pycandle-core/src/codegen/mod.rs">
//! Code generation from PyTorch manifests to Candle Rust code
//!
//! This module generates idiomatic Rust code from recorded PyTorch model manifests.

pub mod gpt2;

use crate::LayerMeta;
use serde::Serialize;
use std::collections::HashMap;

// ============================================================================
// Internal Types for Codegen
// ============================================================================

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ReturnType {
    Tensor,
    Tuple,
    Vec,
}

// ============================================================================
// JSON Output Structs for Analysis
// ============================================================================

#[derive(Serialize, Debug)]
pub struct AnalysisResult {
    pub supported: usize,
    pub unsupported: usize,
    pub total: usize,
    pub coverage_percent: f32,
    pub gaps: Vec<GapInfo>,
    pub layers: Vec<LayerInfo>,
}

#[derive(Serialize, Debug)]
pub struct GapInfo {
    pub module_type: String,
    pub count: usize,
    pub suggestion: String,
}

#[derive(Serialize, serde::Deserialize, Debug, Clone)]
pub struct LayerInfo {
    pub name: String,
    pub module_type: String,
    pub supported: bool,
    pub input_shapes: Vec<Vec<usize>>,
    pub output_shapes: Vec<Vec<usize>>,
}

#[derive(serde::Deserialize, Debug, Clone)]
pub struct GraphNode {
    pub name: String,
    pub op: String,
    pub target: String,
    pub args: Vec<serde_json::Value>,
    pub module_type: Option<String>,
}

#[derive(Serialize, serde::Deserialize, Debug, Clone, Default)]
pub struct SymbolicConfig {
    pub dims: HashMap<String, usize>,
}

/// Code generator that converts manifests to Rust code
pub struct Codegen {
    manifest: HashMap<String, LayerMeta>,
    graph_nodes: Option<Vec<GraphNode>>,
    config: SymbolicConfig,
    hints: HashMap<String, usize>,
}

impl Codegen {
    pub fn new(
        manifest: HashMap<String, LayerMeta>,
        hints: Option<HashMap<String, usize>>,
    ) -> Self {
        let mut slf = Self {
            manifest,
            graph_nodes: None,
            config: SymbolicConfig::default(),
            hints: hints.unwrap_or_default(),
        };
        slf.config = slf.extract_symbolic_config();
        slf
    }

    pub fn with_graph(mut self, graph_nodes: Vec<GraphNode>) -> Self {
        self.graph_nodes = Some(graph_nodes);
        self
    }

    pub fn extract_symbolic_config(&self) -> SymbolicConfig {
        let mut dims = self.hints.clone();

        for meta in self.manifest.values() {
            // GPT2 specific extraction
            if let Some(v) = meta.config.get("vocab_size").and_then(|v| v.as_u64()) {
                dims.entry("vocab_size".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_embd").and_then(|v| v.as_u64()) {
                dims.entry("hidden_dim".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_head").and_then(|v| v.as_u64()) {
                dims.entry("n_head".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_layer").and_then(|v| v.as_u64()) {
                dims.entry("n_layers".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_positions").and_then(|v| v.as_u64()) {
                dims.entry("context_length".to_string())
                    .or_insert(v as usize);
            }

            // Generic extraction
            match meta.module_type.as_str() {
                "Embedding" => {
                    if let Some(n) = meta.config.get("num_embeddings").and_then(|v| v.as_u64()) {
                        dims.entry("vocab_size".to_string()).or_insert(n as usize);
                    }
                    if let Some(d) = meta.config.get("embedding_dim").and_then(|v| v.as_u64()) {
                        dims.entry("hidden_dim".to_string()).or_insert(d as usize);
                    }
                }
                "Linear" | "LoRACompatibleLinear" => {
                    // If out_features is high (like 30000+), it's likely a vocab_size (lm_head)
                    if let Some(out_f) = meta.config.get("out_features").and_then(|v| v.as_u64()) {
                        if out_f > 30000 {
                            dims.entry("vocab_size".to_string())
                                .or_insert(out_f as usize);
                        }
                    }
                }
                _ => {}
            }
        }

        SymbolicConfig { dims }
    }

    fn render_dim(&self, value: usize, preferred_name: &str) -> String {
        // Try preferred name first
        if !preferred_name.is_empty() {
            if let Some(&v) = self.config.dims.get(preferred_name) {
                if v == value {
                    return format!("cfg.{}", preferred_name);
                }
            }
        }

        // Try exact value match in any dim
        for (name, &v) in &self.config.dims {
            if v == value {
                return format!("cfg.{}", name);
            }
        }

        value.to_string()
    }

    /// Analyze the manifest and return a structured result for JSON output
    pub fn analyze(&self) -> AnalysisResult {
        let mut supported = 0;
        let mut unsupported = 0;
        let mut gap_counts: HashMap<String, usize> = HashMap::new();
        let mut layers = Vec::new();

        for (name, meta) in &self.manifest {
            if !meta.is_leaf {
                continue;
            }

            let is_supported = self.is_supported(&meta.module_type);
            if is_supported {
                supported += 1;
            } else {
                unsupported += 1;
                *gap_counts.entry(meta.module_type.clone()).or_default() += 1;
            }

            layers.push(LayerInfo {
                name: name.clone(),
                module_type: meta.module_type.clone(),
                supported: is_supported,
                input_shapes: meta.input_shapes.clone(),
                output_shapes: meta.output_shapes.clone(),
            });
        }

        // Sort layers by name for consistent output
        layers.sort_by(|a, b| a.name.cmp(&b.name));

        let gaps: Vec<GapInfo> = gap_counts
            .into_iter()
            .map(|(t, c)| GapInfo {
                suggestion: self.get_suggestion(&t),
                module_type: t,
                count: c,
            })
            .collect();

        let total = supported + unsupported;
        let coverage_percent = if total > 0 {
            (supported as f32 / total as f32) * 100.0
        } else {
            100.0
        };

        AnalysisResult {
            supported,
            unsupported,
            total,
            coverage_percent,
            gaps,
            layers,
        }
    }

    /// Check if a module type is supported by the codegen
    pub fn is_supported(&self, module_type: &str) -> bool {
        // Check GPT2 helper first
        if gpt2::map_type(module_type).is_some() {
            return true;
        }

        matches!(
            module_type,
            "Linear"
                | "Conv1d"
                | "LayerNorm"
                | "Embedding"
                | "ReLU"
                | "GELU"
                | "Sigmoid"
                | "Tanh"
                | "ELU"
                | "LeakyReLU"
                | "Snake"
                | "BatchNorm1d"
                | "BatchNorm2d"
                | "LSTM"
                | "Mish"
                | "SiLU"
                | "CausalConv1d"
                | "Transpose"
                | "Conv1D"
                | "Dropout"
                | "NewGELUActivation"
                | "LoRACompatibleLinear"
                | "SinusoidalPosEmb"
        )
    }

    /// Get implementation suggestion for an unsupported module type
    pub fn get_suggestion(&self, module_type: &str) -> String {
        match module_type {
            "LSTM" => "Use /add-lstm workflow".to_string(),
            "BatchNorm1d" | "BatchNorm2d" => "Use /add-batchnorm workflow".to_string(),
            "Snake" | "ELU" => "Use /add-activations workflow".to_string(),
            _ => format!("Implement {} manually", module_type),
        }
    }

    pub fn generate_model_rs(&self, model_name: &str) -> String {
        let mut code = String::new();
        code.push_str("use candle_core::{Tensor, Result, Device, Shape};\n");
        code.push_str(
            "use candle_nn::{Linear, Conv1d, LayerNorm, Embedding, VarBuilder, Module};\n",
        );
        code.push_str("use pycandle_core::{PyChecker, py_check, Dropout, Transpose, Mish, CausalConv1d, SiLU, ReLU, GELU, Sigmoid, Tanh, ELU, LeakyReLU, Snake, BatchNorm1d, BatchNorm2d, LSTM};\n");

        // Add gpt2 import if GPT2 types are present
        if self.has_gpt2_types() {
            code.push_str("use pycandle_core::gpt2;\n");
        }
        code.push_str("\n");

        if !self.config.dims.is_empty() {
            code.push_str(&self.generate_config_struct());
            code.push_str("\n\n");
        }

        code.push_str(&self.generate_struct(model_name));
        code.push_str("\n\n");
        code.push_str(&self.generate_impl(model_name));

        code
    }

    fn generate_config_struct(&self) -> String {
        let mut lines = vec!["pub struct Config {".to_string()];
        let mut dims: Vec<_> = self.config.dims.iter().collect();
        dims.sort_by_key(|(k, _)| *k);

        for (name, value) in dims {
            lines.push(format!("    pub {}: usize, // {}", name, value));
        }
        lines.push("}".to_string());
        lines.join("\n")
    }

    fn has_gpt2_types(&self) -> bool {
        self.manifest
            .values()
            .any(|meta| gpt2::is_gpt2_type(&meta.module_type))
    }

    fn generate_struct(&self, model_name: &str) -> String {
        let mut lines = vec![format!("pub struct {} {{", model_name)];

        let mut layers: Vec<_> = self.manifest.iter().collect();
        layers.sort_by_key(|(k, _)| *k);

        for (layer_name, meta) in layers {
            if !meta.is_leaf {
                continue;
            }
            let clean_name = layer_name.replace(".", "_");
            let candle_type = self.map_type(&meta.module_type);
            lines.push(format!("    pub {}: {},", clean_name, candle_type));
        }

        lines.push("    pub checker: Option<PyChecker>,".to_string());
        lines.push("}".to_string());
        lines.join("\n")
    }

    fn map_type(&self, py_type: &str) -> String {
        // Check GPT2 helper first
        if let Some(t) = gpt2::map_type(py_type) {
            return t;
        }

        // Core types
        match py_type {
            "Linear" => "Linear".to_string(),
            "Conv1d" => "Conv1d".to_string(),
            "LayerNorm" => "LayerNorm".to_string(),
            "Embedding" => "Embedding".to_string(),
            // Activations
            "ReLU" => "ReLU".to_string(),
            "GELU" => "GELU".to_string(),
            "Sigmoid" => "Sigmoid".to_string(),
            "Tanh" => "Tanh".to_string(),
            "ELU" => "ELU".to_string(),
            "LeakyReLU" => "LeakyReLU".to_string(),
            "Snake" => "Snake".to_string(),
            "BatchNorm1d" => "BatchNorm1d".to_string(),
            "BatchNorm2d" => "BatchNorm2d".to_string(),
            "LSTM" => "LSTM".to_string(),
            "Mish" => "Mish".to_string(),
            "SiLU" => "SiLU".to_string(),
            "CausalConv1d" => "CausalConv1d".to_string(),
            "Transpose" => "Transpose".to_string(),
            "Conv1D" => "Linear".to_string(),
            "Dropout" => "Dropout".to_string(),
            "NewGELUActivation" => "candle_nn::Activation".to_string(),
            "LoRACompatibleLinear" => "Linear".to_string(),
            "SinusoidalPosEmb" => "SinusoidalPosEmb".to_string(),
            _ => format!("() /* TODO: {} */", py_type),
        }
    }

    fn generate_impl(&self, model_name: &str) -> String {
        let mut code = format!("impl {} {{\n", model_name);

        let load_args = if self.config.dims.is_empty() {
            "vb: VarBuilder, checker: Option<PyChecker>".to_string()
        } else {
            "cfg: Config, vb: VarBuilder, checker: Option<PyChecker>".to_string()
        };

        code.push_str(&format!(
            "    pub fn load({}) -> Result<Self> {{\n",
            load_args
        ));

        let mut layers: Vec<_> = self.manifest.iter().collect();
        layers.sort_by_key(|(k, _)| *k);

        for (layer_name, meta) in &layers {
            if !meta.is_leaf {
                continue;
            }
            let clean_name = layer_name.replace(".", "_");
            let init_call = self.generate_init(layer_name, meta);
            code.push_str(&format!("        let {} = {};\n", clean_name, init_call));
        }

        code.push_str("\n        Ok(Self {\n");
        for (layer_name, meta) in &layers {
            if !meta.is_leaf {
                continue;
            }
            code.push_str(&format!("            {},\n", layer_name.replace(".", "_")));
        }
        code.push_str("            checker,\n");
        code.push_str("        })\n");
        code.push_str("    }\n\n");

        if let Some(nodes) = &self.graph_nodes {
            code.push_str(&self.generate_forward_dag(nodes));
        } else {
            // Forward method
            let ret_type = self.get_forward_return_type();
            code.push_str(&format!(
                "    pub fn forward(&self, xs: &Tensor) -> {} {{\n",
                ret_type
            ));
            code.push_str("        let mut x = xs.clone();\n");

            for (layer_name, meta) in &layers {
                if !meta.is_leaf {
                    continue;
                }
                let clean_name = layer_name.replace(".", "_");
                let forward_call = if self.map_type(&meta.module_type) == "LSTM" {
                    format!("self.{}.forward(&x)?.0", clean_name)
                } else {
                    format!("self.{}.forward(&x)?", clean_name)
                };
                code.push_str(&format!("\n        // Layer: {}\n", layer_name));
                code.push_str(&format!("        x = {};\n", forward_call));
                code.push_str(&format!(
                    "        py_check!(self.checker, \"{}\", &x);\n",
                    layer_name
                ));
            }

            code.push_str("\n        Ok(x)\n");
            code.push_str("    }\n");
        }
        code.push_str("}\n");

        code
    }

    fn generate_forward_dag(&self, nodes: &[GraphNode]) -> String {
        let mut placeholders = Vec::new();
        for node in nodes {
            if node.op == "placeholder" {
                placeholders.push(node.name.clone());
            }
        }

        let mut code = String::new();
        let ret_type = self.get_forward_return_type();

        let inputs = if placeholders.len() <= 1 {
            "xs: &Tensor".to_string()
        } else {
            placeholders
                .iter()
                .enumerate()
                .map(|(i, _)| format!("xs{}: &Tensor", i))
                .collect::<Vec<_>>()
                .join(", ")
        };

        code.push_str(&format!(
            "    pub fn forward(&self, {}) -> {} {{\n",
            inputs, ret_type
        ));

        // Map python placeholder names to Rust input variables
        let mut var_map = HashMap::new();
        let mut node_types = HashMap::new();

        let mut placeholder_idx = 0;
        for node in nodes {
            match node.op.as_str() {
                "placeholder" => {
                    let var_name = if placeholders.len() <= 1 {
                        "xs".to_string()
                    } else {
                        format!("xs{}", placeholder_idx)
                    };
                    var_map.insert(node.name.clone(), var_name);
                    node_types.insert(node.name.clone(), ReturnType::Tensor);
                    placeholder_idx += 1;
                }
                "call_module" => {
                    let clean_name = node.target.replace(".", "_");
                    let var_name = format!("x_{}", node.name);
                    let input_var = if let Some(arg0) = node.args.get(0) {
                        match arg0 {
                            serde_json::Value::String(s) => {
                                var_map.get(s).cloned().unwrap_or(s.clone())
                            }
                            _ => {
                                let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                                first_p.cloned().unwrap_or("xs".to_string())
                            }
                        }
                    } else {
                        let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                        first_p.cloned().unwrap_or("xs".to_string())
                    };

                    let module_type = node.module_type.clone().unwrap_or_default();
                    let is_lstm = self.map_type(&module_type) == "LSTM";

                    let (forward_call, return_type) = if is_lstm {
                        // LSTM returns (output, (h, c))
                        (
                            format!("self.{}.forward(&{})?", clean_name, input_var),
                            ReturnType::Tuple,
                        )
                    } else {
                        let meta = self.manifest.get(&node.target);
                        let multi = meta.map(|m| m.output_shapes.len() > 1).unwrap_or(false);
                        if multi {
                            (
                                format!("self.{}.forward(&{})?", clean_name, input_var),
                                ReturnType::Tuple,
                            )
                        } else {
                            (
                                format!("self.{}.forward(&{})?", clean_name, input_var),
                                ReturnType::Tensor,
                            )
                        }
                    };

                    code.push_str(&format!("        let {} = {};\n", var_name, forward_call));

                    // Only do parity check on single Tensors for now to avoid tuple mismatch in py_check!
                    if return_type == ReturnType::Tensor {
                        code.push_str(&format!(
                            "        py_check!(self.checker, \"{}\", &{});\n",
                            node.target, var_name
                        ));
                    }

                    var_map.insert(node.name.clone(), var_name.clone());
                    node_types.insert(node.name.clone(), return_type);
                }
                "call_function" => {
                    let var_name = format!("x_{}", node.name);
                    let mut resolved_args = Vec::new();
                    for arg in &node.args {
                        resolved_args.push(self.resolve_fx_arg(arg, &var_map));
                    }

                    let (expr, return_type) =
                        self.map_fx_op(&node.target, &resolved_args, &var_map, &node_types);
                    code.push_str(&format!("        let {} = {};\n", var_name, expr));
                    var_map.insert(node.name.clone(), var_name);
                    node_types.insert(node.name.clone(), return_type);
                }
                "call_method" => {
                    let var_name = format!("x_{}", node.name);
                    let mut resolved_args = Vec::new();
                    for arg in &node.args {
                        resolved_args.push(self.resolve_fx_arg(arg, &var_map));
                    }

                    if !resolved_args.is_empty() {
                        let self_var_name = match &node.args[0] {
                            serde_json::Value::String(s) => s.clone(),
                            _ => "".to_string(),
                        };
                        let self_var = &resolved_args[0];
                        let method_args = &resolved_args[1..];
                        let (expr, return_type) = self.map_fx_method(
                            &node.target,
                            self_var,
                            &self_var_name,
                            method_args,
                            &node_types,
                        );
                        code.push_str(&format!("        let {} = {};\n", var_name, expr));
                        var_map.insert(node.name.clone(), var_name);
                        node_types.insert(node.name.clone(), return_type);
                    }
                }
                "output" => {
                    let out_var = if let Some(arg0) = node.args.get(0) {
                        match arg0 {
                            serde_json::Value::String(s) => {
                                var_map.get(s).cloned().unwrap_or(s.clone())
                            }
                            serde_json::Value::Array(arr) => {
                                let items: Vec<String> = arr
                                    .iter()
                                    .map(|v| self.resolve_fx_arg(v, &var_map))
                                    .collect();
                                format!("({})", items.join(", "))
                            }
                            _ => {
                                let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                                first_p.cloned().unwrap_or("xs".to_string())
                            }
                        }
                    } else {
                        let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                        first_p.cloned().unwrap_or("xs".to_string())
                    };
                    code.push_str(&format!("        Ok({})\n", out_var));
                }
                _ => {}
            }
        }

        code.push_str("    }\n");
        code
    }

    fn resolve_fx_arg(&self, arg: &serde_json::Value, var_map: &HashMap<String, String>) -> String {
        match arg {
            serde_json::Value::String(s) => var_map.get(s).cloned().unwrap_or(s.clone()),
            serde_json::Value::Array(arr) => {
                let items: Vec<String> = arr
                    .iter()
                    .map(|v| self.resolve_fx_arg(v, var_map))
                    .collect();
                format!("&[{}]", items.join(", "))
            }
            _ => arg.to_string(),
        }
    }

    fn map_fx_op(
        &self,
        target: &str,
        args: &[String],
        var_map: &HashMap<String, String>,
        node_types: &HashMap<String, ReturnType>,
    ) -> (String, ReturnType) {
        let target_lower = target.to_lowercase();

        // Common binary ops
        if target_lower.contains("add") && args.len() >= 2 {
            return (
                format!("(&{} + &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if target_lower.contains("sub") && args.len() >= 2 {
            return (
                format!("(&{} - &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if target_lower.contains("mul") && args.len() >= 2 {
            return (
                format!("(&{} * &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if (target_lower.contains("div") || target_lower.contains("truediv")) && args.len() >= 2 {
            return (
                format!("(&{} / &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }

        // Functional ops
        match target {
            "torch.cat" | "cat" => {
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                let mut tensors = args[0].clone();
                if tensors.starts_with("&[") {
                    // Convert &[x, y] to &[&x, &y] for Candle's cat
                    let content = &tensors[2..tensors.len() - 1];
                    let items: Vec<String> =
                        content.split(", ").map(|s| format!("&{}", s)).collect();
                    tensors = format!("&[{}]", items.join(", "));
                }
                (
                    format!("Tensor::cat({}, {})?", tensors, dim),
                    ReturnType::Tensor,
                )
            }
            "torch.chunk" | "chunk" => {
                let chunks = args.get(1).map(|s| s.as_str()).unwrap_or("2");
                let dim = args.get(2).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.chunk({}, {})?", args[0], chunks, dim),
                    ReturnType::Vec,
                )
            }
            "torch.split" | "split" => {
                let split_size = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                let dim = args.get(2).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.split({}, {})?", args[0], split_size, dim),
                    ReturnType::Vec,
                )
            }
            "torch.relu" | "relu" => (format!("{}.relu()?", args[0]), ReturnType::Tensor),
            "torch.sigmoid" | "sigmoid" => (
                format!("candle_nn::ops::sigmoid(&{})?", args[0]),
                ReturnType::Tensor,
            ),
            "torch.tanh" | "tanh" => (format!("{}.tanh()?", args[0]), ReturnType::Tensor),
            "torch.squeeze" | "squeeze" => {
                if args.len() > 1 {
                    (
                        format!("{}.squeeze({})?", args[0], args[1]),
                        ReturnType::Tensor,
                    )
                } else {
                    (format!("{}.squeeze(0)?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.unsqueeze" | "unsqueeze" => (
                format!("{}.unsqueeze({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.pow" | "pow" => (
                format!("{}.powf({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.sqrt" | "sqrt" => (format!("{}.sqrt()?", args[0]), ReturnType::Tensor),
            "torch.exp" | "exp" => (format!("{}.exp()?", args[0]), ReturnType::Tensor),
            "torch.log" | "log" => (format!("{}.log()?", args[0]), ReturnType::Tensor),
            "torch.abs" | "abs" => (format!("{}.abs()?", args[0]), ReturnType::Tensor),
            "torch.sum" | "sum" => {
                if args.len() > 1 {
                    (format!("{}.sum({})?", args[0], args[1]), ReturnType::Tensor)
                } else {
                    (format!("{}.sum_all()?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.mean" | "mean" => {
                if args.len() > 1 {
                    (
                        format!("{}.mean({})?", args[0], args[1]),
                        ReturnType::Tensor,
                    )
                } else {
                    (format!("{}.mean_all()?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.transpose" => (
                format!("{}.transpose({}, {})?", args[0], args[1], args[2]),
                ReturnType::Tensor,
            ),
            "torch.reshape" => (
                format!("{}.reshape({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.permute" => (
                format!("{}.permute({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "operator.getitem" => {
                let idx = &args[1];

                // Find the source variable name to check its type
                let src_name = var_map
                    .iter()
                    .find(|(_, v)| *v == &args[0])
                    .map(|(k, _)| k.as_str())
                    .unwrap_or(&args[0]);

                let src_type = node_types
                    .get(src_name)
                    .cloned()
                    .unwrap_or(ReturnType::Tensor);

                match src_type {
                    ReturnType::Tuple => {
                        // Tuple indexing: x.0, x.1
                        if let Ok(i) = idx.parse::<usize>() {
                            (format!("{}.{}", args[0], i), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                    ReturnType::Vec => {
                        // Vec indexing: x[0]
                        if let Ok(i) = idx.parse::<usize>() {
                            (format!("{}[{}].clone()", args[0], i), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                    ReturnType::Tensor => {
                        if idx.contains("slice(") || idx == "None" || idx.starts_with("&[") {
                            // Map to .i() for indexing/slicing
                            let mut cleaned = idx.clone();
                            if cleaned.starts_with("&[") {
                                cleaned = cleaned[2..cleaned.len() - 1].to_string();
                            }

                            let items: Vec<String> = cleaned
                                .split(",")
                                .enumerate()
                                .map(|(i, s)| self.parse_slice_item(s.trim(), &args[0], i))
                                .collect();

                            let final_idx = if items.len() == 1 {
                                items[0].clone()
                            } else {
                                format!("({})", items.join(", "))
                            };

                            (format!("{}.i({})?", args[0], final_idx), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                }
            }
            _ => {
                if target_lower.contains("add") && args.len() >= 2 {
                    return (
                        format!("(&{} + &{})?", args[0], args[1]),
                        ReturnType::Tensor,
                    );
                }
                (
                    format!("todo!(/* function: {} */)", target),
                    ReturnType::Tensor,
                )
            }
        }
    }

    fn parse_slice_item(&self, item: &str, tensor_name: &str, dim_idx: usize) -> String {
        if item == "None" {
            return "..".to_string();
        }

        // Handle single negative index
        if let Ok(val) = item.parse::<isize>() {
            if val < 0 {
                return format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs());
            }
            return item.to_string();
        }

        if !item.contains("slice(") {
            return item.to_string();
        }

        // Parse slice(start, stop, step)
        let content = item
            .strip_prefix("slice(")
            .and_then(|s| s.strip_suffix(")"))
            .unwrap_or(item);
        let parts: Vec<&str> = content.split(",").map(|s| s.trim()).collect();

        let start_str = parts.get(0).copied().unwrap_or("None");
        let stop_str = parts.get(1).copied().unwrap_or("None");

        let start = if let Ok(val) = start_str.parse::<isize>() {
            if val < 0 {
                format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs())
            } else {
                start_str.to_string()
            }
        } else {
            start_str.to_string()
        };

        let stop = if let Ok(val) = stop_str.parse::<isize>() {
            if val < 0 {
                format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs())
            } else {
                stop_str.to_string()
            }
        } else {
            stop_str.to_string()
        };

        match (start.as_str(), stop.as_str()) {
            ("None", "None") => "..".to_string(),
            ("None", stop) => format!("..{}", stop),
            (start, "None") => format!("{}..", start),
            (start, stop) => format!("{}..{}", start, stop),
        }
    }

    fn map_fx_method(
        &self,
        method: &str,
        self_var: &str,
        self_var_name: &str,
        args: &[String],
        node_types: &HashMap<String, ReturnType>,
    ) -> (String, ReturnType) {
        let _src_type = node_types
            .get(self_var_name)
            .cloned()
            .unwrap_or(ReturnType::Tensor);

        match method {
            "view" | "reshape" => {
                if args.len() == 1 && args[0].starts_with("&[") {
                    (
                        format!("{}.reshape({})?", self_var, args[0]),
                        ReturnType::Tensor,
                    )
                } else {
                    (
                        format!("{}.reshape(vec![{}])?", self_var, args.join(", ")),
                        ReturnType::Tensor,
                    )
                }
            }
            "flatten" => (format!("{}.flatten_all()?", self_var), ReturnType::Tensor),
            "transpose" => (
                format!("{}.transpose({}, {})?", self_var, args[0], args[1]),
                ReturnType::Tensor,
            ),
            "permute" => (
                format!("{}.permute({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "unsqueeze" => (
                format!("{}.unsqueeze({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "squeeze" => (
                format!("{}.squeeze({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "chunk" => {
                let chunks = args.get(0).map(|s| s.as_str()).unwrap_or("2");
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.chunk({}, {})?", self_var, chunks, dim),
                    ReturnType::Vec,
                )
            }
            "split" => {
                let split_size = args.get(0).map(|s| s.as_str()).unwrap_or("1");
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                (
                    format!("{}.split({}, {})?", self_var, split_size, dim),
                    ReturnType::Vec,
                )
            }
            "t" => (format!("{}.t()?", self_var), ReturnType::Tensor),
            "contiguous" => (format!("{}.contiguous()?", self_var), ReturnType::Tensor),
            "size" => {
                if args.is_empty() {
                    (format!("{}.dims().to_vec()", self_var), ReturnType::Tensor)
                } else {
                    (
                        format!("{}.dim({})?", self_var, args[0]),
                        ReturnType::Tensor,
                    )
                }
            }
            _ => (
                format!("todo!(/* method: {} on {} */)", method, self_var),
                ReturnType::Tensor,
            ),
        }
    }

    fn generate_init(&self, layer_name: &str, meta: &LayerMeta) -> String {
        // Check GPT2 helper first
        if let Some(init) = gpt2::generate_init(layer_name, meta, &self.config.dims) {
            return init;
        }

        // Core types
        match meta.module_type.as_str() {
            "Linear" | "LoRACompatibleLinear" => {
                let in_f_val = meta.config["in_features"].as_u64().unwrap_or(0) as usize;
                let out_f_val = meta.config["out_features"].as_u64().unwrap_or(0) as usize;
                let in_f = self.render_dim(in_f_val, "hidden_dim");
                let out_f = self.render_dim(out_f_val, "");
                let bias = meta.config["bias"].as_bool().unwrap_or(true);

                // Check for weight shape to detect transpose needs
                let needs_transpose = meta
                    .config
                    .get("weight_shape")
                    .and_then(|v| v.as_array())
                    .map(|arr| {
                        let dims: Vec<u64> = arr.iter().filter_map(|x| x.as_u64()).collect();
                        // PyTorch Linear stores (out, in), if we see (in, out) we need transpose
                        dims.len() == 2 && dims[0] == in_f_val as u64 && dims[1] == out_f_val as u64
                    })
                    .unwrap_or(false);

                if needs_transpose {
                    format!(
                        "{{ let w = vb.pp(\"{}\").get(({}, {}), \"weight\")?.t()?; \
                         let b = {}; Linear::new(w, b) }}",
                        layer_name,
                        in_f,
                        out_f,
                        if bias {
                            format!("Some(vb.pp(\"{}\").get({}, \"bias\")?)", layer_name, out_f)
                        } else {
                            "None".to_string()
                        }
                    )
                } else if bias {
                    format!(
                        "candle_nn::linear({}, {}, vb.pp(\"{}\"))?",
                        in_f, out_f, layer_name
                    )
                } else {
                    format!(
                        "candle_nn::linear_no_bias({}, {}, vb.pp(\"{}\"))?",
                        in_f, out_f, layer_name
                    )
                }
            }
            "Conv1d" => {
                let in_c = self.render_dim(
                    meta.config["in_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let out_c = self.render_dim(
                    meta.config["out_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let k = meta.config["kernel_size"].as_u64().unwrap_or(0);
                let s = meta.config["stride"].as_u64().unwrap_or(1);
                let p = meta.config["padding"].as_u64().unwrap_or(0);
                format!(
                    "candle_nn::conv1d({}, {}, {}, candle_nn::Conv1dConfig {{ stride: {}, padding: {}, ..Default::default() }}, vb.pp(\"{}\"))?",
                    in_c, out_c, k, s, p, layer_name
                )
            }
            "LayerNorm" => {
                let shape: Vec<usize> =
                    serde_json::from_value(meta.config["normalized_shape"].clone())
                        .unwrap_or_default();
                let eps = meta.config["eps"].as_f64().unwrap_or(1e-5);
                // Use a single value if shape is [N], otherwise use a Slice
                let shape_str = if shape.len() == 1 {
                    format!("{}", shape[0])
                } else {
                    format!("Shape::from(vec!{:?})", shape)
                };
                format!(
                    "candle_nn::layer_norm({}, candle_nn::LayerNormConfig {{ eps: {:.1e}, ..Default::default() }}, vb.pp(\"{}\"))?",
                    shape_str, eps, layer_name
                )
            }
            "Embedding" => {
                let n = self.render_dim(
                    meta.config["num_embeddings"].as_u64().unwrap_or(0) as usize,
                    "vocab_size",
                );
                let d = self.render_dim(
                    meta.config["embedding_dim"].as_u64().unwrap_or(0) as usize,
                    "hidden_dim",
                );
                format!(
                    "candle_nn::embedding({}, {}, vb.pp(\"{}\"))?",
                    n, d, layer_name
                )
            }
            // Activations - stateless
            "ReLU" => "ReLU".to_string(),
            "GELU" => "GELU".to_string(),
            "Sigmoid" => "Sigmoid".to_string(),
            "Tanh" => "Tanh".to_string(),
            // Activations - parameterized
            "ELU" => {
                let alpha = meta
                    .config
                    .get("alpha")
                    .and_then(|v| v.as_f64())
                    .unwrap_or(1.0);
                format!("ELU::new({})", alpha)
            }
            "LeakyReLU" => {
                let slope = meta
                    .config
                    .get("negative_slope")
                    .and_then(|v| v.as_f64())
                    .unwrap_or(0.01);
                format!("LeakyReLU::new({})", slope)
            }
            "Snake" => {
                let in_features = meta
                    .config
                    .get("in_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0);
                format!("Snake::load(vb.pp(\"{}\"), {})?", layer_name, in_features)
            }
            "BatchNorm1d" => {
                let num_features = meta
                    .config
                    .get("num_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as usize;
                format!(
                    "BatchNorm1d::load(vb.pp(\"{}\"), {})?",
                    layer_name, num_features
                )
            }
            "BatchNorm2d" => {
                let num_features = meta
                    .config
                    .get("num_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as usize;
                format!(
                    "BatchNorm2d::load(vb.pp(\"{}\"), {})?",
                    layer_name, num_features
                )
            }
            "LSTM" => {
                let input_size = self.render_dim(
                    meta.config
                        .get("input_size")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(0) as usize,
                    "hidden_dim",
                );
                let hidden_size = self.render_dim(
                    meta.config
                        .get("hidden_size")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(0) as usize,
                    "",
                );
                let num_layers = self.render_dim(
                    meta.config
                        .get("num_layers")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(1) as usize,
                    "n_layers",
                );
                format!(
                    "LSTM::load(vb.pp(\"{}\"), {}, {}, {})?",
                    layer_name, input_size, hidden_size, num_layers
                )
            }
            "CausalConv1d" => {
                let in_c = self.render_dim(
                    meta.config["in_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let out_c = self.render_dim(
                    meta.config["out_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let k = meta.config["kernel_size"].as_u64().unwrap_or(0);
                let s = meta.config["stride"].as_u64().unwrap_or(1);
                let bias = meta.config["bias"].as_bool().unwrap_or(true);
                format!(
                    "CausalConv1d::load(vb.pp(\"{}\"), {}, {}, {}, {}, {})?",
                    layer_name, in_c, out_c, k, s, bias
                )
            }
            "Mish" => "Mish".to_string(),
            "SiLU" => "SiLU".to_string(),
            // GPT2 specific
            "Conv1D" => {
                let out_f_val = meta.config["nf"].as_u64().unwrap_or(0) as usize;
                let out_f = self.render_dim(out_f_val, "");
                let nx_val =
                    if let Some(ws) = meta.config.get("weight_shape").and_then(|v| v.as_array()) {
                        ws.get(0).and_then(|x| x.as_u64()).unwrap_or(0) as usize
                    } else {
                        0
                    };
                let nx = self.render_dim(nx_val, "hidden_dim");

                format!(
                    "candle_nn::linear({}, {}, vb.pp(\"{}\"))?",
                    nx, out_f, layer_name
                )
            }
            "NewGELUActivation" => "candle_nn::Activation::NewGelu".to_string(),
            "Dropout" => "Dropout::new()".to_string(),
            "Transpose" => {
                let d0 = meta.config["dim0"].as_u64().unwrap_or(1);
                let d1 = meta.config["dim1"].as_u64().unwrap_or(2);
                format!("Transpose::new({}, {})", d0, d1)
            }
            "SinusoidalPosEmb" => {
                let dim = meta.output_shapes[0][1];
                format!("SinusoidalPosEmb::new({})", dim)
            }
            _ => format!(
                "todo!(\"Implement initialization for {}\")",
                meta.module_type
            ),
        }
    }

    fn get_forward_return_type(&self) -> String {
        if let Some(nodes) = &self.graph_nodes {
            for node in nodes {
                if node.op == "output" {
                    if let Some(arg0) = node.args.get(0) {
                        if let Some(arr) = arg0.as_array() {
                            if arr.len() > 1 {
                                let tensors = vec!["Tensor"; arr.len()];
                                return format!("Result<({})>", tensors.join(", "));
                            }
                        }
                    }
                }
            }
        }
        "Result<Tensor>".to_string()
    }
}
</file>

<file path="crates/pycandle/Cargo.toml">
[package]
name = "pycandle"
description = "CLI for PyTorch ‚Üí Candle porting with layer-wise parity verification"
version.workspace = true
edition.workspace = true
license.workspace = true

[[bin]]
name = "pycandle"
path = "src/main.rs"

[dependencies]
pycandle-core.workspace = true
candle-core.workspace = true
clap.workspace = true
colored.workspace = true
anyhow.workspace = true
serde.workspace = true
serde_json.workspace = true
walkdir.workspace = true
regex.workspace = true
ratatui = "0.29.0"
crossterm = "0.28.1"
toml = "0.8"
</file>

<file path="crates/pycandle/src/main.rs">
//! PyCandle CLI
//!
//! Command-line interface for PyTorch ‚Üí Candle porting.

mod dashboard;
mod init;
mod report;
mod test_gen;
mod todos;

use anyhow::{Context, Result};
use clap::{Parser, Subcommand};
use pycandle_core::LayerMeta;
use pycandle_core::codegen::Codegen;
use report::ReportGenerator;
use std::collections::HashMap;
use std::path::PathBuf;
use std::process::Command;

#[derive(Parser)]
#[command(name = "pycandle")]
#[command(about = "A tool for bit-perfect parity checking between PyTorch and Candle", long_about = None)]
struct Cli {
    /// Output in JSON format for agent consumption
    #[arg(long, global = true)]
    json: bool,

    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Record activations from a PyTorch model
    Record {
        /// Path to the Python script that defines and runs the model
        #[arg(short, long)]
        script: PathBuf,

        /// Project name for the trace
        #[arg(short, long)]
        name: String,

        /// Output directory for the trace and manifest
        #[arg(short, long, default_value = "pycandle_trace")]
        out: PathBuf,
    },
    /// Generate Candle code from a manifest
    Codegen {
        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output path for the generated Rust file or directory
        #[arg(short, long)]
        out: PathBuf,

        /// Name of the model struct to generate
        #[arg(long, default_value = "MyModel")]
        model: String,

        /// Analyze without generating code
        #[arg(long)]
        analyze_only: bool,
    },
    /// Extract and manage TODO markers in generated code
    Todos {
        /// Path to generated Rust file or directory
        #[arg(short, long)]
        path: PathBuf,

        /// Just check if TODOs remain (exit code 1 if any)
        #[arg(long)]
        check: bool,
    },
    /// Generate an HTML coverage report
    Report {
        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output HTML file path
        #[arg(short, long, default_value = "pycandle_report.html")]
        out: PathBuf,
    },
    Weights {
        #[command(subcommand)]
        action: WeightActions,
    },
    /// Launch the TUI Parity Dashboard
    Dashboard {
        // Optional arguments if we want to pass filter to cargo test
        #[arg(last = true)]
        args: Vec<String>,
    },
    /// Initialize a new project with boilerplate
    Init {
        /// Optional project name
        #[arg(short, long)]
        name: Option<String>,
    },
    /// Generate automated parity test
    GenTest {
        /// Name of the model struct (must match generated code)
        #[arg(long, default_value = "MyModel")]
        model: String,

        /// Output path for the generated test file
        #[arg(short, long, default_value = "tests/parity.rs")]
        out: PathBuf,
    },
}

#[derive(Subcommand)]
enum WeightActions {
    /// Surgically extract weights used in a manifest
    Extract {
        /// Path to PyTorch checkpoint (.bin, .pt, .safetensors)
        #[arg(short, long)]
        checkpoint: PathBuf,

        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output .safetensors path
        #[arg(short, long)]
        out: PathBuf,

        /// Optional JSON mapping file for renaming
        #[arg(long)]
        map: Option<PathBuf>,
    },
    /// Rename keys in a safetensors file using a mapping
    Map {
        /// Input .safetensors file
        #[arg(short, long)]
        input: PathBuf,

        /// Output .safetensors file
        #[arg(short, long)]
        out: PathBuf,

        /// JSON mapping file
        #[arg(short, long)]
        map: PathBuf,
    },
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Record { script, name, out } => {
            println!(
                "üöÄ Recording trace for project '{}' using script '{:?}'...",
                name, script
            );

            let status = Command::new("uv")
                .arg("run")
                .arg("python")
                .arg(script)
                .spawn()
                .context("Failed to spawn uv run")?
                .wait()
                .context("Failed to wait for python process")?;

            if status.success() {
                println!("‚úÖ Recording complete. Files should be in {:?}", out);
            } else {
                eprintln!("‚ùå Recording failed.");
            }
        }
        Commands::Codegen {
            manifest: start_path,
            out: out_path,
            model,
            analyze_only,
        } => {
            // Find manifests: if directory, glob *.json, else use file
            let manifest_files = if start_path.is_dir() {
                std::fs::read_dir(&start_path)?
                    .filter_map(|entry| {
                        let path = entry.ok()?.path();
                        if path.extension()?.to_str()? == "json"
                            && path.file_name()?.to_str()?.ends_with("_manifest.json")
                        {
                            Some(path)
                        } else {
                            None
                        }
                    })
                    .collect::<Vec<_>>()
            } else {
                vec![start_path]
            };

            if manifest_files.is_empty() {
                eprintln!("‚ùå No manifest files found.");
                return Ok(());
            }

            for manifest_path in manifest_files {
                let manifest_content = std::fs::read_to_string(&manifest_path)
                    .with_context(|| format!("Failed to read manifest at {:?}", manifest_path))?;

                // Full manifest structure including optional graph
                #[derive(serde::Deserialize)]
                struct Manifest {
                    #[serde(flatten)]
                    layers: HashMap<String, serde_json::Value>,
                    #[serde(rename = "_graph_nodes")]
                    graph_nodes: Option<Vec<pycandle_core::codegen::GraphNode>>,
                    #[serde(rename = "_graph_code")]
                    graph_code: Option<String>,
                    #[serde(rename = "_symbolic_hints")]
                    symbolic_hints: Option<HashMap<String, usize>>,
                }

                let full_manifest: Manifest = serde_json::from_str(&manifest_content)
                    .context("Failed to parse manifest JSON")?;

                // Filter out internal keys starting with "_"
                let layers: HashMap<String, LayerMeta> = full_manifest
                    .layers
                    .into_iter()
                    .filter(|(k, _)| !k.starts_with('_'))
                    .map(|(k, v)| {
                        let meta: LayerMeta = serde_json::from_value(v)
                            .with_context(|| format!("Failed to parse LayerMeta for {}", k))?;
                        Ok((k, meta))
                    })
                    .collect::<Result<_>>()?;

                let mut generator = Codegen::new(layers, full_manifest.symbolic_hints);
                if let Some(nodes) = full_manifest.graph_nodes {
                    generator = generator.with_graph(nodes);
                }

                if analyze_only || cli.json {
                    let analysis = generator.analyze();

                    if cli.json {
                        println!(
                            "{}",
                            serde_json::to_string_pretty(&analysis)
                                .context("Failed to serialize analysis")?
                        );
                    } else {
                        println!("üìä Analysis of {:?}:", manifest_path);
                        println!(
                            "  Supported: {}/{} ({:.1}%)",
                            analysis.supported, analysis.total, analysis.coverage_percent
                        );
                        println!("  Unsupported: {}", analysis.unsupported);
                        if !analysis.gaps.is_empty() {
                            println!("\n  Gaps:");
                            for gap in &analysis.gaps {
                                println!(
                                    "    - {}: {} occurrence(s) ‚Üí {}",
                                    gap.module_type, gap.count, gap.suggestion
                                );
                            }
                        }
                    }
                }

                if !analyze_only {
                    // Determine output path
                    let final_out = if out_path.is_dir() {
                        let stem = manifest_path.file_stem().unwrap().to_str().unwrap();
                        let name = stem.replace("_manifest", "");
                        out_path.join(format!("generated_{}.rs", name))
                    } else {
                        // If multiple manifests but one output file, this is ambiguous/wrong unless overwriting.
                        // We'll enforce directory output if input is directory.
                        out_path.clone()
                    };

                    println!(
                        "üèóÔ∏è Generating Candle code from manifest '{:?}'...",
                        manifest_path
                    );

                    // Use model name from CLI args for struct name.
                    // TODO: maybe derive struct name from manifest filename too?
                    let code = generator.generate_model_rs(&model);

                    std::fs::write(&final_out, code).with_context(|| {
                        format!("Failed to write generated code to {:?}", final_out)
                    })?;

                    println!("‚úÖ Code generated successfully at {:?}", final_out);
                }
            }
        }
        Commands::Todos { path, check } => {
            let files = if path.is_dir() {
                // simple recursion or flat for now? Let's just do one level or walkdir if needed.
                // For now, let's use fs::read_dir and filter .rs
                std::fs::read_dir(&path)?
                    .filter_map(|entry| {
                        let p = entry.ok()?.path();
                        if p.extension()?.to_str()? == "rs" {
                            Some(p)
                        } else {
                            None
                        }
                    })
                    .collect::<Vec<_>>()
            } else {
                vec![path]
            };

            let mut any_todos = false;

            for file_path in files {
                let content = std::fs::read_to_string(&file_path)
                    .with_context(|| format!("Failed to read file at {:?}", file_path))?;

                let todos = todos::extract_todos(&content);
                if !todos.is_empty() {
                    any_todos = true;
                }

                let report = todos::generate_report(file_path.to_str().unwrap_or("unknown"), todos);

                if cli.json {
                    println!(
                        "{}",
                        serde_json::to_string_pretty(&report)
                            .context("Failed to serialize report")?
                    );
                } else {
                    println!("üìã TODOs in {:?}:", file_path);
                    println!("   Total: {}", report.total);
                    if !report.by_type.is_empty() {
                        println!("\n   By type:");
                        let mut types: Vec<_> = report.by_type.iter().collect();
                        types.sort_by_key(|(_, count)| std::cmp::Reverse(*count));
                        for (t, c) in types {
                            println!("   - {}: {}", t, c);
                        }
                    }
                    if !report.todos.is_empty() {
                        println!("\n   Details:");
                        for todo in &report.todos {
                            println!(
                                "   L{}: {} ({}) ‚Üí {}",
                                todo.line, todo.field_name, todo.module_type, todo.suggestion
                            );
                        }
                    }
                }
            }

            if check && any_todos {
                std::process::exit(1);
            }
        }
        Commands::Report { manifest, out } => {
            let manifest_content = std::fs::read_to_string(&manifest)
                .with_context(|| format!("Failed to read manifest at {:?}", manifest))?;

            let manifest_data: HashMap<String, LayerMeta> =
                serde_json::from_str(&manifest_content).context("Failed to parse manifest JSON")?;

            let generator = ReportGenerator::new(manifest_data);
            let data = generator.analyze();
            let html = generator.generate_html(&data);

            std::fs::write(&out, html)
                .with_context(|| format!("Failed to write report to {:?}", out))?;

            println!("üìä Report generated: {:?}", out);
        }
        Commands::Weights { action } => match action {
            WeightActions::Extract {
                checkpoint,
                manifest,
                out,
                map,
            } => {
                println!("üî™ Performing surgical weight extraction...");
                let mut cmd = Command::new("uv");
                cmd.arg("run")
                    .arg("python")
                    .arg("py/weight_extractor.py")
                    .arg("--checkpoint")
                    .arg(&checkpoint)
                    .arg("--manifest")
                    .arg(&manifest)
                    .arg("--out")
                    .arg(&out);

                if let Some(m) = map {
                    cmd.arg("--map").arg(m);
                }

                let status = cmd.spawn()?.wait()?;
                if status.success() {
                    println!("‚úÖ Extraction complete: {:?}", out);
                } else {
                    anyhow::bail!("Weight extraction failed");
                }
            }
            WeightActions::Map { input, out, map } => {
                println!("üîÑ Renaming weights using map {:?}...", map);
                let map_content = std::fs::read_to_string(&map)?;
                let mapper = pycandle_core::WeightMapper::from_json(&map_content)?;

                let weights = candle_core::safetensors::load(&input, &candle_core::Device::Cpu)?;
                let mut renamed_weights = HashMap::new();
                for (name, tensor) in weights {
                    renamed_weights.insert(mapper.map_key(&name), tensor);
                }

                candle_core::safetensors::save(&renamed_weights, &out)?;
                println!("‚úÖ Renaming complete: {:?}", out);
            }
        },
        Commands::Dashboard { args } => {
            dashboard::run_dashboard(&args)?;
        }
        Commands::Init { name } => {
            init::run_init(name)?;
        }
        Commands::GenTest { model, out } => {
            println!("üß™ Generating test harness for model '{}'...", model);
            let generator = test_gen::TestGenerator::new(model);
            let code = generator.generate_test_file();

            if let Some(parent) = out.parent() {
                std::fs::create_dir_all(parent)
                    .with_context(|| format!("Failed to create directory {:?}", parent))?;
            }

            std::fs::write(&out, code)
                .with_context(|| format!("Failed to write test file to {:?}", out))?;

            println!("‚úÖ Test generated at {:?}", out);
        }
    }

    Ok(())
}
</file>

<file path="py/spy.py">
import os
import json
import torch
import torch.nn as nn
from safetensors.torch import save_file
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict
from collections import defaultdict

@dataclass
class LayerMeta:
    name: str
    module_type: str
    input_shapes: List[List[int]]
    output_shapes: List[List[int]]
    parameters: List[str]
    is_leaf: bool
    config: Dict[str, Any]

class GoldenRecorder:
    def __init__(self, output_dir: str = "pycandle_trace", keep_dtype: bool = False):
        self.output_dir = output_dir
        self.keep_dtype = keep_dtype
        self.records: Dict[str, torch.Tensor] = {}
        self.manifest: Dict[str, LayerMeta] = {}
        self.call_counts = defaultdict(int)
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def _get_module_config(self, module: nn.Module) -> Dict[str, Any]:
        cfg = {}
        if isinstance(module, nn.Linear):
            cfg = {
                "in_features": module.in_features,
                "out_features": module.out_features,
                "bias": module.bias is not None,
                # Record actual weight shape for transpose detection
                "weight_shape": list(module.weight.shape),  # [out, in] in PyTorch
            }
        elif isinstance(module, nn.Conv1d) or type(module).__name__ == "Conv1d":
            cfg = {
                "in_channels": module.in_channels,
                "out_channels": module.out_channels,
                "kernel_size": module.kernel_size[0] if isinstance(module.kernel_size, (list, tuple)) else module.kernel_size,
                "stride": module.stride[0] if isinstance(module.stride, (list, tuple)) else module.stride,
                "padding": module.padding[0] if isinstance(module.padding, (list, tuple)) else module.padding,
                "bias": module.bias is not None
            }
        # HF GPT2 often uses Conv1D
        elif type(module).__name__ == "Conv1D":
            cfg = {
                "in_features": module.weight.shape[0],
                "out_features": module.weight.shape[1],
                "bias": module.bias is not None,
                "weight_shape": list(module.weight.shape)
            }
        elif isinstance(module, nn.LayerNorm):
            cfg = {"normalized_shape": list(module.normalized_shape), "eps": module.eps}
        elif isinstance(module, nn.Embedding):
            cfg = {"num_embeddings": module.num_embeddings, "embedding_dim": module.embedding_dim}
        # Activation functions
        elif isinstance(module, nn.ELU):
            cfg = {"alpha": module.alpha}
        elif isinstance(module, nn.LeakyReLU):
            cfg = {"negative_slope": module.negative_slope}
        elif isinstance(module, nn.BatchNorm1d):
            cfg = {"num_features": module.num_features, "eps": module.eps}
        elif isinstance(module, nn.BatchNorm2d):
            cfg = {"num_features": module.num_features, "eps": module.eps}
        elif isinstance(module, nn.LSTM):
            cfg = {
                "input_size": module.input_size,
                "hidden_size": module.hidden_size,
                "num_layers": module.num_layers,
                "batch_first": module.batch_first,
                "bidirectional": module.bidirectional
            }
        # Snake activation (custom) - check for alpha parameter and in_features
        elif hasattr(module, 'alpha') and isinstance(getattr(module, 'alpha', None), torch.nn.Parameter):
            # Snake: alpha is a learnable parameter, extract in_features from its shape
            alpha = module.alpha
            if alpha.dim() >= 2:
                cfg = {"in_features": alpha.shape[1]}
            elif alpha.dim() == 1:
                cfg = {"in_features": alpha.shape[0]}
        # Custom Transpose
        elif type(module).__name__ == "Transpose":
             cfg = {
                "dim0": getattr(module, "dim0", 1),
                "dim1": getattr(module, "dim1", 2),
            }
        
        # GPT2 from HuggingFace transformers
        if hasattr(module, 'config') and hasattr(module.config, 'n_embd'):
            cfg['vocab_size'] = module.config.vocab_size
            cfg['n_positions'] = module.config.n_positions  # context_length
            cfg['n_embd'] = module.config.n_embd  # emb_dim
            cfg['n_head'] = module.config.n_head  # n_heads
            cfg['n_layer'] = module.config.n_layer  # n_layers
            cfg['resid_pdrop'] = module.config.resid_pdrop  # drop_rate
        
        return cfg

    def _tensor_to_cpu(self, t: Any) -> Optional[torch.Tensor]:
        if isinstance(t, torch.Tensor):
            if t.device.type == 'meta':
                # Return placeholder or None
                return None
            t = t.detach().clone().contiguous().cpu()
            if not self.keep_dtype:
                t = t.float()
            return t
        return None

    def _extract_shapes(self, data: Any) -> List[List[int]]:
        shapes = []
        if isinstance(data, torch.Tensor):
            shapes.append(list(data.shape))
        elif isinstance(data, (tuple, list)):
            for item in data:
                shapes.extend(self._extract_shapes(item))
        return shapes

    def hook_factory(self, name: str):
        def hook(m, inp, out):
            call_idx = self.call_counts[name]
            self.call_counts[name] += 1
            
            trace_key = f"{name}.{call_idx}" if call_idx > 0 else name
            
            # Record Inputs
            if isinstance(inp, tuple):
                for i, x in enumerate(inp):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.in.{i}"] = cpu_x
            
            # Record Outputs
            if isinstance(out, torch.Tensor):
                self.records[f"{trace_key}.out.0"] = self._tensor_to_cpu(out)
            elif isinstance(out, (tuple, list)):
                for i, x in enumerate(out):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.out.{i}"] = cpu_x

            # Record Metadata
            self.manifest[trace_key] = LayerMeta(
                name=name,
                module_type=type(m).__name__,
                input_shapes=self._extract_shapes(inp),
                output_shapes=self._extract_shapes(out),
                parameters=[n for n, _ in m.named_parameters(recurse=False)],
                is_leaf=len(list(m.children())) == 0,
                config=self._get_module_config(m)
            )
        return hook

    def record(self, model: nn.Module, *args, trace_fx: bool = False, **kwargs):
        model.eval()
        
        # Record model inputs
        for i, arg in enumerate(args):
            cpu_arg = self._tensor_to_cpu(arg)
            if cpu_arg is not None:
                self.records[f"model_input.{i}"] = cpu_arg

        hooks = []
        for name, module in model.named_modules():
            if name == "": continue 
            hooks.append(module.register_forward_hook(self.hook_factory(name)))
        
        try:
            with torch.no_grad():
                output = model(*args, **kwargs)
        finally:
            for h in hooks:
                h.remove()
        
        if trace_fx:
            self.trace_fx(model, *args, **kwargs)
            
        return output

    def trace_fx(self, model: nn.Module, *example_inputs):
        """Use torch.fx to capture the computation graph."""
        import torch.fx as fx
        
        # We use a symbolic trace to capture the graph
        traced = fx.symbolic_trace(model)
        
        def serialize_arg(arg):
            if isinstance(arg, (list, tuple)):
                return [serialize_arg(a) for a in arg]
            if hasattr(arg, "name"):
                return arg.name
            return str(arg)

        graph_nodes = []
        for node in traced.graph.nodes:
            # We want to map these nodes to the modules or functions
            node_info = {
                "name": node.name,
                "op": node.op,
                "target": str(node.target),
                "args": [serialize_arg(arg) for arg in node.args],
            }
            
            if node.op == "call_module":
                try:
                    submod = traced.get_submodule(node.target)
                    node_info["module_type"] = type(submod).__name__
                except:
                    pass
            
            graph_nodes.append(node_info)
            
        self._fx_graph = {
            "graph_nodes": graph_nodes,
            "graph_code": traced.code
        }
        return self._fx_graph

    def save(self, project_name: str, use_fx: bool = False, hints: Optional[Dict[str, int]] = None):
        tensor_path = os.path.join(self.output_dir, f"{project_name}_trace.safetensors")
        # Filter out None values (meta placeholders)
        real_records = {k: v for k, v in self.records.items() if v is not None}
        if real_records:
            save_file(real_records, tensor_path)
        else:
            print("‚ö†Ô∏è No real tensors recorded (Meta-only mode)")
        
        manifest_path = os.path.join(self.output_dir, f"{project_name}_manifest.json")
        manifest_data = {k: asdict(v) for k, v in self.manifest.items()}
        
        if use_fx and hasattr(self, '_fx_graph'):
            manifest_data["_graph_nodes"] = self._fx_graph["graph_nodes"]
            manifest_data["_graph_code"] = self._fx_graph["graph_code"]

        if hints:
            manifest_data["_symbolic_hints"] = hints

        with open(manifest_path, "w") as f:
            json.dump(manifest_data, f, indent=4)
        print(f"‚úÖ Trace and Manifest saved for {project_name}")
</file>

<file path="README.md">
# PyCandle

**Automated PyTorch ‚Üí Candle porting with layer-wise parity verification.**

PyCandle captures activation traces from PyTorch models to generate verified Rust Candle code. Acting as "Chrome DevTools" for neural networks, it provides full transparency into the internal state of complex models, ensuring seamless parity across Python and Rust.

## Quick Start

### 1. Record a PyTorch model

```python
# your_model_script.py
import sys
sys.path.insert(0, "path/to/pycandle/py")

import torch
from spy import GoldenRecorder

# Your PyTorch model
model = MyModel()
model.eval()

# Create dummy input
x = torch.randn(1, 128)

# Record
recorder = GoldenRecorder(output_dir="traces")
recorder.record(model, x)
recorder.save("my_model")
```

### 2. Generate Candle code

```bash
cargo run -p pycandle -- codegen \
    --manifest traces/my_model_manifest.json \
    --out generated_my_model.rs \
    --model MyModel
```

### 3. Use generated code with parity checking

```rust
use pycandle_core::{PyChecker, py_check};

// Load golden records for verification
let checker = PyChecker::load("my_model", "traces/", &device)?;

// Use the generated model
let model = MyModel::load(vb, Some(checker))?;
let output = model.forward(&input)?;  // py_check! runs at each layer
```

## CLI Commands

### `pycandle record`
Run a Python script that uses `GoldenRecorder`.

```bash
pycandle record --script model_script.py --name my_model --out traces/
```

### `pycandle codegen`
Generate Candle Rust code from a manifest.

```bash
pycandle codegen --manifest traces/manifest.json --out generated.rs --model ModelName
```

**Flags:**
- `--analyze-only` - Show analysis without generating code
- `--json` - Output as JSON (works with all commands)

**Analysis mode example:**
```bash
# Human-readable analysis
pycandle codegen --manifest m.json --out NUL --analyze-only

# JSON output for scripting
pycandle codegen --manifest m.json --out NUL --analyze-only --json
```

### `pycandle todos`
Extract and manage TODO markers in generated code.

```bash
# List all TODOs with suggestions
pycandle todos --file generated_model.rs

# JSON output
pycandle todos --file generated_model.rs --json

# Check mode (exit code 1 if TODOs remain)
pycandle todos --file generated_model.rs --check
```

**Agent workflow example:**
```bash
# 1. Generate code
pycandle codegen --manifest m.json --out model.rs --model MyModel

# 2. Check for TODOs
if ! pycandle todos --file model.rs --check; then
    # 3. Get gaps as JSON and implement
    pycandle todos --file model.rs --json | jq '.by_type'
fi
```

## Python Spy API

```python
from spy import GoldenRecorder

recorder = GoldenRecorder(output_dir="traces")
recorder.record(model, *inputs, **kwargs)  # Runs forward pass with hooks
recorder.save("project_name")  # Saves .safetensors + _manifest.json
```

**Output files:**
- `{name}_trace.safetensors` - Activation tensors for each layer
- `{name}_manifest.json` - Module metadata (types, shapes, configs)

### Advanced: Resolving Symbolic Ambiguity (Hints)

If your model has multiple dimensions with the same size (e.g., `hidden_dim=1024` and `context_length=1024`), the symbolic propagator might pick the wrong one. You can resolve this by passing `hints` to `recorder.save()`:

```python
recorder.save("my_model", hints={
    "vocab_size": 50257,
    "hidden_dim": 768,
    "context_length": 1024
})
```

The codegen will prioritize these hints when generating the `Config` struct and mapping layer dimensions.

## Rust Verification API

```rust
use pycandle_core::{PyChecker, py_check};

// Load checker
let checker = PyChecker::load("model_name", "traces/", &device)?;

// Verify a tensor against golden record
let result = checker.verify("layer_name", &tensor)?;
println!("MSE: {}", result.mse);

// Or use the macro (embedded in generated code)
py_check!(checker, "layer_name", &tensor);
```

## Generated Code Structure

```rust
pub struct Config {
    pub vocab_size: usize,
    pub hidden_dim: usize,
}

pub struct MyModel {
    pub linear1: Linear,
    pub linear2: Linear,
    pub checker: Option<PyChecker>,
}

impl MyModel {
    pub fn load(cfg: Config, vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let linear1 = candle_nn::linear(cfg.hidden_dim, 256, vb.pp("linear1"))?;
        let linear2 = candle_nn::linear(256, cfg.vocab_size, vb.pp("linear2"))?;
        Ok(Self { linear1, linear2, checker })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let mut x = xs.clone();
        x = self.linear1.forward(&x)?;
        py_check!(self.checker, "linear1", &x);
        x = self.linear2.forward(&x)?;
        py_check!(self.checker, "linear2", &x);
        Ok(x)
    }
}
```

## Supported Module Types

| PyTorch | Candle | Status |
|---------|--------|--------|
| `nn.Linear` | `candle_nn::linear` | ‚úÖ Auto (with smart transpose) |
| `nn.Conv1d` | `candle_nn::conv1d` | ‚úÖ Auto |
| `nn.Embedding` | `candle_nn::embedding` | ‚úÖ Auto |
| `nn.LayerNorm` | `candle_nn::layer_norm` | ‚úÖ Auto |
| `nn.BatchNorm1d` | `BatchNorm1d` | ‚úÖ Auto |
| `nn.BatchNorm2d` | `BatchNorm2d` | ‚úÖ Auto |
| `nn.LSTM` | `LSTM` | ‚úÖ Auto |
| `nn.ReLU/GELU/Sigmoid/Tanh` | Activations | ‚úÖ Auto |
| `nn.ELU/LeakyReLU` | Parameterized activations | ‚úÖ Auto |
| `Snake` (BigVGAN) | `Snake` | ‚úÖ Auto |
| Custom modules | - | ‚ö†Ô∏è TODO marker |

## Workspace Structure

```
pycandle/
‚îú‚îÄ‚îÄ Cargo.toml              # Workspace root
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ pycandle/           # CLI binary
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/main.rs
‚îÇ   ‚îú‚îÄ‚îÄ pycandle-core/      # Library (PyChecker, layers, codegen)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lib.rs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ checker.rs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ layers.rs
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ codegen/
‚îÇ   ‚îî‚îÄ‚îÄ pycandle-audio/     # Audio ops (STFT, padding)
‚îÇ       ‚îî‚îÄ‚îÄ src/lib.rs
‚îî‚îÄ‚îÄ py/
    ‚îî‚îÄ‚îÄ spy.py              # GoldenRecorder
```

**Using as a library:**
```toml
[dependencies]
pycandle-core = { git = "https://github.com/user/pycandle" }
# Optional audio support:
pycandle-audio = { git = "https://github.com/user/pycandle" }
```

---

## Roadmap

PyCandle has evolved into a high-fidelity transpilation framework. The following items track the transition from v0.1 to a production-grade v1.0.

### üîÑ DAG Resolver (torch.fx Tracing)
**Status:** Complete ‚úÖ

Handle non-sequential models with skip connections and branches:
- Use `torch.fx` to trace computation graphs automatically.
- Generate named variables based on FX graph nodes (e.g., `let x_conv1 = ...`).
- Automatic residual detection and mapping of functional ops (`add`, `cat`, `mul`).
- Mapping of `operator.getitem` to Candle `.i()` for complex slicing logic.

### üìê Symbolic Shape Propagation
**Status:** Complete ‚úÖ

Generate `Config` structs instead of hardcoded dimensions to decouple model logic from specific input sizes:
- Automatic detection of `vocab_size`, `hidden_dim`, and `context_length`.
- Config-driven initialization in the generated Rust `load` functions.

### üìä Visual Drift Analysis (Mechanistic Diagnostics)
**Status:** Complete ‚úÖ

Enhanced diagnostics for numerical drift using real-time verification data:
- **D3.js Coverage Report:** Standalone HTML report with MSE/Cosine Similarity charts.
- **Divergence Detection:** Automatic identification of the "Point of Failure" where math starts to drift.
- **TUI Dashboard:** Real-time terminal dashboard with 8x8 error heatmaps for instant visual feedback.

### üéµ Audio-Specific Ops (pycandle-audio)
**Status:** Complete ‚úÖ

Bit-perfect PyTorch parity for audio preprocessing and specialized layers:
- **MelSpectrogram:** Full parity with `torchaudio` (including Slaney-scale area normalization).
- **STFT/iSTFT:** High-precision CPU-based transforms using `realfft`.
- **Specialized Layers:** Native support for `Snake` (BigVGAN), `CausalConv1d`, and `Mish`.

### üî¨ Interactive Debugger (Lock-Step)
**Status:** Complete ‚úÖ

Automated post-mortem tools for failed parity checks:
- **Snippet Generation:** Automatically saves `.safetensors` containing the failing Rust tensor and the Golden reference.
- **Python Debug Scripts:** Generates a ready-to-run `.py` script that loads the failure snippet into Matplotlib for visual histogram/heatmap comparison.

### üì¶ Surgical Weight Management
**Status:** Complete ‚úÖ

Tools to handle the "Integration Gap" between PyTorch checkpoints and Rust structs:
- **Checkpoint Mapper:** Regex-based renaming engine to map PyTorch keys to Rust fields.
- **Meta-Extractor:** CLI tool to surgically extract only the weights used in a manifest, significantly reducing checkpoint size.

### üß™ Automated Test Generation
**Status:** Complete ‚úÖ

Eliminate manual test writing by generating the full Rust test harness:
- **Auto-Test CLI:** `pycandle gen-test` to generate `tests/parity.rs` automatically.
- **Data-Driven Harness:** The test will automatically load the input/output tensors from the recorded trace and run the verification loop.

### üìê Symbolic Ambiguity Hints
**Status:** Complete ‚úÖ

Refining the symbolic propagator for complex models:
- **Hint System:** Allow users to provide a `hints.json` to resolve ambiguous dimensions (e.g., when `hidden_dim` and `context_length` are both 1024).
- **Manual Overrides:** Use the Python `recorder.save(..., hints={"vocab_size": 50000})` to guide the codegen when heuristics fail.

### üß© Multi-Input & Complex Slicing
**Status:** Complete ‚úÖ

- **Multi-Input Support:** Models with multiple placeholders (e.g., TTS models) correctly generate `forward(&self, xs0: &Tensor, xs1: &Tensor, ...)` signatures.
- **Robust Slicing:** Improved handling of complex `torch.fx` slicing nodes, mapping `operator.getitem` to Candle's `.i()` with support for multi-dimensional ranges (e.g., `x.i((.., ..-1))?`).

### üìâ Quantization Parity (GGUF/AWQ)
**Status:** Complete ‚úÖ

Extending the verification engine to quantized models:
- **Quantization Drift Tracking:** Measure MSE drift introduced specifically by `Q4_0`, `Q8_0`, or `AWQ` compared to the `f32` Golden Record.
- **Parity-Aware Quantization:** Identify which specific layers are most sensitive to quantization to help guide mixed-precision strategies.

---

### Summary of the "Powerful" PyCandle Vision:
1.  **Python Spy:** Captures Graph (FX) + Config + Activations + Weights.
2.  **Transpiler:** Converts FX Graph to idiomatic Rust DAG (with residuals).
3.  **Verifiable Crate:** Generated code with `py_check!` macros that "lights up" green as you implement layers.
4.  **Diagnostics:** A visual report and Python debug scripts showing exactly where the "Math Leak" is happening.
---
### The "Universal Transpiler" Roadmap (ONNX Edition)

#### üåê Universal ONNX Transpilation
**Status: Researching üîç**
- **Direct Graph Parsing:** Generate Candle Rust code directly from `.onnx` files without requiring Python source code.
- **Operator Mapping:** Translation layer to map standard ONNX Ops (Opset 15-21) to optimized Candle kernels.
- **Multi-Framework Support:** Enable porting from JAX and TensorFlow to Candle via the ONNX intermediate representation.
- **Reference Parity:** Support `onnxruntime` as a verification backend for `PyChecker`.

---

### Technical Challenges to Watch For:
1.  **The "Opset" Nightmare:** ONNX has many versions (Opsets). You‚Äôll need to focus on the most common ones (Opset 17+).
2.  **Naming Conventions:** ONNX often renames layers to generic IDs (like `node_1`, `node_2`). This makes the generated Rust code harder to read than your current `torch.fx` approach, which keeps the original PyTorch names.
3.  **Complex Ops:** Some ONNX ops (like `EinsteinSum` or complex `Loop` nodes) are very hard to map to Candle.

### Should you do it?
**Yes, but as an alternative input.** 
Keep the `torch.fx` path as the "Primary" because it produces the most readable, idiomatic Rust code. Use ONNX as the "Emergency/Universal" path for when the original source code isn't available.

**PyCandle would then be:**
*   **Input:** PyTorch Code OR ONNX File.
*   **Process:** Trace + Analyze + Codegen.
*   **Output:** Verified, Production-Grade Rust.

---

## License

MIT
</file>

</files>
