This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/parity-check.yml
.gitignore
.repomixignore
Cargo.toml
crates/pycandle-audio/Cargo.toml
crates/pycandle-audio/src/lib.rs
crates/pycandle-core/Cargo.toml
crates/pycandle-core/failures/debug_node_linear.py
crates/pycandle-core/src/checker.rs
crates/pycandle-core/src/codegen/gpt2.rs
crates/pycandle-core/src/codegen/mod.rs
crates/pycandle-core/src/gpt2.rs
crates/pycandle-core/src/layers.rs
crates/pycandle-core/src/lib.rs
crates/pycandle-core/src/samplers.rs
crates/pycandle-core/src/weights.rs
crates/pycandle-core/tests/dummy_test.rs
crates/pycandle-core/tests/quantization_drift.rs
crates/pycandle-core/verification_results.jsonl
crates/pycandle/Cargo.toml
crates/pycandle/src/dashboard.rs
crates/pycandle/src/init.rs
crates/pycandle/src/main.rs
crates/pycandle/src/report.rs
crates/pycandle/src/test_gen.rs
crates/pycandle/src/todos.rs
docs/meta_trace_guide.md
docs/PORTING_SOP.md
py/.python-version
py/dag_model.py
py/hello.py
py/onnx_to_fx.py
py/pycandle_spy.egg-info/dependency_links.txt
py/pycandle_spy.egg-info/PKG-INFO
py/pycandle_spy.egg-info/requires.txt
py/pycandle_spy.egg-info/SOURCES.txt
py/pycandle_spy.egg-info/top_level.txt
py/pyproject.toml
py/spy.py
py/weight_extractor.py
README.md
recorder.py
specs/melspectrogram_parity.md
tests/parity.rs
tests/quantization_drift.rs
tests/simple_onnx_parity.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="py/pycandle_spy.egg-info/dependency_links.txt">

</file>

<file path="py/pycandle_spy.egg-info/PKG-INFO">
Metadata-Version: 2.4
Name: pycandle-spy
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: packaging
Requires-Dist: safetensors
Requires-Dist: torch>=2.0.0
Requires-Dist: onnx
Requires-Dist: onnx2torch
Requires-Dist: onnxscript
</file>

<file path="py/pycandle_spy.egg-info/requires.txt">
numpy
packaging
safetensors
torch>=2.0.0
onnx
onnx2torch
onnxscript
</file>

<file path="py/pycandle_spy.egg-info/SOURCES.txt">
README.md
pyproject.toml
pycandle_spy.egg-info/PKG-INFO
pycandle_spy.egg-info/SOURCES.txt
pycandle_spy.egg-info/dependency_links.txt
pycandle_spy.egg-info/requires.txt
pycandle_spy.egg-info/top_level.txt
</file>

<file path="py/pycandle_spy.egg-info/top_level.txt">
pycandle_trace
</file>

<file path=".github/workflows/parity-check.yml">
name: Parity Check

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  CARGO_TERM_COLOR: always

jobs:
  parity-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    # 1. Setup Rust
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    # 2. Setup Python & UV
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        python-version: '3.12'

    - name: Install Python dependencies
      run: uv pip install torch pycandle-core

    # 3. Compile PyCandle CLI (or use pre-built if available)
    # For now, we assume we build it from source in the workspace
    - name: Build PyCandle CLI
      run: cargo build --release --bin pycandle

    # 4. Generate Traces (Simulating 'pycandle record')
    # Assuming 'recorder.py' is in the root as per SOP
    - name: Generate Traces
      run: uv run recorder.py
      env:
        # Ensure we can find local pycandle if not installed
        PYTHONPATH: ${{ github.workspace }}/py

    # 5. Generate Code (Optional if committed, but good to verify codegen runs)
    # If the user committed generated code, this step might be skipped or used to check for drift
    - name: Run Codegen (Verify)
      run: |
        ./target/release/pycandle codegen \
          --manifest traces/debug_run_manifest.json \
          --out generated_model.rs \
          --model MyModel \
          --analyze-only

    # 6. Run Parity Tests
    - name: Run Parity Tests
      run: cargo test --test parity
</file>

<file path=".repomixignore">
chatterbox-repo/*
</file>

<file path="crates/pycandle-core/failures/debug_node_linear.py">
import torch
from safetensors.torch import load_file
import matplotlib.pyplot as plt
import numpy as np

def analyze():
    print(f"üîç Analyzing Failure: {'node_linear'}")
    tensors = load_file("node_linear.safetensors")
    rust = tensors["rust_actual"]
    gold = tensors["py_golden"]

    diff = (rust - gold).abs()
    print(f"  Max Diff: {diff.max().item():.6f}")
    print(f"  MSE:      {(diff ** 2).mean().item():.8f}")

    # Plot
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.title("Rust Tensor Histogram")
    plt.hist(rust.flatten().float().numpy(), bins=50, alpha=0.7, label='Rust')
    plt.hist(gold.flatten().float().numpy(), bins=50, alpha=0.7, label='Gold')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.title("Difference Heatmap (First Slice)")
    if diff.ndim > 1:
        plt.imshow(diff.flatten(0, -2)[0].float().numpy(), cmap='hot', aspect='auto')
    else:
        plt.plot(diff.float().numpy())
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    analyze()
</file>

<file path="crates/pycandle-core/src/samplers.rs">
use candle_core::{Result, Tensor};

pub trait LogitsProcessor {
    fn apply(&self, logits: &Tensor) -> Result<Tensor>;
}

/// Repetition Penalty
/// Reference: https://arxiv.org/pdf/1909.05858.pdf
pub struct RepetitionPenalty {
    pub penalty: f64,
    pub context: Vec<u32>,
}

impl RepetitionPenalty {
    pub fn new(penalty: f64, context: Vec<u32>) -> Self {
        Self { penalty, context }
    }
}

impl LogitsProcessor for RepetitionPenalty {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.penalty <= 1.0 || self.context.is_empty() {
            return Ok(logits.clone());
        }

        let mut logits_vec = logits.squeeze(0)?.to_vec1::<f32>()?;

        for &token in &self.context {
            if (token as usize) < logits_vec.len() {
                let logit = logits_vec[token as usize];
                if logit < 0.0 {
                    logits_vec[token as usize] = logit * self.penalty as f32;
                } else {
                    logits_vec[token as usize] = logit / self.penalty as f32;
                }
            }
        }

        Tensor::from_vec(logits_vec, (1, logits.dim(1)?), logits.device())
    }
}

/// Temperature scaling
pub struct Temperature {
    pub temperature: f64,
}

impl LogitsProcessor for Temperature {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.temperature <= 0.0 {
            // Greedy sampling handled by caller usually, but here we can just return
            return Ok(logits.clone());
        }
        logits / self.temperature
    }
}

/// Top-P (Nucleus) Sampling
pub struct TopP {
    pub p: f64,
}

impl LogitsProcessor for TopP {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.p >= 1.0 {
            return Ok(logits.clone());
        }

        let probs = candle_nn::ops::softmax(logits, 1)?;
        let probs_vec = probs.squeeze(0)?.to_vec1::<f32>()?;

        // Sort descending, keep indices
        let mut indices: Vec<usize> = (0..probs_vec.len()).collect();
        indices.sort_by(|&a, &b| probs_vec[b].partial_cmp(&probs_vec[a]).unwrap());

        let mut cum_sum = 0.0;
        let mut cutoff_index = indices.len() - 1;

        for (i, &idx) in indices.iter().enumerate() {
            cum_sum += probs_vec[idx];
            if cum_sum > self.p as f32 {
                cutoff_index = i;
                break;
            }
        }

        // Everything after cutoff gets -inf
        let mut new_logits = logits.squeeze(0)?.to_vec1::<f32>()?;
        let neg_inf = f32::NEG_INFINITY;

        // Mask out the tail
        for &idx in &indices[cutoff_index + 1..] {
            new_logits[idx] = neg_inf;
        }

        Tensor::from_vec(new_logits, (1, logits.dim(1)?), logits.device())
    }
}

/// Min-P Sampling (Alternative to Top-P)
/// Reference: https://github.com/huggingface/transformers/issues/27670
pub struct MinP {
    pub p: f64,
}

impl LogitsProcessor for MinP {
    fn apply(&self, logits: &Tensor) -> Result<Tensor> {
        if self.p <= 0.0 {
            return Ok(logits.clone());
        }

        let probs = candle_nn::ops::softmax(logits, 1)?;
        let max_prob = probs.max_keepdim(1)?.squeeze(0)?.to_vec1::<f32>()?[0];
        let scaled_min = max_prob * self.p as f32;

        let probs_vec = probs.squeeze(0)?.to_vec1::<f32>()?;
        let mut new_logits = logits.squeeze(0)?.to_vec1::<f32>()?;
        let neg_inf = f32::NEG_INFINITY;

        for (i, &prob) in probs_vec.iter().enumerate() {
            if prob < scaled_min {
                new_logits[i] = neg_inf;
            }
        }

        Tensor::from_vec(new_logits, (1, logits.dim(1)?), logits.device())
    }
}
</file>

<file path="crates/pycandle-core/src/weights.rs">
use anyhow::{Context, Result};
use regex::Regex;
use std::collections::{HashMap, HashSet};

/// A renaming engine for tensor keys using Regex patterns.
pub struct WeightMapper {
    mappings: Vec<(Regex, String)>,
}

impl WeightMapper {
    /// Create a mapper from a JSON string containing pattern -> replacement mappings.
    pub fn from_json(json: &str) -> Result<Self> {
        let raw: HashMap<String, String> = serde_json::from_str(json)?;
        let mut mappings = Vec::new();
        // Sort keys to ensure deterministic order (important for overlapping regexes)
        let mut keys: Vec<_> = raw.keys().collect();
        keys.sort();

        for pattern in keys {
            let replacement = raw.get(pattern).unwrap();
            let re = Regex::new(pattern)
                .with_context(|| format!("Invalid regex pattern: {}", pattern))?;
            mappings.push((re, replacement.clone()));
        }
        Ok(Self { mappings })
    }

    /// Map a key using all registered patterns.
    pub fn map_key(&self, key: &str) -> String {
        let mut current = key.to_string();
        for (re, replacement) in &self.mappings {
            current = re.replace_all(&current, replacement.as_str()).to_string();
        }
        current
    }
}

/// Identifies which weights are actually used in a model based on its manifest.
pub struct WeightExtractor {
    active_params: HashSet<String>,
}

impl WeightExtractor {
    /// Create an extractor from a manifest JSON.
    pub fn from_manifest(manifest_json: &str) -> Result<Self> {
        let manifest: HashMap<String, serde_json::Value> = serde_json::from_str(manifest_json)?;
        let mut active_params = HashSet::new();

        for (module_name, meta) in manifest {
            if module_name.starts_with('_') {
                continue;
            }

            if let Some(params) = meta.get("parameters").and_then(|p| p.as_array()) {
                for p in params {
                    if let Some(p_name) = p.as_str() {
                        // PyTorch module parameters are accessed as module_name.weight, etc.
                        active_params.insert(format!("{}.{}", module_name, p_name));
                    }
                }
            }
        }

        Ok(Self { active_params })
    }

    /// Check if a given tensor key is used in the manifest.
    pub fn should_keep(&self, key: &str) -> bool {
        self.active_params.contains(key)
    }

    /// Return the set of all active parameter keys.
    pub fn active_keys(&self) -> &HashSet<String> {
        &self.active_params
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_mapper_basic() {
        let json = r#"{"encoder\\.layers\\.(\\d+)\\.": "h.$1."}"#;
        let mapper = WeightMapper::from_json(json).unwrap();
        assert_eq!(mapper.map_key("encoder.layers.0.weight"), "h.0.weight");
        assert_eq!(mapper.map_key("encoder.layers.15.bias"), "h.15.bias");
    }

    #[test]
    fn test_extractor_basic() {
        let manifest = r#"{
            "encoder.layers.0": {
                "parameters": ["weight", "bias"]
            },
            "decoder": {
                "parameters": ["weight"]
            },
            "_internal": { "parameters": ["unused"] }
        }"#;
        let extractor = WeightExtractor::from_manifest(manifest).unwrap();
        assert!(extractor.should_keep("encoder.layers.0.weight"));
        assert!(extractor.should_keep("encoder.layers.0.bias"));
        assert!(extractor.should_keep("decoder.weight"));
        assert!(!extractor.should_keep("encoder.layers.0.other"));
        assert!(!extractor.should_keep("_internal.unused"));
    }
}
</file>

<file path="crates/pycandle-core/tests/dummy_test.rs">
#[cfg(test)]
mod tests {
    use candle_core::{Device, Tensor};
    use pycandle_core::{ComparisonResult, PyChecker};

    // Mock PyChecker just to access log_result (which we can't easily do without full setup)
    // Actually, we can just use PyChecker if we mock the files, but easier to just implement a similar test
    // that writes to the same file to verify the dashboard picks it up.

    // Wait, let's try to use the actual PyChecker if possible.
    // We need a dummy manifest and safetensors.
    // Instead, I'll just write a test that prints the expected output format for the dashboard to parse.

    #[test]
    fn test_layer_1_pass() {
        println!("test_layer_1_pass ... ok");
    }

    #[test]
    fn test_layer_2_fail() {
        // Dashboard should pick this up as failure
        assert!(false, "Simulated failure");
    }

    #[test]
    fn test_layer_3_pass() {
        std::thread::sleep(std::time::Duration::from_millis(500));
        println!("test_layer_3_pass ... ok");
    }
}
</file>

<file path="crates/pycandle-core/tests/quantization_drift.rs">
use candle_core::{Device, Tensor};
use pycandle_core::{LayerMeta, PyChecker, VerificationMode};
use std::collections::HashMap;

#[test]
fn test_quantization_drift() -> anyhow::Result<()> {
    let device = Device::Cpu;

    // 1. Create a dummy manifest
    let manifest_json = r#"{
        "layer_perfect": {
            "name": "layer_perfect", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_small": {
            "name": "layer_drift_small", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_large": {
            "name": "layer_drift_large", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        }
    }"#;

    // Write dummy files for PyChecker::load
    // Use a unique subdir to avoid conflicts
    let trace_dir = "test_trace_drift_core";
    std::fs::create_dir_all(trace_dir)?;
    std::fs::write(
        format!("{}/drift_run_manifest.json", trace_dir),
        manifest_json,
    )?;

    // Create dummy golden tensors
    let t_perfect = Tensor::new(&[1.0f32, 2.0, 3.0], &device)?;
    let t_drift_small = Tensor::new(&[10.0f32, 20.0, 30.0], &device)?;
    let t_drift_large = Tensor::new(&[100.0f32, 200.0, 300.0], &device)?;

    let tensors_map: HashMap<String, Tensor> = HashMap::from([
        ("layer_perfect.out.0".to_string(), t_perfect.clone()),
        ("layer_drift_small.out.0".to_string(), t_drift_small.clone()),
        ("layer_drift_large.out.0".to_string(), t_drift_large.clone()),
    ]);
    candle_core::safetensors::save(
        &tensors_map,
        format!("{}/drift_run_trace.safetensors", trace_dir),
    )?;

    // 2. Load Checker in DriftTracking mode
    // Note: PyChecker::load takes project_name and base_path
    let checker = PyChecker::load("drift_run", trace_dir, &device)?
        .with_mode(VerificationMode::DriftTracking);

    println!("Checking in mode: {:?}", checker.mode);

    // 3. Verify Layers

    // A) Perfect Match
    let res1 = checker.verify("layer_perfect", &t_perfect)?;
    assert!(res1.mse < 1e-6);

    // B) Small Drift (MSE = 1e-6, barely passing strict if atol=1e-4, but let's make it small enough)
    // Let's add noise: 1e-3. 1e-3^2 = 1e-6.
    let noise_small = Tensor::new(&[0.001f32, 0.001, 0.001], &device)?;
    let t_small_noisy = (&t_drift_small + noise_small)?;
    let res2 = checker.verify("layer_drift_small", &t_small_noisy)?;
    println!("Layer Small Drift MSE: {}", res2.mse);

    // C) Large Drift (MSE ~ 0.01) - Should fail in Strict, but pass here
    // Noise 0.1. 0.1^2 = 0.01. > 1e-4.
    let noise_large = Tensor::new(&[0.1f32, 0.1, 0.1], &device)?;
    let t_large_noisy = (&t_drift_large + noise_large)?;

    // This should NOT panic or return Err, unlike Strict mode
    let res3 = checker.verify("layer_drift_large", &t_large_noisy)?;
    println!("Layer Large Drift MSE: {}", res3.mse);

    assert!(
        res3.mse > checker.atol,
        "Expected large drift to exceed atol"
    );

    // 4. Print Report (Manual verify on console output)
    checker.print_drift_report();

    // Cleanup
    std::fs::remove_dir_all(trace_dir)?;

    Ok(())
}
</file>

<file path="crates/pycandle-core/verification_results.jsonl">
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
{"name":"layer_perfect","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"layer_drift_small","mse":9.995374e-7,"max_diff":0.0010004044,"cosine_sim":1.0,"passed":true,"heatmap":null}
{"name":"layer_drift_large","mse":0.010000712,"max_diff":0.1000061,"cosine_sim":1.0,"passed":false,"heatmap":null}
{"name":"node_linear","mse":0.44827208,"max_diff":1.1747879,"cosine_sim":0.0,"passed":false,"heatmap":null}
{"name":"node_linear","mse":0.44827208,"max_diff":1.1747879,"cosine_sim":0.0,"passed":false,"heatmap":null}
{"name":"node_linear","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"node_relu","mse":0.0,"max_diff":0.0,"cosine_sim":1.0000001,"passed":true,"heatmap":null}
{"name":"node_relu","mse":0.0,"max_diff":0.0,"cosine_sim":1.0000001,"passed":true,"heatmap":null}
{"name":"node_linear","mse":0.0,"max_diff":0.0,"cosine_sim":0.99999994,"passed":true,"heatmap":null}
{"name":"node_relu","mse":0.0,"max_diff":0.0,"cosine_sim":1.0000001,"passed":true,"heatmap":null}
{"name":"node_relu","mse":0.0,"max_diff":0.0,"cosine_sim":1.0000001,"passed":true,"heatmap":null}
</file>

<file path="crates/pycandle/src/dashboard.rs">
use anyhow::Result;
use crossterm::{
    event::{self, Event, KeyCode, KeyEventKind},
    execute,
    terminal::{EnterAlternateScreen, LeaveAlternateScreen, disable_raw_mode, enable_raw_mode},
};
use pycandle_core::ComparisonResult;
use ratatui::{
    prelude::*,
    widgets::{
        Block, Borders, List, ListItem, Paragraph,
        canvas::{Canvas, Rectangle},
    },
};
use std::collections::HashMap;
use std::io::{BufRead, BufReader};
use std::process::{Command, Stdio};
use std::sync::mpsc;
use std::thread;
use std::time::Duration;

pub enum DashboardEvent {
    Input(event::KeyEvent),
    TestLine(String),
    Tick,
}

struct App {
    test_results: Vec<TestResult>,
    parity_results: HashMap<String, ComparisonResult>,
    output_log: Vec<String>,
    running: bool,
    scroll: usize,
    passed: usize,
    failed: usize,
    total: usize,
}

struct TestResult {
    name: String,
    status: TestStatus,
    details: String,
}

enum TestStatus {
    Running,
    Passed,
    Failed,
}

impl App {
    fn new() -> Self {
        Self {
            test_results: Vec::new(),
            parity_results: HashMap::new(),
            output_log: Vec::new(),
            running: true,
            scroll: 0,
            passed: 0,
            failed: 0,
            total: 0,
        }
    }

    fn on_tick(&mut self) {
        // Periodically reload parity results
        self.load_parity_results();
    }

    fn load_parity_results(&mut self) {
        if let Ok(file) = std::fs::File::open("verification_results.jsonl") {
            let reader = BufReader::new(file);
            for line in reader.lines() {
                if let Ok(l) = line {
                    if let Ok(res) = serde_json::from_str::<ComparisonResult>(&l) {
                        // Normalize name to help matching?
                        // For now just store by layer name
                        self.parity_results.insert(res.name.clone(), res);
                    }
                }
            }
        }
    }

    fn on_log(&mut self, line: String) {
        // Parse cargo test output
        if line.contains("test result: ok") {
            // Summary line, maybe parse stats?
        } else if line.contains("test ") && line.contains(" ... ok") {
            let name = line.replace("test ", "").replace(" ... ok", "");
            self.test_results.push(TestResult {
                name,
                status: TestStatus::Passed,
                details: "".to_string(),
            });
            self.passed += 1;
            self.total += 1;
        } else if line.contains("test ") && line.contains(" ... FAILED") {
            let name = line.replace("test ", "").replace(" ... FAILED", "");
            self.test_results.push(TestResult {
                name,
                status: TestStatus::Failed,
                details: "".to_string(),
            });
            self.failed += 1;
            self.total += 1;
        }

        // Keep a rolling log
        self.output_log.push(line);
        if self.output_log.len() > 100 {
            self.output_log.remove(0);
        }
    }
}

pub fn run_dashboard(_args: &[String]) -> Result<()> {
    // Setup terminal
    enable_raw_mode()?;
    let mut stdout = std::io::stdout();
    execute!(stdout, EnterAlternateScreen)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    // Channels
    let (tx, rx) = mpsc::channel();
    let tick_rate = Duration::from_millis(100);

    // Spawn Input Thread
    let tx_input = tx.clone();
    thread::spawn(move || {
        loop {
            if event::poll(Duration::from_millis(50)).unwrap() {
                if let Event::Key(key) = event::read().unwrap() {
                    if key.kind == KeyEventKind::Press {
                        if tx_input.send(DashboardEvent::Input(key)).is_err() {
                            return;
                        }
                    }
                }
            }
        }
    });

    // Spawn Tick Thread
    let tx_tick = tx.clone();
    thread::spawn(move || {
        loop {
            thread::sleep(tick_rate);
            if tx_tick.send(DashboardEvent::Tick).is_err() {
                return;
            }
        }
    });

    // Spawn Test Runner Thread
    let tx_log = tx.clone();
    thread::spawn(move || {
        let mut cmd = Command::new("cargo")
            .arg("test")
            .arg("--")
            .arg("--nocapture") // Important to see output in real-time
            .stdout(Stdio::piped())
            .stderr(Stdio::piped()) // Capture stderr too
            .spawn()
            .expect("Failed to spawn cargo test");

        if let Some(stdout) = cmd.stdout.take() {
            let reader = BufReader::new(stdout);
            for line in reader.lines() {
                if let Ok(l) = line {
                    let _ = tx_log.send(DashboardEvent::TestLine(l));
                }
            }
        }
    });

    // App Loop
    let mut app = App::new();
    let mut should_quit = false;

    while !should_quit {
        terminal.draw(|f| ui(f, &mut app))?;

        match rx.recv()? {
            DashboardEvent::Input(key) => match key.code {
                KeyCode::Char('q') => should_quit = true,
                KeyCode::Down => {
                    if app.scroll < app.test_results.len().saturating_sub(1) {
                        app.scroll += 1;
                    }
                }
                KeyCode::Up => {
                    if app.scroll > 0 {
                        app.scroll -= 1;
                    }
                }
                _ => {}
            },
            DashboardEvent::TestLine(line) => app.on_log(line),
            DashboardEvent::Tick => app.on_tick(),
        }
    }

    // Restore terminal
    disable_raw_mode()?;
    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
    terminal.show_cursor()?;

    Ok(())
}

fn ui(f: &mut Frame, app: &mut App) {
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(3), // Header
            Constraint::Min(0),    // Main (List + Details)
            Constraint::Length(8), // Log
            Constraint::Length(1), // Footer
        ])
        .split(f.area());

    // Header
    let title = Paragraph::new(format!(
        "Parity Dashboard | Total: {} | Passed: {} | Failed: {}",
        app.total, app.passed, app.failed
    ))
    .block(
        Block::default()
            .borders(Borders::ALL)
            .title("PyCandle Status"),
    )
    .style(Style::default().fg(Color::Cyan));
    f.render_widget(title, chunks[0]);

    // Main Content Split
    let main_chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(40), Constraint::Percentage(60)])
        .split(chunks[1]);

    // --- List Widget (Left) ---
    let items: Vec<ListItem> = app
        .test_results
        .iter()
        .enumerate()
        .skip(app.scroll.saturating_sub(10)) // Simple pseudo-windowing
        .take(50)
        .map(|(i, res)| {
            let style = match res.status {
                TestStatus::Passed => Style::default().fg(Color::Green),
                TestStatus::Failed => Style::default().fg(Color::Red),
                TestStatus::Running => Style::default().fg(Color::Yellow),
            };
            let icon = match res.status {
                TestStatus::Passed => "‚úî",
                TestStatus::Failed => "‚úñ",
                TestStatus::Running => "‚è≥",
            };

            let prefix = if i == app.scroll { "> " } else { "  " };
            ListItem::new(format!("{}{}{}", prefix, icon, res.name)).style(if i == app.scroll {
                style.add_modifier(Modifier::BOLD | Modifier::REVERSED)
            } else {
                style
            })
        })
        .collect();

    let list_title = if app.test_results.is_empty() {
        "Running Tests..."
    } else {
        "Tests"
    };
    let list = List::new(items).block(Block::default().borders(Borders::ALL).title(list_title));
    f.render_widget(list, main_chunks[0]);

    // --- Details Widget (Right) ---
    // Try to find matching parity result for the selected test
    let selected_test = app.test_results.get(app.scroll);

    let details_block = Block::default().borders(Borders::ALL).title("Details");
    let inner = details_block.inner(main_chunks[1]);
    f.render_widget(details_block, main_chunks[1]);

    let details_layout = Layout::default()
        .direction(Direction::Vertical)
        .constraints([Constraint::Length(5), Constraint::Min(0)])
        .split(inner);

    if let Some(test) = selected_test {
        // Find result by fuzzy matching name
        let matched_result = app.parity_results.values().find(|r| {
            let parts: Vec<&str> = r.name.split('.').collect();
            if parts.len() >= 2 {
                test.name.contains(parts[parts.len() - 1])
                    && test.name.contains(parts[parts.len() - 2])
            } else {
                test.name.contains(&r.name)
            }
        });

        if let Some(res) = matched_result {
            // Stats
            let status_line = Line::from(vec![
                Span::raw(format!("Layer: {}\nStatus: ", res.name)),
                if res.passed {
                    Span::styled("PASS", Style::default().fg(Color::Green))
                } else {
                    Span::styled("FAIL", Style::default().fg(Color::Red))
                },
                Span::raw(format!(
                    "\nMSE: {:.2e}\nCosSim: {:.6}",
                    res.mse, res.cosine_sim
                )),
            ]);

            f.render_widget(Paragraph::new(status_line), details_layout[0]);

            // Heatmap
            if let Some(heatmap) = &res.heatmap {
                // Render 8x8 heatmap
                if heatmap.len() == 64 {
                    // Normalize for coloring
                    let max_val = heatmap.iter().cloned().fold(0.0, f32::max);

                    let canvas = Canvas::default()
                        .block(
                            Block::default()
                                .borders(Borders::ALL)
                                .title("Error Heatmap (8x8)"),
                        )
                        .x_bounds([0.0, 8.0])
                        .y_bounds([0.0, 8.0])
                        .paint(move |ctx| {
                            for r in 0..8 {
                                for c in 0..8 {
                                    let idx = r * 8 + c;
                                    let val = heatmap[idx];
                                    let intensity = if max_val > 0.0 { val / max_val } else { 0.0 };

                                    // Color ramp: Black -> Blue -> Red
                                    let color = if intensity < 0.2 {
                                        Color::DarkGray
                                    } else if intensity < 0.5 {
                                        Color::Blue
                                    } else if intensity < 0.8 {
                                        Color::Magenta
                                    } else {
                                        Color::Red
                                    };

                                    // Draw 1x1 rectangle
                                    // r=0 is first row (top), so map to y=7
                                    ctx.draw(&Rectangle {
                                        x: c as f64,
                                        y: 7.0 - r as f64,
                                        width: 1.0,
                                        height: 1.0,
                                        color,
                                    });
                                }
                            }
                        });
                    f.render_widget(canvas, details_layout[1]);
                }
            } else if !res.passed {
                f.render_widget(Paragraph::new("No heatmap available."), details_layout[1]);
            }
        } else {
            f.render_widget(
                Paragraph::new("No trace data found for this test."),
                details_layout[0],
            );
        }
    }

    // Footer
    let footer = Paragraph::new("Press 'q' to quit | '‚Üë/‚Üì' to scroll")
        .style(Style::default().fg(Color::Gray));
    f.render_widget(footer, chunks[3]);

    // Log output (Raw)
    let log_items: Vec<ListItem> = app
        .output_log
        .iter()
        .rev()
        .take(8)
        .rev()
        .map(|l| ListItem::new(l.clone()).style(Style::default().fg(Color::DarkGray)))
        .collect();

    let log_box =
        List::new(log_items).block(Block::default().borders(Borders::ALL).title("Raw Output"));
    f.render_widget(log_box, chunks[2]);
}
</file>

<file path="crates/pycandle/src/init.rs">
use anyhow::{Context, Result};
use colored::Colorize;
use std::fs;
use std::path::Path;

const RECORDER_TEMPLATE: &str = r#"import torch
import sys
import os

# Try to import pycandle spy. 
# in a real setup this would be installed, but for now we look in relative paths common in this workspace.
try:
    from pycandle.spy import GoldenRecorder
except ImportError:
    # Add potential fallback paths
    possible_paths = ["py", "../py", "../../py"]
    for p in possible_paths:
        if os.path.exists(os.path.join(p, "spy.py")):
            sys.path.append(p)
            break
    from spy import GoldenRecorder

# TODO: Import your model class
# from my_project.model import MyModel

def main():
    print("üöÄ Initializing model configuration...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device}")

    # TODO: Instantiate your model
    # model = MyModel().to(device)
    # model.eval()

    # TODO: Create dummy input matching your model's requirement
    # dummy_input = torch.randn(1, 3, 224, 224).to(device)

    print("üé• Starting recording...")
    recorder = GoldenRecorder(output_dir="traces")
    
    # TODO: Run the forward pass with the recorder
    # recorder.record(model, dummy_input)
    
    # Save the trace
    name = "debug_run"
    recorder.save(name)
    print(f"‚úÖ Recording saved to traces/{name}")

if __name__ == "__main__":
    main()
"#;

const TEST_TEMPLATE: &str = r#"#[cfg(test)]
mod tests {
    use anyhow::Result;
    // use super::*; // Import your model

    #[test]
    fn test_parity() -> Result<()> {
        // TODO: Load the checker
        // let device = candle_core::Device::Cpu;
        // let checker = pycandle_core::PyChecker::load("debug_run", "traces/", &device)?;
        
        // TODO: Load your model and run forward pass
        // let model = MyModel::load(..., Some(checker))?;
        // let output = model.forward(&input)?;

        Ok(())
    }
}
"#;

pub fn run_init(name: Option<String>) -> Result<()> {
    println!("{}", "‚ö° PyCandle Project Initialization".bold().green());

    // 1. Create recorder.py
    let recorder_path = Path::new("recorder.py");
    if recorder_path.exists() {
        println!("   {} recorder.py already exists, skipping.", "‚ö†Ô∏è".yellow());
    } else {
        fs::write(recorder_path, RECORDER_TEMPLATE).context("Failed to write recorder.py")?;
        println!("   {} Created recorder.py", "‚úÖ".green());
    }

    // 2. Create tests directory and parity test if requested
    let tests_dir = Path::new("tests");
    if !tests_dir.exists() {
        fs::create_dir(tests_dir).ok(); // failure ok, maybe we are not in root
    }

    let test_path = tests_dir.join("parity.rs");
    if tests_dir.exists() && !test_path.exists() {
        fs::write(&test_path, TEST_TEMPLATE).context("Failed to write tests/parity.rs")?;
        println!("   {} Created tests/parity.rs", "‚úÖ".green());
    } else {
        println!(
            "   {} tests/parity.rs already exists (or tests/ missing), skipping.",
            "‚ö†Ô∏è".yellow()
        );
    }

    println!("\nNext steps:");
    println!("1. Edit {} to import your model.", "recorder.py".bold());
    println!(
        "2. Run {} to capture traces.",
        "pycandle record --script recorder.py --name debug_run".bold()
    );
    println!(
        "3. Run {} to generate Rust code.",
        "pycandle codegen ...".bold()
    );

    Ok(())
}
</file>

<file path="crates/pycandle/src/todos.rs">
// TODO extraction and management for generated code
use regex::Regex;
use serde::Serialize;
use std::collections::HashMap;

#[derive(Serialize, Debug)]
pub struct TodoItem {
    pub line: usize,
    pub module_type: String,
    pub field_name: String,
    pub context: String,
    pub suggestion: String,
}

#[derive(Serialize, Debug)]
pub struct TodoReport {
    pub file: String,
    pub total: usize,
    pub by_type: HashMap<String, usize>,
    pub todos: Vec<TodoItem>,
}

/// Extract TODO markers from generated Rust code
pub fn extract_todos(content: &str) -> Vec<TodoItem> {
    let mut todos = Vec::new();

    // Match: let field_name = todo!("Implement initialization for ModuleType");
    let re = Regex::new(r#"let\s+(\w+)\s*=\s*todo!\("Implement initialization for (\w+)"\)"#)
        .expect("Invalid regex");

    for (line_num, line) in content.lines().enumerate() {
        if let Some(caps) = re.captures(line) {
            let field_name = caps.get(1).unwrap().as_str().to_string();
            let module_type = caps.get(2).unwrap().as_str().to_string();

            todos.push(TodoItem {
                line: line_num + 1,
                field_name,
                module_type: module_type.clone(),
                context: line.trim().to_string(),
                suggestion: get_implementation_hint(&module_type),
            });
        }
    }

    todos
}

/// Get implementation hint for a module type
pub fn get_implementation_hint(module_type: &str) -> String {
    match module_type {
        "LSTM" => r#"LSTM::load(vb.pp("field"), input_size, hidden_size, num_layers)?"#.to_string(),
        "BatchNorm1d" => r#"BatchNorm1d::load(vb.pp("field"), num_features)?"#.to_string(),
        "BatchNorm2d" => r#"BatchNorm2d::load(vb.pp("field"), num_features)?"#.to_string(),
        "Snake" => r#"Snake::load(vb.pp("field"), in_features)?"#.to_string(),
        "ReLU" => "ReLU".to_string(),
        "Sigmoid" => "Sigmoid".to_string(),
        "ELU" => "ELU::new(1.0)".to_string(),
        "Dropout" => "// Dropout is a no-op at inference time".to_string(),
        "Conv1D" => r#"// HuggingFace Conv1D: implement as Linear with transpose"#.to_string(),
        "NewGELUActivation" => "GELU // or x.gelu_erf()".to_string(),
        _ => format!("// Manual implementation needed for {}", module_type),
    }
}

/// Generate a report from extracted TODOs
pub fn generate_report(file_path: &str, todos: Vec<TodoItem>) -> TodoReport {
    let mut by_type: HashMap<String, usize> = HashMap::new();
    for todo in &todos {
        *by_type.entry(todo.module_type.clone()).or_default() += 1;
    }

    TodoReport {
        file: file_path.to_string(),
        total: todos.len(),
        by_type,
        todos,
    }
}
</file>

<file path="docs/meta_trace_guide.md">
# PyCandle Meta-Trace Strategy

The **Meta-Trace Strategy** is the recommended SOP for porting large deep learning models (e.g., Transformers, LLMs) to Candle, especially on machines with limited RAM (e.g., 16GB RAM / 4GB available).

## The Problem: Paging File Errors
Loading weight files directly into PyTorch (e.g., 1.8GB + intermediate allocations) often exhausts system memory, leading to `OSError: The paging file is too small`.

## The Solution: Record Architecture on `meta`
Instead of loading weights into memory, we initialize the model on the PyTorch `meta` device. This records the **shapes**, **parameters**, and **computation graph** without allocating any actual weight buffers.

### Step 1: Initialize on Meta Device
Update your recording script to use the `meta` device.

```python
import torch
from py.spy import GoldenRecorder
from my_model import MyLargeModel

device = "meta"
with torch.device(device):
    model = MyLargeModel()
model.eval()

# Dummy inputs must also be on meta
dummy_input = torch.randn(1, 128, model.dim, device=device)

recorder = GoldenRecorder(output_dir="pycandle_trace")
recorder.record(model, dummy_input)
recorder.save("my_large_model_meta")
```

### Step 2: Generate Rust Code
Run the `pycandle codegen` as usual. The manifest contains everything needed to define the Rust structs.

```bash
cargo run -- codegen --manifest pycandle_trace/my_large_model_meta_manifest.json --out generated_model.rs
```

### Step 3: Stream-Extract Weights
Since you didn't record weights, your `trace.safetensors` will be empty or missing tensors. Use a specialized extraction script to map weights from the original checkpoint directly to the manifest-compatible keys.

```python
from safetensors.torch import load_file, save_file
# Load only what manifests says we need
# Rename keys to match manifest
# Save to a new, minimal weights file
```

## Benefits
- **Zero Memory Consumption**: Tracing happens in milliseconds with no RAM pressure.
- **Permanent Solution**: Works for models of any size (7B, 70B+) as long as the architecture fits in memory.
- **Power User DX**: Separates architecture capture from weight management.
</file>

<file path="docs/PORTING_SOP.md">
# PyCandle Porting SOP

This document outlines the Standard Operating Procedure (SOP) for porting PyTorch models to Rust/Candle using the PyCandle framework.

## 1. Project Setup
Initialize a new porting project. This generates the necessary boilerplate for recording and parity testing.

```bash
# In your workspace root
pycandle init --name <RunName>
```

**What this does:**
- Creates `recorder.py`: A standard PyTorch recording script tailored for your model.
- Creates `tests/main.rs`: A standard integration test harness for parity verification.

## 2. Model Wrapping (Python)
Edit the generated `recorder.py` to import and instantiate your specific PyTorch model.

```python
# recorder.py
from my_model_source import MyModel  # <--- Import your model

model = MyModel(...)
input_tensor = torch.randn(...)
```

## 3. Recording Traces
Run the recorder to capture the Golden Trace (activations, weights, and config).

```bash
# Uses uv under the hood to manage dependencies
pycandle record --script recorder.py --name <RunName>
```

*Output:* `traces/<RunName>/` containing `.safetensors` and `_manifest.json`.

## 4. Codegen & Analysis
Analyze the trace to see what layers are supported and generate the Rust code.

```bash
# Analyze first
pycandle codegen --manifest traces/<RunName>/_manifest.json --analyze-only

# Generate Rust code
pycandle codegen --manifest traces/<RunName>/_manifest.json --out crates/my-model/src/model.rs --model MyModel
```

## 5. Parity Verification
Run the generated test harness to verify that your Rust implementation matches the PyTorch golden record bit-for-bit.

```bash
cargo test
```

## 6. Iterative Refinement
If `cargo test` fails or parity is low:
1.  Use `pycandle dashboard` to visualize the error.
2.  Fix the Rust implementation.
3.  Re-run `cargo test`.
</file>

<file path="py/.python-version">
3.11
</file>

<file path="py/dag_model.py">
import torch
import torch.nn as nn
from spy import GoldenRecorder

class ComplexModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv1d(16, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(32 * 10, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        # x is (B, 32, 10)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

if __name__ == "__main__":
    model = ComplexModel()
    recorder = GoldenRecorder()
    
    # Dummy input (B, C, T)
    x = torch.randn(1, 16, 10)
    
    recorder.record(model, x)
    recorder.trace_fx(model, x)
    recorder.save("complex_model", use_fx=True)
</file>

<file path="py/hello.py">
def main():
    print("Hello from pycandle-spy!")


if __name__ == "__main__":
    main()
</file>

<file path="py/onnx_to_fx.py">
# ONNX to FX Converter
import os
import sys
import argparse
import torch
import onnx
from onnx2torch import convert
from spy import GoldenRecorder

def main():
    parser = argparse.ArgumentParser(description="Convert ONNX model to PyCandle manifest via torch.fx")
    parser.add_argument("--onnx", type=str, required=True, help="Path to ONNX model")
    parser.add_argument("--name", type=str, required=True, help="Project name")
    parser.add_argument("--out", type=str, default="pycandle_trace", help="Output directory")
    
    args = parser.parse_args()
    
    print(f"üì¶ Loading ONNX model from {args.onnx}...")
    onnx_model = onnx.load(args.onnx)
    
    print("üîÑ Converting ONNX to PyTorch...")
    torch_model = convert(onnx_model)
    torch_model.eval()
    
    # Try to infer input shape from ONNX model
    input_shape = [1, 3, 224, 224] # Default fallback
    if len(onnx_model.graph.input) > 0:
        dims = onnx_model.graph.input[0].type.tensor_type.shape.dim
        input_shape = [d.dim_value if d.dim_value > 0 else 1 for d in dims]
    
    print(f"üß™ Using input shape: {input_shape}")
    dummy_input = torch.randn(*input_shape)
    
    recorder = GoldenRecorder(output_dir=args.out)
    
    print("üé¨ Recording trace and FX graph...")
    # Record will also perform FX tracing if requested
    recorder.record(torch_model, dummy_input, trace_fx=True)
    
    print(f"üíæ Saving manifest and weights to {args.out}...")
    recorder.save(args.name, use_fx=True)
    
    # Save weights
    from safetensors.torch import save_file
    weight_path = os.path.join(args.out, f"{args.name}_weights.safetensors")
    # convert state_dict to use keys matching the manifest (which uses node names)
    # Actually, recorder.manifest has the mapping.
    # But for a simple ONNX converted model, state_dict keys often match node names if convert is clean.
    # Let's just save the state_dict for now.
    save_file(torch_model.state_dict(), weight_path)
    
    print(f"‚úÖ Conversion complete! You can now run:")
    print(f"   pycandle codegen --manifest {args.out}/{args.name}_manifest.json --out generated_{args.name}.rs")

if __name__ == "__main__":
    main()
</file>

<file path="py/weight_extractor.py">
import os
import sys
import json
import torch
from safetensors.torch import save_file
from typing import Dict, Any, Set

def extract_weights(checkpoint_path: str, manifest_path: str, output_path: str, mapper_path: str = None):
    """
    Extracts only the weights specified in the manifest from a PyTorch checkpoint.
    Supports .bin/.pt (Pickle) and .safetensors.
    """
    if not os.path.exists(manifest_path):
        print(f"‚ùå Manifest not found: {manifest_path}")
        sys.exit(1)
        
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    # Identify active parameters
    active_params: Set[str] = set()
    for name, meta in manifest.items():
        if name.startswith('_'): 
            continue
        if 'parameters' in meta:
            for p in meta['parameters']:
                active_params.add(f"{name}.{p}")
    
    print(f"üîç Manifest contains {len(active_params)} active parameters.")
    
    # Load weights selectively
    weights: Dict[str, torch.Tensor] = {}
    
    if checkpoint_path.endswith('.safetensors'):
        from safetensors import safe_open
        with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                if key in active_params:
                    weights[key] = f.get_tensor(key)
    else:
        # Pickle-based (.bin, .pt, .pth)
        # We use weights_only=True for security and map_location='cpu' for memory
        try:
            state_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            for k, v in state_dict.items():
                if k in active_params:
                    weights[k] = v
        except Exception as e:
            print(f"‚ùå Failed to load checkpoint: {e}")
            # Fallback for older torch versions or complex pickles
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            for k, v in state_dict.items():
                if k in active_params:
                    weights[k] = v

    if not weights:
        print("‚ö†Ô∏è No matching weights found in checkpoint!")
        # Print a few examples from the checkpoint if possible
        return

    # Optional renaming
    if mapper_path and os.path.exists(mapper_path):
        import re
        with open(mapper_path, 'r') as f:
            mappings = json.load(f)
        
        print(f"üîÑ Applying {len(mappings)} renaming patterns...")
        renamed_weights = {}
        # Sort mappings by length of pattern (desc) or just alphabetical for consistency?
        # Typically we want deterministic order.
        sorted_patterns = sorted(mappings.items())
        
        for k, v in weights.items():
            new_k = k
            for pattern, replacement in sorted_patterns:
                new_k = re.sub(pattern, replacement, new_k)
            renamed_weights[new_k] = v
        weights = renamed_weights

    # Ensure directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    
    save_file(weights, output_path)
    print(f"‚úÖ Surgically extracted {len(weights)} weights to {output_path}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Surgically extract model weights.")
    parser.add_argument("--checkpoint", required=True, help="Path to PyTorch checkpoint (.bin, .pt, .safetensors)")
    parser.add_argument("--manifest", required=True, help="Path to manifest.json")
    parser.add_argument("--out", required=True, help="Output .safetensors path")
    parser.add_argument("--map", help="Optional JSON mapping file for renaming")
    args = parser.parse_args()
    
    extract_weights(args.checkpoint, args.manifest, args.out, args.map)
</file>

<file path="recorder.py">
import torch
import sys
import os

# Try to import pycandle spy. 
# in a real setup this would be installed, but for now we look in relative paths common in this workspace.
try:
    from pycandle.spy import GoldenRecorder
except ImportError:
    # Add potential fallback paths
    possible_paths = ["py", "../py", "../../py"]
    for p in possible_paths:
        if os.path.exists(os.path.join(p, "spy.py")):
            sys.path.append(p)
            break
    from spy import GoldenRecorder

# TODO: Import your model class
# from my_project.model import MyModel

def main():
    print("üöÄ Initializing model configuration...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device}")

    # TODO: Instantiate your model
    # model = MyModel().to(device)
    # model.eval()

    # TODO: Create dummy input matching your model's requirement
    # dummy_input = torch.randn(1, 3, 224, 224).to(device)

    print("üé• Starting recording...")
    recorder = GoldenRecorder(output_dir="traces")
    
    # TODO: Run the forward pass with the recorder
    # recorder.record(model, dummy_input)
    
    # Save the trace
    name = "debug_run"
    recorder.save(name)
    print(f"‚úÖ Recording saved to traces/{name}")

if __name__ == "__main__":
    main()
</file>

<file path="specs/melspectrogram_parity.md">
# Walkthrough - MelSpectrogram Implementation

I have implemented and verified the `MelSpectrogram` operation in `pycandle-audio` with full parity against `torchaudio`.

## Changes

### `crates/pycandle-audio`

#### [lib.rs](file:///d:/pycandle/crates/pycandle-audio/src/lib.rs)
-   Implemented `MelSpectrogramConfig` with support for `HTK` and `Slaney` mel scales.
-   Implemented `MelNorm::Slaney` for area normalization.
-   Implemented `hz_to_mel` and `mel_to_hz` for both scales.
-   Implemented `get_mel_banks` to generate the Mel filterbank matrix.
-   Implemented `mel_spectrogram` (STFT + Power + Mel Filterbank).

## Verification Results

Verified against `torchaudio` using a Python script.

### Mel Filterbank Parity
-   **Max Diff:** `6.89e-08`
-   **Status:** ‚úÖ EXACT MATCH
-   this confirms the implementation of Slaney scale and area normalization is correct.

### End-to-End MelSpectrogram
-   **Max Diff:** `0.018` (approx `1e-5` relative error)
-   **Status:** ‚úÖ PASS (High Precision)
-   Differences are due to floating point variations between Rust's `realfft` and PyTorch's FFT backend (MKL/FFTW). The core logic is verified correct.

### STFT & Power
-   **STFT Max Diff:** `0.010`
-   **Power Max Diff:** `1.0` (approx `4e-6` relative error)
</file>

<file path="tests/quantization_drift.rs">
use candle_core::{Device, Tensor};
use pycandle_core::checker::LayerMeta; // Needed for mock manifest
use pycandle_core::{PyChecker, VerificationMode};
use std::collections::HashMap;

#[test]
fn test_quantization_drift() -> anyhow::Result<()> {
    let device = Device::Cpu;

    // 1. Create a dummy manifest
    let manifest_json = r#"{
        "layer_perfect": {
            "name": "layer_perfect", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_small": {
            "name": "layer_drift_small", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        },
        "layer_drift_large": {
            "name": "layer_drift_large", "module_type": "Linear", 
            "input_shapes": [], "output_shapes": [], "parameters": [], "is_leaf": true, "config": {} 
        }
    }"#;

    // Write dummy files for PyChecker::load
    std::fs::create_dir_all("test_trace_drift")?;
    std::fs::write("test_trace_drift/drift_run_manifest.json", manifest_json)?;

    // Create dummy golden tensors
    let t_perfect = Tensor::new(&[1.0f32, 2.0, 3.0], &device)?;
    let t_drift_small = Tensor::new(&[10.0f32, 20.0, 30.0], &device)?;
    let t_drift_large = Tensor::new(&[100.0f32, 200.0, 300.0], &device)?;

    let tensors_map: HashMap<String, Tensor> = HashMap::from([
        ("layer_perfect.out.0".to_string(), t_perfect.clone()),
        ("layer_drift_small.out.0".to_string(), t_drift_small.clone()),
        ("layer_drift_large.out.0".to_string(), t_drift_large.clone()),
    ]);
    candle_core::safetensors::save(&tensors_map, "test_trace_drift/drift_run_trace.safetensors")?;

    // 2. Load Checker in DriftTracking mode
    let checker = PyChecker::load("drift_run", "test_trace_drift", &device)?
        .with_mode(VerificationMode::DriftTracking);

    println!("Checking in mode: {:?}", checker.mode);

    // 3. Verify Layers

    // A) Perfect Match
    let res1 = checker.verify("layer_perfect", &t_perfect)?;
    assert!(res1.mse < 1e-6);

    // B) Small Drift (MSE = 1e-6, barely passing strict if atol=1e-4, but let's make it small enough)
    // Let's add noise: 1e-3. 1e-3^2 = 1e-6.
    let noise_small = Tensor::new(&[0.001f32, 0.001, 0.001], &device)?;
    let t_small_noisy = (&t_drift_small + noise_small)?;
    let res2 = checker.verify("layer_drift_small", &t_small_noisy)?;
    println!("Layer Small Drift MSE: {}", res2.mse);

    // C) Large Drift (MSE ~ 0.01) - Should fail in Strict, but pass here
    // Noise 0.1. 0.1^2 = 0.01. > 1e-4.
    let noise_large = Tensor::new(&[0.1f32, 0.1, 0.1], &device)?;
    let t_large_noisy = (&t_drift_large + noise_large)?;

    // This should NOT panic or return Err, unlike Strict mode
    let res3 = checker.verify("layer_drift_large", &t_large_noisy)?;
    println!("Layer Large Drift MSE: {}", res3.mse);

    assert!(
        res3.mse > checker.atol,
        "Expected large drift to exceed atol"
    );

    // 4. Print Report (Manual verify on console output)
    checker.print_drift_report();

    // Cleanup
    std::fs::remove_dir_all("test_trace_drift")?;

    Ok(())
}
</file>

<file path="tests/simple_onnx_parity.rs">
#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::{Device, Tensor};
    use pycandle_core::{PyChecker, VerificationMode};
    use my_project::SimpleOnnxModel;
    use anyhow::Result;

    #[test]
    fn test_parity() -> Result<()> {
        // 1. Setup Device
        let device = Device::cuda_if_available(0).unwrap_or(Device::Cpu);
        println!("Running on device: {:?}", device);

        // 2. Load Checker and Golden Trace
        // Assumes the trace directory is in the current project root
        let checker = PyChecker::load("simple_onnx", "pycandle_trace", &device)?
            .with_mode(VerificationMode::Strict);
        println!("Loaded checker with trace: {}", checker.name);

        // 3. Load Model
        // We use zeros VB as a placeholder; in a real parity test, 
        // you might want to load weights using pycandle weight tools.
        let vb = candle_nn::VarBuilder::zeros(candle_core::DType::F32, &device);
        let model = SimpleOnnxModel::load(vb, Some(checker.clone()))?;

        // 4. Load Inputs from Trace
        let trace_path = format!("pycandle_trace/simple_onnx_trace.safetensors", "simple_onnx");
        let tensors = candle_core::safetensors::load(&trace_path, &device)?;
        
        let x0 = tensors.get("model_input.0").context("Missing model_input.0")?.clone();


        // 5. Run Forward Pass & Verify
        let output = model.forward(&x0)?;
        checker.verify("node_relu", &output)?;
        println!("‚úÖ Parity test passed for SimpleOnnxModel!");

        Ok(())
    }
}
</file>

<file path="crates/pycandle-audio/Cargo.toml">
[package]
name = "pycandle-audio"
description = "Audio ops for PyCandle: STFT, iSTFT, and padding with PyTorch parity"
version.workspace = true
edition.workspace = true
license.workspace = true

[dependencies]
candle-core.workspace = true
candle-nn.workspace = true
thiserror.workspace = true
realfft.workspace = true
num-complex.workspace = true
</file>

<file path="crates/pycandle-core/Cargo.toml">
[package]
name = "pycandle-core"
description = "Core library for PyCandle: PyChecker, layers, and codegen"
version.workspace = true
edition.workspace = true
license.workspace = true

[dependencies]
candle-core.workspace = true
candle-nn.workspace = true
safetensors.workspace = true
regex.workspace = true
serde.workspace = true
anyhow.workspace = true
serde_json.workspace = true
colored.workspace = true
thiserror.workspace = true
</file>

<file path="crates/pycandle-core/src/gpt2.rs">
use candle_core::{IndexOp, Result, Tensor};
use candle_nn::{Embedding, LayerNorm, Linear, Module, VarBuilder};

#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Config {
    pub vocab_size: usize,
    pub context_length: usize,
    pub emb_dim: usize,
    pub n_heads: usize,
    pub n_layers: usize,
    pub drop_rate: f32,
    pub qkv_bias: bool,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            vocab_size: 50257,
            context_length: 1024,
            emb_dim: 768,
            n_heads: 12,
            n_layers: 12,
            drop_rate: 0.1,
            qkv_bias: true, // Standard GPT2 has bias
        }
    }
}

// ============================================================================
// KV Cache
// ============================================================================

#[derive(Debug, Clone)]
pub struct KVCache {
    pub k: Tensor, // (B, n_head, T, head_dim)
    pub v: Tensor, // (B, n_head, T, head_dim)
}

impl KVCache {
    pub fn new() -> Self {
        // Placeholder, usually initialized during first forward pass
        // or we use Option<KVCache>
        unimplemented!("Use Option<KVCache> for now")
    }
}

// ============================================================================
// Layers
// ============================================================================

// Conv1D in HuggingFace is actually a Linear layer with (nx, nf) weight shape
// so transposing is needed if loading from standard HF checkpoints.
// However, standard Candle `linear` expects (out, in).
// HF Conv1D weight is (in, out).
fn conv1d(in_f: usize, out_f: usize, vb: VarBuilder) -> Result<Linear> {
    let weight = vb.get((in_f, out_f), "weight")?.t()?;
    let bias = vb.get((out_f,), "bias")?;
    Ok(Linear::new(weight, Some(bias)))
}
pub struct MultiHeadAttention {
    c_attn: Linear,
    c_proj: Linear,
    n_head: usize,
    head_dim: usize,
    scale: f64,
}

impl MultiHeadAttention {
    pub fn new(
        n_embd: usize,
        n_head: usize,
        _context_length: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let c_attn = conv1d(n_embd, 3 * n_embd, vb.pp("c_attn"))?;
        let c_proj = conv1d(n_embd, n_embd, vb.pp("c_proj"))?;
        let head_dim = n_embd / n_head;
        let scale = 1.0 / (head_dim as f64).sqrt();

        Ok(Self {
            c_attn,
            c_proj,
            n_head,
            head_dim,
            scale,
        })
    }

    pub fn forward(
        &self,
        x: &Tensor,
        mask: Option<&Tensor>,
        layer_cache: Option<&mut Option<KVCache>>,
    ) -> Result<Tensor> {
        let (b_sz, t, c) = x.dims3()?;
        let qkv = self.c_attn.forward(x)?;

        // (B, T, 3 * n_embd) -> (B, T, 3, n_head, head_dim)
        let qkv = qkv.reshape((b_sz, t, 3, self.n_head, self.head_dim))?;
        // (3, B, n_head, T, head_dim)
        let qkv = qkv.permute((2, 0, 3, 1, 4))?;

        let q = qkv.i(0)?;
        let k = qkv.i(1)?;
        let v = qkv.i(2)?;

        // KV Cache handling
        let (k, v) = if let Some(cache_opt) = layer_cache {
            if let Some(past_kv) = cache_opt.take() {
                let k = Tensor::cat(&[&past_kv.k, &k], 2)?;
                let v = Tensor::cat(&[&past_kv.v, &v], 2)?;
                *cache_opt = Some(KVCache {
                    k: k.clone(),
                    v: v.clone(),
                });
                (k, v)
            } else {
                *cache_opt = Some(KVCache {
                    k: k.clone(),
                    v: v.clone(),
                });
                (k, v)
            }
        } else {
            (k, v)
        };

        // Standard attention
        let t_total = k.dim(2)?;
        // q: (B, H, T_q, D) @ k.t: (B, H, D, T_k) -> (B, H, T_q, T_k)
        let att = (q.matmul(&k.t()?)? * self.scale)?;

        let att = if let Some(mask) = mask {
            let infinite =
                Tensor::new(f32::NEG_INFINITY, att.device())?.broadcast_as(att.shape())?;
            let mask = mask
                .narrow(2, 0, t)?
                .narrow(3, 0, t_total)?
                .eq(0.0)?
                .broadcast_as(att.shape())?;
            mask.where_cond(&infinite, &att)?
        } else {
            att // Attend to all past
        };

        let att = candle_nn::ops::softmax(&att, 3)?;

        // (B, H, T_q, T_k) @ (B, H, T_k, D) -> (B, H, T_q, D)
        let y = att.matmul(&v)?;
        // (B, T_q, H, D) -> (B, T_q, C)
        let y = y.permute((0, 2, 1, 3))?.reshape((b_sz, t, c))?;

        self.c_proj.forward(&y)
    }
}

pub struct FeedForward {
    c_fc: Linear,
    c_proj: Linear,
    act: candle_nn::Activation,
}

impl FeedForward {
    pub fn new(n_embd: usize, vb: VarBuilder) -> Result<Self> {
        let c_fc = conv1d(n_embd, 4 * n_embd, vb.pp("c_fc"))?;
        let c_proj = conv1d(4 * n_embd, n_embd, vb.pp("c_proj"))?;
        Ok(Self {
            c_fc,
            c_proj,
            act: candle_nn::Activation::Gelu,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.c_fc.forward(x)?;
        let x = self.act.forward(&x)?;
        self.c_proj.forward(&x)
    }
}

pub struct TransformerBlock {
    ln_1: LayerNorm,
    attn: MultiHeadAttention,
    ln_2: LayerNorm,
    mlp: FeedForward,
}

impl TransformerBlock {
    pub fn new(
        n_embd: usize,
        n_head: usize,
        context_length: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let ln_1 = candle_nn::layer_norm(n_embd, 1e-5, vb.pp("ln_1"))?;
        let attn = MultiHeadAttention::new(n_embd, n_head, context_length, vb.pp("attn"))?;
        let ln_2 = candle_nn::layer_norm(n_embd, 1e-5, vb.pp("ln_2"))?;
        let mlp = FeedForward::new(n_embd, vb.pp("mlp"))?;
        Ok(Self {
            ln_1,
            attn,
            ln_2,
            mlp,
        })
    }

    pub fn forward(
        &self,
        x: &Tensor,
        mask: Option<&Tensor>,
        layer_cache: Option<&mut Option<KVCache>>,
    ) -> Result<Tensor> {
        let residual = x;
        let x = self.ln_1.forward(x)?;
        let x = self.attn.forward(&x, mask, layer_cache)?;
        let x = (x + residual)?;

        let residual = &x;
        let x = self.ln_2.forward(&x)?;
        let x = self.mlp.forward(&x)?;
        let x = (x + residual)?;
        Ok(x)
    }
}
pub struct GPTModel {
    pub wte: Embedding,
    pub wpe: Embedding,
    pub h: Vec<TransformerBlock>,
    pub ln_f: LayerNorm,
    pub mask: Option<Tensor>,
}

impl GPTModel {
    pub fn new(cfg: Config, vb: VarBuilder) -> Result<Self> {
        let wte = candle_nn::embedding(cfg.vocab_size, cfg.emb_dim, vb.pp("wte"))?;
        let wpe = candle_nn::embedding(cfg.context_length, cfg.emb_dim, vb.pp("wpe"))?;

        let mut h = Vec::new();
        for i in 0..cfg.n_layers {
            h.push(TransformerBlock::new(
                cfg.emb_dim,
                cfg.n_heads,
                cfg.context_length,
                vb.pp(format!("h.{}", i)),
            )?);
        }

        let ln_f = candle_nn::layer_norm(cfg.emb_dim, 1e-5, vb.pp("ln_f"))?;

        // Causal mask buffer - compute once
        let mask: Vec<_> = (0..cfg.context_length)
            .flat_map(|i| {
                (0..cfg.context_length).map(move |j| if j <= i { 1.0f32 } else { 0.0f32 })
            })
            .collect();
        let mask = Tensor::from_vec(
            mask,
            (1, 1, cfg.context_length, cfg.context_length),
            vb.device(),
        )?;

        Ok(Self {
            wte,
            wpe,
            h,
            ln_f,
            mask: Some(mask),
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        self.forward_kv(x, None)
    }

    pub fn forward_kv(
        &self,
        x: &Tensor,
        mut kv_cache: Option<&mut Vec<Option<KVCache>>>,
    ) -> Result<Tensor> {
        let (_b, t) = x.dims2()?;

        let offset = if let Some(cache) = &kv_cache {
            if let Some(Some(prev)) = cache.first() {
                prev.k.dim(2)?
            } else {
                0
            }
        } else {
            0
        };

        let pos = Tensor::arange(offset as u32, (offset + t) as u32, x.device())?;

        let tok_emb = self.wte.forward(x)?;
        let pos_emb = self.wpe.forward(&pos)?;

        let mut x = (tok_emb + pos_emb)?;

        for (i, block) in self.h.iter().enumerate() {
            let layer_cache = if let Some(cache) = &mut kv_cache {
                if cache.len() <= i {
                    cache.resize_with(i + 1, || None);
                }
                cache.get_mut(i)
            } else {
                None
            };
            x = block.forward(&x, self.mask.as_ref(), layer_cache)?;
        }

        self.ln_f.forward(&x)
    }
}
</file>

<file path="crates/pycandle/src/report.rs">
// Report generation for PyCandle coverage analysis
use pycandle_core::LayerMeta;
use std::collections::HashMap;

/// Data structure holding analysis results
pub struct ReportData {
    pub supported: usize,
    pub unsupported: usize,
    pub gaps: HashMap<String, usize>,
    pub layers: HashMap<String, LayerMeta>,
}

/// Generates HTML coverage reports from manifest data
pub struct ReportGenerator {
    manifest: HashMap<String, LayerMeta>,
}

impl ReportGenerator {
    pub fn new(manifest: HashMap<String, LayerMeta>) -> Self {
        Self { manifest }
    }

    /// Analyze the manifest and categorize layers
    pub fn analyze(&self) -> ReportData {
        let mut supported = 0;
        let mut unsupported = 0;
        let mut gaps: HashMap<String, usize> = HashMap::new();

        for (_name, meta) in &self.manifest {
            if !meta.is_leaf {
                continue;
            }
            if self.is_supported(&meta.module_type) {
                supported += 1;
            } else {
                unsupported += 1;
                *gaps.entry(meta.module_type.clone()).or_default() += 1;
            }
        }

        ReportData {
            supported,
            unsupported,
            gaps,
            layers: self.manifest.clone(),
        }
    }

    /// Check if a module type is supported by PyCandle codegen
    fn is_supported(&self, module_type: &str) -> bool {
        matches!(
            module_type,
            "Linear"
                | "Conv1d"
                | "Conv2d"
                | "Embedding"
                | "LayerNorm"
                | "ReLU"
                | "GELU"
                | "Sigmoid"
                | "Tanh"
                | "ELU"
                | "LeakyReLU"
                | "Snake"
                | "BatchNorm1d"
                | "BatchNorm2d"
                | "LSTM"
        )
    }

    /// Generate a standalone HTML coverage report
    pub fn generate_html(&self, data: &ReportData) -> String {
        format!(
            r#"<!DOCTYPE html>
<html>
<head>
    <title>PyCandle Coverage Report</title>
    <style>
        :root {{
            --bg: #0f172a;
            --card-bg: #1e293b;
            --text: #e2e8f0;
            --text-muted: #94a3b8;
            --green: #22c55e;
            --red: #ef4444;
            --blue: #3b82f6;
            --border: #334155;
        }}
        * {{ box-sizing: border-box; margin: 0; padding: 0; }}
        body {{
            font-family: 'Inter', system-ui, -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            min-height: 100vh;
            padding: 40px 20px;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        h1 {{
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 8px;
            background: linear-gradient(135deg, var(--blue), var(--green));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }}
        .subtitle {{
            color: var(--text-muted);
            margin-bottom: 32px;
        }}
        .dashboard {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }}
        .card {{
            background: var(--card-bg);
            padding: 24px;
            border-radius: 12px;
            border: 1px solid var(--border);
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(0,0,0,0.3);
        }}
        .card h3 {{
            font-size: 0.875rem;
            font-weight: 500;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }}
        .card .value {{
            font-size: 3rem;
            font-weight: 700;
        }}
        .card.supported .value {{ color: var(--green); }}
        .card.unsupported .value {{ color: var(--red); }}
        .card.total .value {{ color: var(--blue); }}
        .progress-bar {{
            height: 8px;
            background: var(--border);
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }}
        .progress-fill {{
            height: 100%;
            background: linear-gradient(90deg, var(--green), var(--blue));
            border-radius: 4px;
            transition: width 0.5s ease;
        }}
        h2 {{
            font-size: 1.5rem;
            font-weight: 600;
            margin: 32px 0 16px 0;
            color: var(--text);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            background: var(--card-bg);
            border-radius: 12px;
            overflow: hidden;
            margin-bottom: 32px;
        }}
        th {{
            text-align: left;
            padding: 16px;
            background: rgba(0,0,0,0.2);
            font-weight: 600;
            color: var(--text-muted);
            text-transform: uppercase;
            font-size: 0.75rem;
            letter-spacing: 0.05em;
        }}
        td {{
            padding: 12px 16px;
            border-bottom: 1px solid var(--border);
        }}
        tr:last-child td {{
            border-bottom: none;
        }}
        tr:hover {{
            background: rgba(255,255,255,0.02);
        }}
        .status {{
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 4px 12px;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 500;
        }}
        .status.supported {{
            background: rgba(34, 197, 94, 0.15);
            color: var(--green);
        }}
        .status.unsupported {{
            background: rgba(239, 68, 68, 0.15);
            color: var(--red);
        }}
        .status::before {{
            content: '';
            width: 6px;
            height: 6px;
            border-radius: 50%;
            background: currentColor;
        }}
        .mono {{
            font-family: 'JetBrains Mono', 'Fira Code', monospace;
            font-size: 0.875rem;
        }}
        .shape {{
            color: var(--text-muted);
            font-size: 0.8rem;
        }}
        .count-badge {{
            display: inline-block;
            background: var(--border);
            padding: 2px 10px;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 600;
        }}
        
        /* Component grouping styles */
        .component-section {{
            background: var(--card-bg);
            border-radius: 12px;
            border: 1px solid var(--border);
            margin-bottom: 16px;
            overflow: hidden;
        }}
        .component-header {{
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 16px 20px;
            background: rgba(0,0,0,0.2);
            cursor: pointer;
            user-select: none;
            transition: background 0.2s;
        }}
        .component-header:hover {{
            background: rgba(0,0,0,0.3);
        }}
        .component-header h3 {{
            font-size: 1rem;
            font-weight: 600;
            color: var(--text);
            display: flex;
            align-items: center;
            gap: 12px;
        }}
        .component-header .chevron {{
            transition: transform 0.2s;
            color: var(--text-muted);
        }}
        .component-header.collapsed .chevron {{
            transform: rotate(-90deg);
        }}
        .component-stats {{
            display: flex;
            gap: 16px;
            font-size: 0.875rem;
        }}
        .component-stats .stat {{
            display: flex;
            align-items: center;
            gap: 6px;
        }}
        .component-stats .stat.ok {{ color: var(--green); }}
        .component-stats .stat.err {{ color: var(--red); }}
        .component-content {{
            max-height: 2000px;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }}
        .component-content.collapsed {{
            max-height: 0;
        }}
        .component-content table {{
            margin-bottom: 0;
            border-radius: 0;
        }}
        .layer-name {{
            padding-left: 24px;
            position: relative;
        }}
        .layer-name::before {{
            content: '‚îî';
            position: absolute;
            left: 8px;
            color: var(--border);
        }}
        
        /* Filters */
        .filters {{
            display: flex;
            gap: 12px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }}
        .filter-btn {{
            padding: 8px 16px;
            border: 1px solid var(--border);
            background: transparent;
            color: var(--text);
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s;
            font-size: 0.875rem;
        }}
        .filter-btn:hover {{
            background: var(--card-bg);
        }}
        .filter-btn.active {{
            background: var(--blue);
            border-color: var(--blue);
        }}
        .search-box {{
            flex: 1;
            min-width: 200px;
            padding: 8px 16px;
            border: 1px solid var(--border);
            background: var(--card-bg);
            color: var(--text);
            border-radius: 8px;
            font-size: 0.875rem;
        }}
        .search-box:focus {{
            outline: none;
            border-color: var(--blue);
        }}
        .hidden {{ display: none !important; }}
        
        /* Drift Analysis Chart Styles */
        .drift-section {{
            background: var(--card-bg);
            border-radius: 12px;
            border: 1px solid var(--border);
            padding: 24px;
            margin-bottom: 32px;
        }}
        .drift-section h2 {{
            margin-top: 0;
            margin-bottom: 16px;
        }}
        .drift-chart {{
            width: 100%;
            height: 300px;
            position: relative;
        }}
        .drift-chart svg {{
            width: 100%;
            height: 100%;
        }}
        .drift-legend {{
            display: flex;
            gap: 24px;
            margin-top: 16px;
            font-size: 0.875rem;
            color: var(--text-muted);
        }}
        .drift-legend-item {{
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        .drift-legend-color {{
            width: 16px;
            height: 16px;
            border-radius: 4px;
        }}
        .divergence-alert {{
            background: rgba(239, 68, 68, 0.15);
            border: 1px solid var(--red);
            border-radius: 8px;
            padding: 16px;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 12px;
        }}
        .divergence-alert .icon {{
            font-size: 1.5rem;
        }}
        .divergence-alert .message {{
            flex: 1;
        }}
        .divergence-alert .layer-name {{
            font-family: 'JetBrains Mono', monospace;
            color: var(--red);
            font-weight: 600;
        }}
        .drift-placeholder {{
            text-align: center;
            padding: 40px;
            color: var(--text-muted);
        }}
        .drift-placeholder .icon {{
            font-size: 3rem;
            margin-bottom: 16px;
            opacity: 0.5;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üïØÔ∏è PyCandle Coverage Report</h1>
        <p class="subtitle">Module coverage analysis for Candle code generation</p>
        
        <div class="dashboard">
            <div class="card total">
                <h3>Total Layers</h3>
                <div class="value">{total}</div>
            </div>
            <div class="card supported">
                <h3>Supported</h3>
                <div class="value">{supported}</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {coverage:.1}%"></div>
                </div>
            </div>
            <div class="card unsupported">
                <h3>Needs Implementation</h3>
                <div class="value">{unsupported}</div>
            </div>
        </div>
        
        <!-- Drift Analysis Section -->
        <div class="drift-section">
            <h2>üìä Numerical Drift Analysis</h2>
            <div id="divergenceAlert" class="divergence-alert" style="display: none;">
                <span class="icon">‚ö†Ô∏è</span>
                <div class="message">
                    <strong>Divergence Detected!</strong> Cosine similarity dropped below 0.99 at layer 
                    <span class="layer-name" id="divergenceLayer">-</span>
                </div>
            </div>
            <div class="drift-chart" id="driftChart">
                <div class="drift-placeholder">
                    <div class="icon">üìà</div>
                    <p>Run parity verification to see drift analysis</p>
                    <p style="font-size: 0.8rem; margin-top: 8px;">
                        Use <code>PyChecker::verify()</code> and pass results to generate this chart
                    </p>
                </div>
            </div>
            <div class="drift-legend">
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #3b82f6;"></div>
                    <span>MSE (log scale)</span>
                </div>
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #22c55e;"></div>
                    <span>Cosine Similarity</span>
                </div>
                <div class="drift-legend-item">
                    <div class="drift-legend-color" style="background: #ef4444;"></div>
                    <span>Divergence Threshold (0.99)</span>
                </div>
            </div>
        </div>
        
        <script src="https://d3js.org/d3.v7.min.js"></script>
        <script>
            // Drift data will be injected here when parity checks are run
            const driftData = {drift_data_json};
            
            if (driftData && driftData.length > 0) {{
                renderDriftChart(driftData);
            }}
            
            function renderDriftChart(data) {{
                const container = document.getElementById('driftChart');
                container.innerHTML = '';
                
                const margin = {{top: 20, right: 60, bottom: 60, left: 60}};
                const width = container.clientWidth - margin.left - margin.right;
                const height = 260 - margin.top - margin.bottom;
                
                const svg = d3.select('#driftChart')
                    .append('svg')
                    .attr('width', width + margin.left + margin.right)
                    .attr('height', height + margin.top + margin.bottom)
                    .append('g')
                    .attr('transform', `translate(${{margin.left}},${{margin.top}})`);
                
                // Scales
                const x = d3.scaleBand()
                    .domain(data.map((d, i) => i))
                    .range([0, width])
                    .padding(0.1);
                
                const yMSE = d3.scaleLog()
                    .domain([1e-10, d3.max(data, d => d.mse) * 10])
                    .range([height, 0]);
                
                const yCosSim = d3.scaleLinear()
                    .domain([0.9, 1])
                    .range([height, 0]);
                
                // MSE bars
                svg.selectAll('.bar-mse')
                    .data(data)
                    .enter()
                    .append('rect')
                    .attr('class', 'bar-mse')
                    .attr('x', (d, i) => x(i))
                    .attr('y', d => yMSE(Math.max(d.mse, 1e-10)))
                    .attr('width', x.bandwidth())
                    .attr('height', d => height - yMSE(Math.max(d.mse, 1e-10)))
                    .attr('fill', '#3b82f6')
                    .attr('opacity', 0.7);
                
                // Cosine similarity line
                const line = d3.line()
                    .x((d, i) => x(i) + x.bandwidth() / 2)
                    .y(d => yCosSim(d.cosine_sim))
                    .curve(d3.curveMonotoneX);
                
                svg.append('path')
                    .datum(data)
                    .attr('fill', 'none')
                    .attr('stroke', '#22c55e')
                    .attr('stroke-width', 2)
                    .attr('d', line);
                
                // Threshold line at 0.99
                svg.append('line')
                    .attr('x1', 0)
                    .attr('x2', width)
                    .attr('y1', yCosSim(0.99))
                    .attr('y2', yCosSim(0.99))
                    .attr('stroke', '#ef4444')
                    .attr('stroke-width', 1)
                    .attr('stroke-dasharray', '4,4');
                
                // Find divergence point
                const divergencePoint = data.findIndex(d => d.cosine_sim < 0.99);
                if (divergencePoint >= 0) {{
                    document.getElementById('divergenceAlert').style.display = 'flex';
                    document.getElementById('divergenceLayer').textContent = data[divergencePoint].name;
                    
                    // Highlight divergence point
                    svg.append('circle')
                        .attr('cx', x(divergencePoint) + x.bandwidth() / 2)
                        .attr('cy', yCosSim(data[divergencePoint].cosine_sim))
                        .attr('r', 6)
                        .attr('fill', '#ef4444')
                        .attr('stroke', '#fff')
                        .attr('stroke-width', 2);
                }}
                
                // Axes
                svg.append('g')
                    .attr('transform', `translate(0,${{height}})`)
                    .call(d3.axisBottom(x).tickFormat(i => data[i]?.name?.split('.').pop() || i))
                    .selectAll('text')
                    .attr('transform', 'rotate(-45)')
                    .style('text-anchor', 'end')
                    .style('fill', '#94a3b8')
                    .style('font-size', '10px');
                
                svg.append('g')
                    .call(d3.axisLeft(yMSE).ticks(5, '.0e'))
                    .selectAll('text')
                    .style('fill', '#3b82f6');
                
                svg.append('g')
                    .attr('transform', `translate(${{width}},0)`)
                    .call(d3.axisRight(yCosSim).ticks(5))
                    .selectAll('text')
                    .style('fill', '#22c55e');
            }}
        </script>
        
        <h2>Gap Analysis</h2>
        <table>
            <thead>
                <tr>
                    <th>Module Type</th>
                    <th>Count</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody>
                {gaps_table}
            </tbody>
        </table>
        
        <h2>Layers by Component</h2>
        <div class="filters">
            <input type="text" class="search-box" placeholder="Search layers..." id="searchBox">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="supported">Supported Only</button>
            <button class="filter-btn" data-filter="unsupported">Unsupported Only</button>
            <button class="filter-btn" data-filter="expand">Expand All</button>
            <button class="filter-btn" data-filter="collapse">Collapse All</button>
        </div>
        
        {components_html}
    </div>
</body>
</html>"#,
            total = data.supported + data.unsupported,
            supported = data.supported,
            unsupported = data.unsupported,
            coverage = if data.supported + data.unsupported > 0 {
                (data.supported as f64 / (data.supported + data.unsupported) as f64) * 100.0
            } else {
                100.0
            },
            gaps_table = self.render_gaps_table(&data.gaps),
            components_html = self.render_components(&data.layers),
            // Drift data - read from jsonl if exists
            drift_data_json = self.read_drift_data(),
        )
    }

    fn read_drift_data(&self) -> String {
        let path = std::path::Path::new("verification_results.jsonl");
        if !path.exists() {
            return "[]".to_string();
        }

        if let Ok(content) = std::fs::read_to_string(path) {
            let lines: Vec<String> = content
                .lines()
                .filter(|l| !l.trim().is_empty())
                .map(|l| l.to_string())
                .collect();

            format!("[{}]", lines.join(","))
        } else {
            "[]".to_string()
        }
    }

    fn render_gaps_table(&self, gaps: &HashMap<String, usize>) -> String {
        if gaps.is_empty() {
            return "<tr><td colspan=\"3\" style=\"text-align: center; color: var(--green);\">‚úÖ All module types are supported!</td></tr>".to_string();
        }

        let mut sorted: Vec<_> = gaps.iter().collect();
        sorted.sort_by(|a, b| b.1.cmp(a.1));

        sorted
            .iter()
            .map(|(module_type, count)| {
                format!(
                    r#"<tr>
                        <td class="mono">{}</td>
                        <td><span class="count-badge">{}</span></td>
                        <td><span class="status unsupported">Needs Implementation</span></td>
                    </tr>"#,
                    module_type, count
                )
            })
            .collect::<Vec<_>>()
            .join("\n")
    }

    /// Group layers by their top-level component and render as collapsible sections
    fn render_components(&self, layers: &HashMap<String, LayerMeta>) -> String {
        // Group layers by their first path component
        let mut groups: HashMap<String, Vec<(&String, &LayerMeta)>> = HashMap::new();

        for (name, meta) in layers.iter().filter(|(_, m)| m.is_leaf) {
            let component = name.split('.').next().unwrap_or(name).to_string();
            groups.entry(component).or_default().push((name, meta));
        }

        // Sort groups by name
        let mut sorted_groups: Vec<_> = groups.into_iter().collect();
        sorted_groups.sort_by(|a, b| a.0.cmp(&b.0));

        let components_html = sorted_groups
            .iter()
            .map(|(component, layers)| {
                // Sort layers within component
                let mut sorted_layers = layers.clone();
                sorted_layers.sort_by(|a, b| a.0.cmp(b.0));

                // Count supported/unsupported
                let supported_count = sorted_layers
                    .iter()
                    .filter(|(_, m)| self.is_supported(&m.module_type))
                    .count();
                let unsupported_count = sorted_layers.len() - supported_count;

                let rows: String = sorted_layers
                    .iter()
                    .map(|(name, meta)| {
                        let supported = self.is_supported(&meta.module_type);
                        let status_class = if supported {
                            "supported"
                        } else {
                            "unsupported"
                        };
                        let status_text = if supported {
                            "Supported"
                        } else {
                            "Needs Implementation"
                        };

                        // Get the short name (everything after the first dot)
                        let short_name = name.split('.').skip(1).collect::<Vec<_>>().join(".");
                        let display_name = if short_name.is_empty() {
                            name.to_string()
                        } else {
                            short_name
                        };

                        let input_shapes = meta
                            .input_shapes
                            .iter()
                            .map(|s| format!("{:?}", s))
                            .collect::<Vec<_>>()
                            .join(", ");
                        let output_shapes = meta
                            .output_shapes
                            .iter()
                            .map(|s| format!("{:?}", s))
                            .collect::<Vec<_>>()
                            .join(", ");

                        format!(
                            r#"<tr class="layer-row" data-supported="{}">
                            <td class="mono layer-name">{}</td>
                            <td class="mono">{}</td>
                            <td class="shape">{}</td>
                            <td class="shape">{}</td>
                            <td><span class="status {}">{}</span></td>
                        </tr>"#,
                            supported,
                            display_name,
                            meta.module_type,
                            input_shapes,
                            output_shapes,
                            status_class,
                            status_text
                        )
                    })
                    .collect();

                format!(
                    r#"<div class="component-section" data-has-unsupported="{}">
                        <div class="component-header">
                            <h3>
                                <span class="chevron">‚ñº</span>
                                {}
                            </h3>
                            <div class="component-stats">
                                <span class="stat">{} layers</span>
                                <span class="stat ok">‚úì {}</span>
                                {}
                            </div>
                        </div>
                        <div class="component-content">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Layer</th>
                                        <th>Type</th>
                                        <th>Input Shape</th>
                                        <th>Output Shape</th>
                                        <th>Status</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {}
                                </tbody>
                            </table>
                        </div>
                    </div>"#,
                    unsupported_count > 0,
                    component,
                    sorted_layers.len(),
                    supported_count,
                    if unsupported_count > 0 {
                        format!("<span class=\"stat err\">‚úó {}</span>", unsupported_count)
                    } else {
                        String::new()
                    },
                    rows
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        format!(
            r#"
        <div id="componentList">
            {}
        </div>
        
        <script>
            // State
            let currentFilter = 'all';
            let searchQuery = '';

            // Toggle component sections
            function setupCollapsibles() {{
                document.querySelectorAll('.component-header').forEach(header => {{
                    // Remove old listener if any
                    const newHeader = header.cloneNode(true);
                    header.parentNode.replaceChild(newHeader, header);
                    
                    newHeader.addEventListener('click', () => {{
                        newHeader.classList.toggle('collapsed');
                        newHeader.nextElementSibling.classList.toggle('collapsed');
                    }});
                }});
            }}
            setupCollapsibles();
            
            function updateVisibility() {{
                document.querySelectorAll('.component-section').forEach(section => {{
                    const rows = section.querySelectorAll('.layer-row');
                    let visibleRowsInSection = 0;
                    
                    rows.forEach(row => {{
                        const isSupported = row.dataset.supported === 'true';
                        let matchesFilter = true;
                        
                        if (currentFilter === 'supported') matchesFilter = isSupported;
                        else if (currentFilter === 'unsupported') matchesFilter = !isSupported;
                        
                        const matchesSearch = !searchQuery || row.textContent.toLowerCase().includes(searchQuery) || 
                                           section.querySelector('h3').textContent.toLowerCase().includes(searchQuery);
                        
                        const isVisible = matchesFilter && matchesSearch;
                        row.classList.toggle('hidden', !isVisible);
                        if (isVisible) visibleRowsInSection++;
                    }});
                    
                    section.classList.toggle('hidden', visibleRowsInSection === 0);
                    
                    // Auto-expand if we are filtering for unsupported and there are some
                    if (currentFilter === 'unsupported' && visibleRowsInSection > 0) {{
                        section.querySelector('.component-header').classList.remove('collapsed');
                        section.querySelector('.component-content').classList.remove('collapsed');
                    }}
                }});
            }}

            // Filter buttons
            document.querySelectorAll('.filter-btn').forEach(btn => {{
                btn.addEventListener('click', () => {{
                    const filter = btn.dataset.filter;
                    
                    if (filter === 'expand') {{
                        document.querySelectorAll('.component-header').forEach(h => h.classList.remove('collapsed'));
                        document.querySelectorAll('.component-content').forEach(c => c.classList.remove('collapsed'));
                        return;
                    }}
                    if (filter === 'collapse') {{
                        document.querySelectorAll('.component-header').forEach(h => h.classList.add('collapsed'));
                        document.querySelectorAll('.component-content').forEach(c => c.classList.add('collapsed'));
                        return;
                    }}
                    
                    document.querySelectorAll('.filter-btn').forEach(b => {{
                        if (['all', 'supported', 'unsupported'].includes(b.dataset.filter)) {{
                            b.classList.remove('active');
                        }}
                    }});
                    btn.classList.add('active');
                    
                    currentFilter = filter;
                    updateVisibility();
                }});
            }});
            
            // Search
            document.getElementById('searchBox').addEventListener('input', (e) => {{
                searchQuery = e.target.value.toLowerCase();
                updateVisibility();
            }});
        </script>
        "#,
            components_html
        )
    }
}
</file>

<file path="crates/pycandle/src/test_gen.rs">
use anyhow::{Context, Result};
use pycandle_core::LayerMeta;
use std::collections::HashMap;
use std::fs;
use std::path::PathBuf;

pub struct TestGenerator {
    model_name: String,
    crate_name: String,
    manifest: HashMap<String, LayerMeta>,
    project_name: String,
}

impl TestGenerator {
    pub fn new(model_name: String, manifest_path: PathBuf) -> Result<Self> {
        let manifest_content = fs::read_to_string(&manifest_path)
            .with_context(|| format!("Failed to read manifest at {:?}", manifest_path))?;

        let full_manifest: HashMap<String, serde_json::Value> =
            serde_json::from_str(&manifest_content).context("Failed to parse manifest JSON")?;

        let manifest: HashMap<String, LayerMeta> = full_manifest
            .into_iter()
            .filter(|(k, _)| !k.starts_with('_'))
            .map(|(k, v)| {
                let meta: LayerMeta = serde_json::from_value(v)
                    .with_context(|| format!("Failed to parse LayerMeta for {}", k))?;
                Ok((k, meta))
            })
            .collect::<Result<_>>()?;

        let stem = manifest_path.file_stem().unwrap().to_str().unwrap();
        let project_name = stem.replace("_manifest", "");

        Ok(Self {
            model_name,
            crate_name: Self::detect_crate_name().unwrap_or_else(|_| "my_project".to_string()),
            manifest,
            project_name,
        })
    }

    fn detect_crate_name() -> Result<String> {
        let content = fs::read_to_string("Cargo.toml").context("Failed to read Cargo.toml")?;
        let toml: toml::Value = toml::from_str(&content).context("Failed to parse Cargo.toml")?;

        if let Some(package) = toml.get("package") {
            if let Some(name) = package.get("name") {
                return Ok(name.as_str().unwrap_or("my_project").replace("-", "_"));
            }
        }

        Ok("my_project".to_string())
    }

    pub fn generate_test_file(&self) -> String {
        let input_count = self.count_model_inputs();
        let inputs_code = self.generate_inputs_loading_code(input_count);
        let forward_call = self.generate_forward_call(input_count);

        format!(
            r#"#[cfg(test)]
mod tests {{
    use super::*;
    use candle_core::{{Device, Tensor}};
    use pycandle_core::{{PyChecker, VerificationMode}};
    use {}::{};
    use anyhow::Result;

    #[test]
    fn test_parity() -> Result<()> {{
        // 1. Setup Device
        let device = Device::cuda_if_available(0).unwrap_or(Device::Cpu);
        println!("Running on device: {{:?}}", device);

        // 2. Load Checker and Golden Trace
        // Assumes the trace directory is in the current project root
        let checker = PyChecker::load("{}", "pycandle_trace", &device)?
            .with_mode(VerificationMode::Strict);
        println!("Loaded checker with trace: {{}}", checker.name);

        // 3. Load Model
        // We use zeros VB as a placeholder; in a real parity test, 
        // you might want to load weights using pycandle weight tools.
        let vb = candle_nn::VarBuilder::zeros(candle_core::DType::F32, &device);
        let model = {}::load(vb, Some(checker.clone()))?;

        // 4. Load Inputs from Trace
        let trace_path = format!("pycandle_trace/{}_trace.safetensors", "{}");
        let tensors = candle_core::safetensors::load(&trace_path, &device)?;
        
{}

        // 5. Run Forward Pass & Verify
{}
        println!("‚úÖ Parity test passed for {}!");

        Ok(())
    }}
}}
"#,
            self.crate_name,
            self.model_name,
            self.project_name,
            self.model_name,
            self.project_name,
            self.project_name,
            inputs_code,
            forward_call,
            self.model_name
        )
    }

    fn count_model_inputs(&self) -> usize {
        // Simple heuristic: check manifest for model_input.N keys or count input shapes of top level
        // Actually, we can just look at what's in the trace if we had it,
        // but from manifest we rely on the fact that GoldenRecorder saves 'model_input.0'
        // For now, let's assume we can find up to 10.
        // A better way: the GoldenRecorder knows.
        // Let's just check for the existence of "model_input.0" in manifest if it was recorded there?
        // Actually GoldenRecorder saves them to records, but not necessarily manifest unless they are layers.
        // But we know 'model_input.0' is the standard naming.
        // Let's just default to 1 for now if we can't find more, or maybe look at the first layer's input count.
        1
    }

    fn generate_inputs_loading_code(&self, count: usize) -> String {
        let mut code = String::new();
        for i in 0..count {
            code.push_str(&format!(
                "        let x{} = tensors.get(\"model_input.{}\").context(\"Missing model_input.{}\")?.clone();\n",
                i, i, i
            ));
        }
        code
    }

    fn generate_forward_call(&self, count: usize) -> String {
        let args = (0..count)
            .map(|i| format!("&x{}", i))
            .collect::<Vec<_>>()
            .join(", ");

        // We need to know if the model returns a single tensor or multiple
        // For now, assume single.
        let output_layer = self.detect_output_layer();
        format!(
            r#"        let output = model.forward({})?;
        checker.verify("{}", &output)?;"#,
            args, output_layer
        )
    }

    fn detect_output_layer(&self) -> String {
        // Heuristic: The layer that isn't used as an input to any other layer,
        // or the last one alphabetically if it looks like a sequence.
        // If we have FX graph, it's the last node.
        // For now, return a placeholder or try to find the "last" one.
        if let Some(last) = self.manifest.keys().max() {
            last.clone()
        } else {
            "TODO_output_layer".to_string()
        }
    }
}
</file>

<file path="py/pyproject.toml">
[project]
name = "pycandle-spy"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "numpy",
    "packaging",
    "safetensors",
    "torch>=2.0.0",
    "onnx",
    "onnx2torch",
    "onnxscript",
]
</file>

<file path="tests/parity.rs">
#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::{Device, Tensor};
    use pycandle_core::PyChecker;
    use my_project::MyModel;
    use anyhow::Result;

    #[test]
    fn test_parity() -> Result<()> {
        // 1. Setup Device
        let device = Device::cuda_if_available(0).unwrap_or(Device::Cpu);
        println!("Running on device: {:?}", device);

        // 2. Load Checker and Golden Trace
        // Assumes "pycandle_trace" directory exists with trace files
        let checker = PyChecker::load("debug_run", "pycandle_trace", &device)?;
        println!("Loaded checker with trace: {}", checker.name);

        // 3. Load Model
        // We use the same variable builder as normally, but verify loaded weights match if needed
        // For parity test, we rely on the implementation's load to initialize weights correctly 
        // (often random or specific config). 
        // Ideally, we should load exact weights from the trace if available, but for now 
        // we assume the user might have a load_from_weights or similar if critical.
        // However, for pure activation parity on specific inputs, we usually need the weights to match.
        // TODO: In a real scenario, we might need to load weights from the trace too.
        // For now, we assume the user constructs the model. 
        // NOTE: If the model uses random initialization, this test WILL FAIL unless 
        // we load weights from the python trace.
        // 
        // As a workaround for this generic generator, we assume the user has a `load` function
        // that takes the checker.
        let vb = candle_nn::VarBuilder::zeros(candle_core::DType::F32, &device);
        let model = MyModel::load(vb, Some(checker.clone()))?;

        // 4. Load Inputs from Trace
        // The trace should contain 'model_input.0', 'model_input.1', etc.
        // We'll try to load at least the first input.
        // Note: PyChecker holds "golden_tensors", which contains the inputs too if we saved them!
        
        // We access the inputs directly from the golden tensors loaded in checker
        // (This requires PyChecker to expose golden_tensors or a getter, or we access directly if pub)
        // Since `golden_tensors` is private in PyChecker but we need it, ensure PyChecker exposes a way.
        // Assuming PyChecker has a `get_tensor` method or we can clone from the file.
        // Let's use `candle_core::safetensors::load` again here for simplicity to get inputs.
        
        let tensors = candle_core::safetensors::load("pycandle_trace/debug_run_trace.safetensors", &device)?;
        
        // 5. Run Forward Pass
        if let Some(input) = tensors.get("model_input.0") {
            let _output = model.forward(input)?;
            println!("Forward pass completed successfully!");
        } else {
            eprintln!("‚ö†Ô∏è No 'model_input.0' found in trace. Skipping forward pass execution.");
            println!("Available keys: {:?}", tensors.keys().take(5));
        }

        Ok(())
    }
}
</file>

<file path="crates/pycandle-core/src/checker.rs">
//! PyChecker - Golden tensor comparison for parity verification

use candle_core::{Device, Error, Result, Shape, Tensor};
use colored::*;
use serde::{Deserialize, Serialize};
use std::cell::RefCell;
use std::collections::HashMap;
use std::fs::OpenOptions;
use std::io::Write;
use std::path::Path;

/// Mode for verification: Strict (panic on failure) or DriftTracking (record and continue)
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum VerificationMode {
    Strict,
    DriftTracking,
}

/// Metadata for a recorded layer
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct LayerMeta {
    pub name: String,
    pub module_type: String,
    pub input_shapes: Vec<Vec<usize>>,
    pub output_shapes: Vec<Vec<usize>>,
    pub parameters: Vec<String>,
    pub buffers: Vec<String>,
    pub is_leaf: bool,
    pub config: serde_json::Value,
}

/// Result of comparing two tensors
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ComparisonResult {
    pub name: String,
    pub mse: f32,
    pub max_diff: f32,
    pub cosine_sim: f32,
    pub passed: bool,
    pub heatmap: Option<Vec<f32>>,
}

/// PyChecker loads golden tensors and verifies Rust outputs against them
#[derive(Clone)]
pub struct PyChecker {
    pub name: String,
    golden_tensors: HashMap<String, Tensor>,
    pub manifest: HashMap<String, LayerMeta>,
    pub atol: f32,
    pub rtol: f32,
    pub device: Device,
    pub mode: VerificationMode,
    history: RefCell<Vec<ComparisonResult>>,
}

impl PyChecker {
    /// Load golden records from safetensors and manifest JSON
    pub fn load<P: AsRef<Path>>(project_name: &str, base_path: P, device: &Device) -> Result<Self> {
        let base = base_path.as_ref();
        let tensor_path = base.join(format!("{}_trace.safetensors", project_name));
        let manifest_path = base.join(format!("{}_manifest.json", project_name));

        let golden_tensors = candle_core::safetensors::load(&tensor_path, device)?;

        let manifest_file = std::fs::read_to_string(&manifest_path)
            .map_err(|e| Error::Msg(format!("Failed to read manifest: {}", e)))?;
        let full_manifest: HashMap<String, serde_json::Value> =
            serde_json::from_str(&manifest_file)
                .map_err(|e| Error::Msg(format!("Failed to parse manifest: {}", e)))?;

        let manifest: HashMap<String, LayerMeta> = full_manifest
            .into_iter()
            .filter(|(k, _)| !k.starts_with('_'))
            .map(|(k, v)| {
                let meta: LayerMeta = serde_json::from_value(v).map_err(|e| {
                    Error::Msg(format!("Failed to parse LayerMeta for {}: {}", k, e))
                })?;
                Ok((k, meta))
            })
            .collect::<Result<HashMap<String, LayerMeta>>>()?;

        Ok(Self {
            name: project_name.to_string(),
            golden_tensors,
            manifest,
            atol: 1e-4,
            rtol: 1e-4,
            device: device.clone(),
            mode: VerificationMode::Strict,
            history: RefCell::new(Vec::new()),
        })
    }

    /// Set verification mode
    pub fn with_mode(mut self, mode: VerificationMode) -> Self {
        self.mode = mode;
        self
    }

    /// Verify a tensor against the golden record for a layer
    pub fn verify(&self, layer_name: &str, actual: &Tensor) -> Result<ComparisonResult> {
        let key = format!("{}.out.0", layer_name);
        let expected = self.golden_tensors.get(&key).ok_or_else(|| {
            Error::Msg(format!(
                "Layer '{}' not found in trace. Available: {:?}",
                layer_name,
                self.golden_tensors.keys().take(5).collect::<Vec<_>>()
            ))
        })?;

        if actual.shape() != expected.shape() {
            self.diagnose_shape_mismatch(layer_name, actual.shape(), expected.shape());
            return Err(Error::Msg(format!(
                "Shape mismatch for {}: actual {:?}, expected {:?}",
                layer_name,
                actual.shape(),
                expected.shape()
            )));
        }

        let mut result = self.compare_tensors(actual, expected)?;
        result.name = layer_name.to_string();

        if result.mse > self.atol {
            // Compute heatmap for dashboard
            result.heatmap = self.compute_heatmap(actual, expected);

            // In Strict mode, we fail immediately
            if self.mode == VerificationMode::Strict {
                self.report_failure(layer_name, &result, actual, expected);
                self.log_result(&result);
                return Err(Error::Msg(format!(
                    "Numerical parity failed for {}",
                    layer_name
                )));
            } else {
                // In DriftTracking mode, we warn but continue
                println!(
                    "{} Layer '{}' drifted. (MSE: {:.2e})",
                    "‚ö†".yellow(),
                    layer_name.yellow(),
                    result.mse
                );
            }
        } else {
            println!(
                "{} Layer '{}' passed. (MSE: {:.2e}, CosSim: {:.4})",
                "‚úî".green(),
                layer_name.yellow(),
                result.mse,
                result.cosine_sim
            );
        }

        self.log_result(&result);
        self.history.borrow_mut().push(result.clone());
        Ok(result)
    }

    /// Print a report of the most sensitive layers (highest drift)
    pub fn print_drift_report(&self) {
        let history = self.history.borrow();
        if history.is_empty() {
            return;
        }

        println!("\n{}", "üìâ QUANTIZATION DRIFT REPORT".blue().bold());
        println!("{:<40} | {:<12} | {:<12}", "Layer", "MSE", "Status");
        println!("{}", "-".repeat(70));

        let mut sorted_history = history.clone();
        sorted_history.sort_by(|a, b| b.mse.partial_cmp(&a.mse).unwrap());

        for res in sorted_history.iter().take(20) {
            let status = if res.mse > self.atol {
                "DRIFT".red()
            } else {
                "OK".green()
            };
            println!("{:<40} | {:.2e}   | {}", res.name, res.mse, status);
        }
        println!("\n");
    }

    fn log_result(&self, result: &ComparisonResult) {
        if let Ok(json) = serde_json::to_string(result) {
            if let Ok(mut file) = OpenOptions::new()
                .create(true)
                .append(true)
                .open("verification_results.jsonl")
            {
                let _ = writeln!(file, "{}", json);
            }
        }
    }

    fn compare_tensors(&self, a: &Tensor, b: &Tensor) -> Result<ComparisonResult> {
        let diff = (a - b)?;
        let mse = diff.sqr()?.mean_all()?.to_scalar::<f32>()?;
        let max_diff = diff.abs()?.max_all()?.to_scalar::<f32>()?;

        let a_flat = a.flatten_all()?;
        let b_flat = b.flatten_all()?;
        let dot = (&a_flat * &b_flat)?.sum_all()?.to_scalar::<f32>()?;
        let norm_a = a_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let norm_b = b_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let cosine_sim = dot / (norm_a * norm_b + 1e-8);

        Ok(ComparisonResult {
            name: "unknown".to_string(), // Will be overwritten by caller
            mse,
            max_diff,
            cosine_sim,
            passed: mse <= self.atol,
            heatmap: None,
        })
    }

    fn diagnose_shape_mismatch(&self, name: &str, actual: &Shape, expected: &Shape) {
        println!("\n{}", "‚ùå SHAPE MISMATCH DETECTED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  Rust:   {:?}", actual.dims());
        println!("  Python: {:?}", expected.dims());

        let a_dims = actual.dims();
        let e_dims = expected.dims();

        if a_dims.len() == 3 && e_dims.len() == 3 {
            if a_dims[1] == e_dims[2] && a_dims[2] == e_dims[1] {
                println!(
                    "{}",
                    "üí° DIAGNOSIS: Dimension Swap. (B, C, T) vs (B, T, C). Try .transpose(1, 2)?"
                        .cyan()
                );
            }
        }
        println!("{}\n", "---------------------------".red());
    }

    fn report_failure(
        &self,
        name: &str,
        res: &ComparisonResult,
        actual: &Tensor,
        expected: &Tensor,
    ) {
        println!("\n{}", "‚ùå NUMERICAL PARITY FAILED".red().bold());
        println!("Layer: {}", name.yellow());
        println!("  MSE:      {:.8}", res.mse);
        println!("  Cos Sim:  {:.8}", res.cosine_sim);

        let a_mean = actual.mean_all().unwrap().to_scalar::<f32>().unwrap_or(0.0);
        let e_mean = expected
            .mean_all()
            .unwrap()
            .to_scalar::<f32>()
            .unwrap_or(0.0);
        println!("  Means:    Rust={:.6}, Py={:.6}", a_mean, e_mean);

        // --- Active Debugging Artifacts ---
        let failures_dir = Path::new("failures");
        if !failures_dir.exists() {
            let _ = std::fs::create_dir(failures_dir);
        }

        // 1. Save Tensor Snippet (.safetensors)
        let snippet_path = failures_dir.join(format!("{}.safetensors", name));
        let tensors_to_save = HashMap::from([
            ("rust_actual".to_string(), actual.clone()),
            ("py_golden".to_string(), expected.clone()),
        ]);
        if let Err(e) = candle_core::safetensors::save(&tensors_to_save, &snippet_path) {
            println!("  Failed to save snippet: {}", e);
        } else {
            println!(
                "  üíæ Snippet saved: {}",
                snippet_path.display().to_string().cyan()
            );
        }

        // 2. Generate Python Analysis Script
        let script_path = failures_dir.join(format!("debug_{}.py", name));
        let script_content = format!(
            r#"
import torch
from safetensors.torch import load_file
import matplotlib.pyplot as plt
import numpy as np

def analyze():
    print(f"üîç Analyzing Failure: {{'{name}'}}")
    tensors = load_file("{filename}")
    rust = tensors["rust_actual"]
    gold = tensors["py_golden"]

    diff = (rust - gold).abs()
    print(f"  Max Diff: {{diff.max().item():.6f}}")
    print(f"  MSE:      {{(diff ** 2).mean().item():.8f}}")

    # Plot
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.title("Rust Tensor Histogram")
    plt.hist(rust.flatten().float().numpy(), bins=50, alpha=0.7, label='Rust')
    plt.hist(gold.flatten().float().numpy(), bins=50, alpha=0.7, label='Gold')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.title("Difference Heatmap (First Slice)")
    if diff.ndim > 1:
        plt.imshow(diff.flatten(0, -2)[0].float().numpy(), cmap='hot', aspect='auto')
    else:
        plt.plot(diff.float().numpy())
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    analyze()
"#,
            name = name,
            filename = format!("{}.safetensors", name)
        );

        if let Ok(mut file) = std::fs::File::create(&script_path) {
            let _ = file.write_all(script_content.as_bytes());
            println!(
                "  üêç Script generated: {}",
                script_path.display().to_string().cyan()
            );
        }

        println!("{}\n", "---------------------------".red());
    }

    pub fn compute_heatmap(&self, a: &Tensor, b: &Tensor) -> Option<Vec<f32>> {
        // Compute absolute difference
        let diff = (a - b).ok()?.abs().ok()?;

        // 1. Flatten to 1D
        let flat = diff.flatten_all().ok()?;
        let numel = flat.elem_count();

        // 2. We want an 8x8 grid = 64 buckets
        let grid_size = 64;
        if numel < grid_size {
            return None; // Too small to pool usefuly
        }

        let chunk_size = numel / grid_size;

        // 3. Simple max pooling into buckets
        // Walking through the tensor on CPU is slow for huge tensors, but this is a failure case anyway.
        // A faster way is to reshape (64, chunk_size) and max(dim=1).

        let reshaped = flat
            .narrow(0, 0, grid_size * chunk_size)
            .ok()?
            .reshape((grid_size, chunk_size))
            .ok()?;

        let pooled = reshaped.max(1).ok()?;

        // Convert to Vec<f32>
        pooled.to_vec1::<f32>().ok()
    }
}
</file>

<file path="crates/pycandle-core/src/codegen/gpt2.rs">
//! GPT2 code generation helpers
//!
//! This module provides codegen utilities for HuggingFace GPT2 models.

use crate::LayerMeta;

/// Configuration extracted from GPT2 manifest
#[derive(Debug, Clone)]
pub struct GptConfig {
    pub vocab_size: usize,
    pub context_length: usize,
    pub emb_dim: usize,
    pub n_heads: usize,
    pub n_layers: usize,
    pub drop_rate: f32,
}

impl Default for GptConfig {
    fn default() -> Self {
        Self {
            vocab_size: 50257,
            context_length: 1024,
            emb_dim: 768,
            n_heads: 12,
            n_layers: 12,
            drop_rate: 0.1,
        }
    }
}

/// Check if module type is a GPT2 variant
pub fn is_gpt2_type(module_type: &str) -> bool {
    matches!(
        module_type,
        "GPT2Model" | "GPT2LMHeadModel" | "GPT2Block" | "GPT2Attention" | "GPT2MLP"
    )
}

/// Map GPT2 Python type to Candle type
pub fn map_type(py_type: &str) -> Option<String> {
    match py_type {
        "GPT2Model" | "GPT2LMHeadModel" => Some("gpt2::GPTModel".to_string()),
        "GPT2Block" => Some("gpt2::TransformerBlock".to_string()),
        "GPT2Attention" => Some("gpt2::MultiHeadAttention".to_string()),
        "GPT2MLP" => Some("gpt2::FeedForward".to_string()),
        _ => None,
    }
}

/// Extract GPT config from layer metadata
pub fn extract_config(meta: &LayerMeta) -> GptConfig {
    GptConfig {
        vocab_size: meta
            .config
            .get("vocab_size")
            .and_then(|v| v.as_u64())
            .unwrap_or(50257) as usize,
        context_length: meta
            .config
            .get("n_positions")
            .and_then(|v| v.as_u64())
            .unwrap_or(1024) as usize,
        emb_dim: meta
            .config
            .get("n_embd")
            .and_then(|v| v.as_u64())
            .unwrap_or(768) as usize,
        n_heads: meta
            .config
            .get("n_head")
            .and_then(|v| v.as_u64())
            .unwrap_or(12) as usize,
        n_layers: meta
            .config
            .get("n_layer")
            .and_then(|v| v.as_u64())
            .unwrap_or(12) as usize,
        drop_rate: meta
            .config
            .get("resid_pdrop")
            .and_then(|v| v.as_f64())
            .unwrap_or(0.1) as f32,
    }
}

/// Generate initialization code for GPT2 layer types
pub fn generate_init(
    layer_name: &str,
    meta: &LayerMeta,
    symbolic_dims: &std::collections::HashMap<String, usize>,
) -> Option<String> {
    let render_dim = |val: usize, preferred: &str| -> String {
        if !preferred.is_empty() {
            if let Some(&v) = symbolic_dims.get(preferred) {
                if v == val {
                    return format!("config.{}", preferred);
                }
            }
        }
        for (name, &v) in symbolic_dims {
            if v == val {
                return format!("config.{}", name);
            }
        }
        val.to_string()
    };

    match meta.module_type.as_str() {
        "GPT2Model" | "GPT2LMHeadModel" => {
            let config_val = extract_config(meta);
            let vocab_size = render_dim(config_val.vocab_size, "vocab_size");
            let context_length = render_dim(config_val.context_length, "context_length");
            let emb_dim = render_dim(config_val.emb_dim, "hidden_dim");
            let n_heads = render_dim(config_val.n_heads, "n_head");
            let n_layers = render_dim(config_val.n_layers, "n_layers");

            Some(format!(
                r#"{{
                let gpt_cfg = pycandle_core::gpt2::Config {{
                    vocab_size: {},
                    context_length: {},
                    emb_dim: {},
                    n_heads: {},
                    n_layers: {},
                    drop_rate: {:.1},
                    qkv_bias: false,
                }};
                pycandle_core::gpt2::GPTModel::new(gpt_cfg, &vb.pp("{}"))?
            }}"#,
                vocab_size,
                context_length,
                emb_dim,
                n_heads,
                n_layers,
                config_val.drop_rate,
                layer_name
            ))
        }
        "GPT2Block" => Some(format!(
            "pycandle_core::gpt2::TransformerBlock::new(gpt2_cfg, &vb.pp(\"{}\"))?",
            layer_name
        )),
        "GPT2Attention" => {
            let dim_val = meta
                .config
                .get("n_embd")
                .and_then(|v| v.as_u64())
                .unwrap_or(768) as usize;
            let heads_val = meta
                .config
                .get("n_head")
                .and_then(|v| v.as_u64())
                .unwrap_or(12) as usize;
            let dim = render_dim(dim_val, "hidden_dim");
            let heads = render_dim(heads_val, "n_head");

            Some(format!(
                "pycandle_core::gpt2::MultiHeadAttention::new({}, {}, 0.1, {}, false, &vb.pp(\"{}\"))?",
                dim, dim, heads, layer_name
            ))
        }
        "GPT2MLP" => Some(format!(
            "pycandle_core::gpt2::FeedForward::new(gpt2_cfg, &vb.pp(\"{}\"))?",
            layer_name
        )),
        _ => None,
    }
}
</file>

<file path="crates/pycandle-core/src/layers.rs">
//! Neural network layer implementations for Candle
//!
//! These provide PyTorch-compatible implementations that Candle doesn't have built-in.

use candle_nn::Module;

use candle_core::{IndexOp, Result, Tensor};

// ============================================================================
// Activation Functions
// ============================================================================

/// ReLU activation: max(0, x)
pub struct ReLU;
impl ReLU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.relu()
    }
}

/// GELU activation (Gaussian Error Linear Unit)
pub struct GELU;
impl GELU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.gelu_erf()
    }
}

/// Sigmoid activation: 1 / (1 + exp(-x))
pub struct Sigmoid;
impl Sigmoid {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        candle_nn::ops::sigmoid(x)
    }
}

/// Tanh activation
pub struct Tanh;
impl Tanh {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.tanh()
    }
}

/// ELU activation: x if x > 0, else alpha * (exp(x) - 1)
pub struct ELU {
    pub alpha: f64,
}
impl ELU {
    pub fn new(alpha: f64) -> Self {
        Self { alpha }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.elu(self.alpha)
    }
}

/// LeakyReLU activation: x if x > 0, else negative_slope * x
pub struct LeakyReLU {
    pub negative_slope: f64,
}
impl LeakyReLU {
    pub fn new(negative_slope: f64) -> Self {
        Self { negative_slope }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // leaky_relu: max(0, x) + negative_slope * min(0, x)
        let zeros = x.zeros_like()?;
        let pos = x.maximum(&zeros)?;
        let neg = x.minimum(&zeros)?;
        pos + (neg * self.negative_slope)?
    }
}

/// Snake activation: x + sin¬≤(Œ±x)/Œ±
/// Used in neural vocoders like BigVGAN
pub struct Snake {
    pub alpha: Tensor,
}
impl Snake {
    pub fn load(vb: candle_nn::VarBuilder, in_features: usize) -> Result<Self> {
        let alpha = vb
            .get((in_features,), "alpha")
            .or_else(|_| vb.get((1, in_features, 1), "alpha"))?;
        let alpha = alpha.reshape((1, in_features, 1))?;
        Ok(Self { alpha })
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, T), alpha: (1, C, 1)
        let ax = x.broadcast_mul(&self.alpha)?;
        let sin_ax = ax.sin()?;
        let sin_sq = sin_ax.sqr()?;
        x + sin_sq.broadcast_div(&self.alpha)?
    }
}

/// Mish activation: x * tanh(softplus(x))
pub struct Mish;
impl Mish {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Mish(x) = x * tanh(softplus(x))
        // softplus(x) = ln(1 + exp(x))
        let sp = (x.exp()? + 1.0)?.log()?;
        x.broadcast_mul(&sp.tanh()?)
    }
}

/// SiLU / Swish activation: x * sigmoid(x)
pub struct SiLU;
impl SiLU {
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x * candle_nn::ops::sigmoid(x)?
    }
}

// ============================================================================
// Normalization Layers
// ============================================================================

/// BatchNorm1d for inference (uses running statistics)
/// Input: (B, C, T) or (B, C)
pub struct BatchNorm1d {
    pub weight: Tensor, // gamma
    pub bias: Tensor,   // beta
    pub running_mean: Tensor,
    pub running_var: Tensor,
    pub eps: f64,
}

impl BatchNorm1d {
    pub fn load(vb: candle_nn::VarBuilder, num_features: usize) -> Result<Self> {
        let weight = vb.get((num_features,), "weight")?;
        let bias = vb.get((num_features,), "bias")?;
        let running_mean = vb.get((num_features,), "running_mean")?;
        let running_var = vb.get((num_features,), "running_var")?;
        Ok(Self {
            weight,
            bias,
            running_mean,
            running_var,
            eps: 1e-5,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, T) for 1d or (B, C)
        // Normalize: (x - mean) / sqrt(var + eps) * weight + bias
        let ndim = x.dims().len();
        let (mean, var, weight, bias) = if ndim == 3 {
            // (B, C, T) - unsqueeze to (1, C, 1)
            (
                self.running_mean.unsqueeze(0)?.unsqueeze(2)?,
                self.running_var.unsqueeze(0)?.unsqueeze(2)?,
                self.weight.unsqueeze(0)?.unsqueeze(2)?,
                self.bias.unsqueeze(0)?.unsqueeze(2)?,
            )
        } else {
            // (B, C) - unsqueeze to (1, C)
            (
                self.running_mean.unsqueeze(0)?,
                self.running_var.unsqueeze(0)?,
                self.weight.unsqueeze(0)?,
                self.bias.unsqueeze(0)?,
            )
        };

        let normalized = x
            .broadcast_sub(&mean)?
            .broadcast_div(&(var + self.eps)?.sqrt()?)?;
        normalized.broadcast_mul(&weight)?.broadcast_add(&bias)
    }
}

/// BatchNorm2d for inference (uses running statistics)
/// Input: (B, C, H, W)
pub struct BatchNorm2d {
    pub weight: Tensor,
    pub bias: Tensor,
    pub running_mean: Tensor,
    pub running_var: Tensor,
    pub eps: f64,
}

impl BatchNorm2d {
    pub fn load(vb: candle_nn::VarBuilder, num_features: usize) -> Result<Self> {
        let weight = vb.get((num_features,), "weight")?;
        let bias = vb.get((num_features,), "bias")?;
        let running_mean = vb.get((num_features,), "running_mean")?;
        let running_var = vb.get((num_features,), "running_var")?;
        Ok(Self {
            weight,
            bias,
            running_mean,
            running_var,
            eps: 1e-5,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, H, W)
        // Reshape stats to (1, C, 1, 1) for broadcasting
        let mean = self.running_mean.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let var = self.running_var.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let weight = self.weight.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;
        let bias = self.bias.unsqueeze(0)?.unsqueeze(2)?.unsqueeze(3)?;

        let normalized = x
            .broadcast_sub(&mean)?
            .broadcast_div(&(var + self.eps)?.sqrt()?)?;
        normalized.broadcast_mul(&weight)?.broadcast_add(&bias)
    }
}

// ============================================================================
// Recurrent Layers
// ============================================================================

/// LSTM layer (multi-layer, unidirectional)
/// Input: (B, T, input_size) if batch_first=true
/// Output: (output, (h_n, c_n))
pub struct LSTM {
    pub weight_ih: Vec<Tensor>, // One per layer: (4*hidden, input_size or hidden_size)
    pub weight_hh: Vec<Tensor>, // One per layer: (4*hidden, hidden_size)
    pub bias_ih: Vec<Tensor>,   // One per layer: (4*hidden,)
    pub bias_hh: Vec<Tensor>,   // One per layer: (4*hidden,)
    pub num_layers: usize,
    pub hidden_size: usize,
}

impl LSTM {
    pub fn load(
        vb: candle_nn::VarBuilder,
        input_size: usize,
        hidden_size: usize,
        num_layers: usize,
    ) -> Result<Self> {
        let mut weight_ih = Vec::new();
        let mut weight_hh = Vec::new();
        let mut bias_ih = Vec::new();
        let mut bias_hh = Vec::new();

        for layer in 0..num_layers {
            let in_size = if layer == 0 { input_size } else { hidden_size };
            weight_ih.push(vb.get((4 * hidden_size, in_size), &format!("weight_ih_l{}", layer))?);
            weight_hh.push(vb.get(
                (4 * hidden_size, hidden_size),
                &format!("weight_hh_l{}", layer),
            )?);
            bias_ih.push(vb.get((4 * hidden_size,), &format!("bias_ih_l{}", layer))?);
            bias_hh.push(vb.get((4 * hidden_size,), &format!("bias_hh_l{}", layer))?);
        }

        Ok(Self {
            weight_ih,
            weight_hh,
            bias_ih,
            bias_hh,
            num_layers,
            hidden_size,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<(Tensor, (Tensor, Tensor))> {
        // x: (B, T, input_size) assuming batch_first
        let (batch, seq_len, _) = x.dims3()?;
        let device = x.device();
        let dtype = x.dtype();

        let h = Tensor::zeros((self.num_layers, batch, self.hidden_size), dtype, device)?;
        let c = Tensor::zeros((self.num_layers, batch, self.hidden_size), dtype, device)?;
        let mut output = x.clone();

        for layer in 0..self.num_layers {
            let mut h_t = h.i(layer)?;
            let mut c_t = c.i(layer)?;
            let mut outputs = Vec::new();

            for t in 0..seq_len {
                let x_t = output.i((.., t, ..))?;

                // gates = x @ W_ih.T + h @ W_hh.T + b_ih + b_hh
                let gates = x_t
                    .matmul(&self.weight_ih[layer].t()?)?
                    .broadcast_add(&h_t.matmul(&self.weight_hh[layer].t()?)?)?
                    .broadcast_add(&self.bias_ih[layer])?
                    .broadcast_add(&self.bias_hh[layer])?;

                // Split into i, f, g, o (each of size hidden_size)
                let chunks = gates.chunk(4, 1)?;
                let i_gate = candle_nn::ops::sigmoid(&chunks[0])?;
                let f_gate = candle_nn::ops::sigmoid(&chunks[1])?;
                let g_gate = chunks[2].tanh()?;
                let o_gate = candle_nn::ops::sigmoid(&chunks[3])?;

                c_t = f_gate
                    .broadcast_mul(&c_t)?
                    .broadcast_add(&i_gate.broadcast_mul(&g_gate)?)?;
                h_t = o_gate.broadcast_mul(&c_t.tanh()?)?;

                outputs.push(h_t.unsqueeze(1)?);
            }

            output = Tensor::cat(&outputs, 1)?;
        }

        Ok((output, (h, c)))
    }
}

// ============================================================================
// Specialized Layers
// ============================================================================

/// CausalConv1d: A 1D convolution with causal padding
/// Ensures that output at time t only depends on inputs at time <= t
pub struct CausalConv1d {
    pub conv: candle_nn::Conv1d,
    pub padding: usize,
}

impl CausalConv1d {
    pub fn load(
        vb: candle_nn::VarBuilder,
        in_channels: usize,
        out_channels: usize,
        kernel_size: usize,
        stride: usize,
        bias: bool,
    ) -> Result<Self> {
        let padding = kernel_size - 1;
        let config = candle_nn::Conv1dConfig {
            stride,
            padding,
            ..Default::default()
        };
        let conv = if bias {
            candle_nn::conv1d(in_channels, out_channels, kernel_size, config, vb)?
        } else {
            candle_nn::conv1d_no_bias(in_channels, out_channels, kernel_size, config, vb)?
        };
        Ok(Self { conv, padding })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.conv.forward(x)?;
        // Causal slice: remove the 'future' padding at the end
        if self.padding > 0 {
            let dim = x.dims().len() - 1;
            let seq_len = x.dim(dim)?;
            x.narrow(dim, 0, seq_len - self.padding)
        } else {
            Ok(x)
        }
    }
}

/// Dropout layer (inference no-op)
pub struct Dropout {
    pub p: f32,
}
impl Dropout {
    pub fn new() -> Self {
        Self { p: 0.5 }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        Ok(x.clone())
    }
}

/// Transpose layer (swaps two dimensions)
pub struct Transpose {
    pub dim0: usize,
    pub dim1: usize,
}
impl Transpose {
    pub fn new(dim0: usize, dim1: usize) -> Self {
        Self { dim0, dim1 }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        x.transpose(self.dim0, self.dim1)
    }
}

/// ConvTranspose1d implementation
pub struct ConvTranspose1d {
    inner: candle_nn::ConvTranspose1d,
}

impl ConvTranspose1d {
    pub fn load(
        vb: candle_nn::VarBuilder,
        in_c: usize,
        out_c: usize,
        kernel_size: usize,
        stride: usize,
        padding: usize,
    ) -> Result<Self> {
        let weight = vb.get((in_c, out_c, kernel_size), "weight").or_else(|_| {
            vb.pp("parametrizations.weight")
                .get((in_c, out_c, kernel_size), "original1")
        })?;
        let bias = vb.get((out_c,), "bias").ok();

        let config = candle_nn::ConvTranspose1dConfig {
            stride,
            padding,
            ..Default::default()
        };
        let inner = candle_nn::ConvTranspose1d::new(weight, bias, config);
        Ok(Self { inner })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        self.inner.forward(x)
    }
}

/// Sinusoidal Positional Embedding
pub struct SinusoidalPosEmb {
    pub dim: usize,
}
impl SinusoidalPosEmb {
    pub fn new(dim: usize) -> Self {
        Self { dim }
    }
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let half_dim = self.dim / 2;
        let device = x.device();
        let dtype = x.dtype();
        let inv_freq: Vec<_> = (0..half_dim)
            .map(|i| 1.0f32 / (10000.0f32.powf(i as f32 / (half_dim as f32 - 1.0f32))))
            .collect();
        let inv_freq = Tensor::from_vec(inv_freq, half_dim, device)?.to_dtype(dtype)?;
        let emb = x.unsqueeze(1)?.broadcast_mul(&inv_freq.unsqueeze(0)?)?;
        Tensor::cat(&[emb.sin()?, emb.cos()?], 1)
    }
}

/// Helper to load a Linear layer that uses PyTorch Weight Normalization.
/// Composes w = g * (v / ||v||) at runtime.
pub fn load_weight_norm_linear(
    vb: candle_nn::VarBuilder,
    in_f: usize,
    out_f: usize,
    bias: bool,
) -> Result<candle_nn::Linear> {
    // Attempt to load 'original1' (v) and 'original0' (g) from parametrizations
    // If not found, try legacy 'weight_v' and 'weight_g'

    let v_path = if vb.contains_tensor("parametrizations.weight.original1") {
        "parametrizations.weight.original1"
    } else {
        "weight_v"
    };

    let g_path = if vb.contains_tensor("parametrizations.weight.original0") {
        "parametrizations.weight.original0"
    } else {
        "weight_g"
    };

    let v = vb.get((out_f, in_f), v_path)?; // Shape: [out, in]
    let g = vb.get((out_f, 1), g_path)?; // Shape: [out, 1]

    // Normalize v: v / ||v||
    // Norm along dim 1 (input dimension) for Linear
    let v_norm = v.sqr()?.sum_keepdim(1)?.sqrt()?;

    // Compose: w = g * (v / norm)
    let w = v.broadcast_div(&v_norm)?.broadcast_mul(&g)?;

    // Transpose for Candle (Candle Linear is [out, in], but usually we transpose standard weights.
    // However, weight_norm vectors in PT are usually [out, in].
    // Candle's Linear::new expects weight to be [out, in].
    // Let's standardise on returning the layer directly.

    let b = if bias {
        Some(vb.get((out_f,), "bias")?)
    } else {
        None
    };

    Ok(candle_nn::Linear::new(w, b))
}
</file>

<file path=".gitignore">
/target
Cargo.lock
# chatterbox-repo/*/
*.safetensors
*.pyc
__pycache__/
.venv/
pycandle_trace/
py_trace/
*.html
py/__pycache__/*/
</file>

<file path="crates/pycandle-audio/src/lib.rs">
//! PyCandle Audio - Audio operations with PyTorch parity
//!
//! This crate provides STFT, iSTFT, and padding operations that match
//! PyTorch's behavior for audio model porting.

use candle_core::{Result, Tensor};

/// Padding modes for audio operations
#[derive(Debug, Clone, Copy)]
pub enum PadMode {
    /// Reflect padding: [1,2,3] -> [3,2,1,2,3,2,1]
    Reflect,
    /// Replicate edge values
    Replicate,
    /// Pad with constant value
    Constant(f64),
}

/// Mel scale types
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MelScale {
    /// HTK scale: 2595 * log10(1 + f / 700)
    Htk,
    /// Slaney scale: linear below 1kHz, log above
    Slaney,
}

/// Normalization modes for Mel banks
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MelNorm {
    /// No normalization
    None,
    /// Slaney-style area normalization
    Slaney,
}

/// MelSpectrogram configuration matching torchaudio.transforms.MelSpectrogram
#[derive(Debug, Clone)]
pub struct MelSpectrogramConfig {
    pub stft_config: StftConfig,
    pub sample_rate: usize,
    pub n_mels: usize,
    pub f_min: f64,
    pub f_max: Option<f64>,
    pub mel_scale: MelScale,
    pub norm: MelNorm,
}

impl Default for MelSpectrogramConfig {
    fn default() -> Self {
        Self {
            stft_config: StftConfig::default(),
            sample_rate: 16000,
            n_mels: 128,
            f_min: 0.0,
            f_max: None,
            mel_scale: MelScale::Htk,
            norm: MelNorm::None,
        }
    }
}

/// STFT configuration matching PyTorch's torch.stft
#[derive(Debug, Clone)]
pub struct StftConfig {
    pub n_fft: usize,
    pub hop_length: Option<usize>,
    pub win_length: Option<usize>,
    pub center: bool,
    pub pad_mode: PadMode,
    pub normalized: bool,
    pub onesided: bool,
    pub return_complex: bool,
}

impl Default for StftConfig {
    fn default() -> Self {
        Self {
            n_fft: 400,
            hop_length: None,
            win_length: None,
            center: true,
            pad_mode: PadMode::Reflect,
            normalized: false,
            onesided: true,
            return_complex: true,
        }
    }
}

/// Apply 1D padding to a tensor
///
/// # Arguments
/// * `input` - Tensor of shape (B, T) or (B, C, T)
/// * `pad_left` - Amount of padding on the left
/// * `pad_right` - Amount of padding on the right
/// * `mode` - Padding mode
pub fn pad_1d(input: &Tensor, pad_left: usize, pad_right: usize, mode: PadMode) -> Result<Tensor> {
    let ndim = input.dims().len();
    let time_dim = ndim - 1;
    let time_len = input.dim(time_dim)?;

    match mode {
        PadMode::Reflect => {
            // Reflect padding - build indices and use index_select
            if pad_left > time_len - 1 || pad_right > time_len - 1 {
                return Err(candle_core::Error::Msg(
                    "Reflect padding size exceeds input size".to_string(),
                ));
            }

            // Build reflection indices
            let mut indices = Vec::with_capacity(pad_left + time_len + pad_right);

            // Left reflection: [pad_left, pad_left-1, ..., 1]
            for i in (1..=pad_left).rev() {
                indices.push(i as u32);
            }

            // Original: [0, 1, ..., time_len-1]
            for i in 0..time_len {
                indices.push(i as u32);
            }

            // Right reflection: [time_len-2, time_len-3, ..., time_len-1-pad_right]
            for i in 0..pad_right {
                indices.push((time_len - 2 - i) as u32);
            }

            let idx_tensor =
                Tensor::from_vec(indices, (pad_left + time_len + pad_right,), input.device())?;
            input.index_select(&idx_tensor, time_dim)
        }
        PadMode::Replicate => {
            // Edge/replicate padding using constant padding with edge values
            // For simplicity, use constant padding with zero and then copy edge values
            let total_len = pad_left + time_len + pad_right;
            let mut indices = Vec::with_capacity(total_len);

            for _ in 0..pad_left {
                indices.push(0u32);
            }
            for i in 0..time_len {
                indices.push(i as u32);
            }
            for _ in 0..pad_right {
                indices.push((time_len - 1) as u32);
            }

            let idx_tensor = Tensor::from_vec(indices, (total_len,), input.device())?;
            input.index_select(&idx_tensor, time_dim)
        }
        PadMode::Constant(val) => {
            let mut left_shape: Vec<usize> = input.dims().to_vec();
            left_shape[time_dim] = pad_left;
            let mut right_shape: Vec<usize> = input.dims().to_vec();
            right_shape[time_dim] = pad_right;

            let left =
                Tensor::full(val as f32, left_shape, input.device())?.to_dtype(input.dtype())?;
            let right =
                Tensor::full(val as f32, right_shape, input.device())?.to_dtype(input.dtype())?;
            Tensor::cat(&[&left, input, &right], time_dim)
        }
    }
}

/// Create a Hann window
pub fn hann_window(length: usize, device: &candle_core::Device) -> Result<Tensor> {
    let mut values = vec![0f32; length];
    for i in 0..length {
        values[i] = 0.5 * (1.0 - (2.0 * std::f32::consts::PI * i as f32 / length as f32).cos());
    }
    Tensor::from_vec(values, (length,), device)
}

/// Convert Hz to Mel frequency
pub fn hz_to_mel(freq: f64, scale: MelScale) -> f64 {
    match scale {
        MelScale::Htk => 2595.0 * (1.0 + freq / 700.0).log10(),
        MelScale::Slaney => {
            let min_log_hz = 1000.0;
            let min_log_mel = 15.0;
            let log_step = (6.4f64).ln() / 27.0;
            if freq >= min_log_hz {
                min_log_mel + (freq / min_log_hz).ln() / log_step
            } else {
                3.0 * freq / 200.0
            }
        }
    }
}

/// Convert Mel frequency to Hz
pub fn mel_to_hz(mel: f64, scale: MelScale) -> f64 {
    match scale {
        MelScale::Htk => 700.0 * (10f64.powf(mel / 2595.0) - 1.0),
        MelScale::Slaney => {
            let min_log_hz = 1000.0;
            let min_log_mel = 15.0;
            let log_step = (6.4f64).ln() / 27.0;
            if mel >= min_log_mel {
                min_log_hz * (log_step * (mel - min_log_mel)).exp()
            } else {
                200.0 * mel / 3.0
            }
        }
    }
}

/// Create a Mel filterbank matrix of shape (n_mels, n_fft / 2 + 1)
pub fn get_mel_banks(
    n_mels: usize,
    n_fft: usize,
    sample_rate: usize,
    f_min: f64,
    f_max: f64,
    scale: MelScale,
    norm: MelNorm,
) -> Result<Tensor> {
    let n_bins = n_fft / 2 + 1;
    let mel_min = hz_to_mel(f_min, scale);
    let mel_max = hz_to_mel(f_max, scale);

    let mut mel_points = vec![0.0f64; n_mels + 2];
    for i in 0..n_mels + 2 {
        mel_points[i] = mel_min + i as f64 * (mel_max - mel_min) / (n_mels + 1) as f64;
    }

    let mut hz_points = vec![0.0f64; n_mels + 2];
    for i in 0..n_mels + 2 {
        hz_points[i] = mel_to_hz(mel_points[i], scale);
    }

    let mut fft_freqs = vec![0.0f64; n_bins];
    for i in 0..n_bins {
        fft_freqs[i] = i as f64 * sample_rate as f64 / n_fft as f64;
    }

    let mut filterbank = vec![0.0f32; n_mels * n_bins];

    for i in 0..n_mels {
        let left = hz_points[i];
        let center = hz_points[i + 1];
        let right = hz_points[i + 2];

        for j in 0..n_bins {
            let f = fft_freqs[j];
            if f > left && f < right {
                let val = if f <= center {
                    (f - left) / (center - left)
                } else {
                    (right - f) / (right - center)
                };

                // Area normalization (Slaney)
                let val = if norm == MelNorm::Slaney {
                    val * 2.0 / (right - left)
                } else {
                    val
                };

                filterbank[i * n_bins + j] = val as f32;
            }
        }
    }

    Tensor::from_vec(filterbank, (n_mels, n_bins), &candle_core::Device::Cpu)
}

/// MelSpectrogram transformation
pub fn mel_spectrogram(
    input: &Tensor,
    config: &MelSpectrogramConfig,
    window: Option<&Tensor>,
) -> Result<Tensor> {
    let n_fft = config.stft_config.n_fft;
    let f_max = config.f_max.unwrap_or(config.sample_rate as f64 / 2.0);

    // 1. STFT
    let spec = stft(input, &config.stft_config, window)?;

    // spec is (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2)
    // 2. Power Spectrogram (magnitude squared)
    let power = spec
        .narrow(spec.dims().len() - 1, 0, 1)?
        .sqr()?
        .add(&spec.narrow(spec.dims().len() - 1, 1, 1)?.sqr()?)?;
    let power = power.squeeze(spec.dims().len() - 1)?;

    // 3. Mel Filterbank
    let mel_banks = get_mel_banks(
        config.n_mels,
        n_fft,
        config.sample_rate,
        config.f_min,
        f_max,
        config.mel_scale,
        config.norm,
    )?
    .to_device(input.device())?
    .to_dtype(input.dtype())?;

    // Apply filterbank: (n_mels, n_bins) * (..., n_bins, n_frames)
    // We need to reorder power to (..., n_frames, n_bins) to use matmul or just use a custom dot product
    // Or just use matmul if we transpose.
    // power is (B, n_bins, n_frames)
    let power = power.transpose(power.dims().len() - 2, power.dims().len() - 1)?;
    // power is (B, n_frames, n_bins)

    let mel_spec = power.broadcast_matmul(&mel_banks.transpose(0, 1)?)?;
    // mel_spec is (B, n_frames, n_mels)

    mel_spec.transpose(mel_spec.dims().len() - 2, mel_spec.dims().len() - 1)
}

/// Short-time Fourier Transform (STFT)
pub fn stft(input: &Tensor, config: &StftConfig, window: Option<&Tensor>) -> Result<Tensor> {
    let device = input.device();
    let dtype = input.dtype();

    // 1. Padding
    let n_fft = config.n_fft;
    let hop_length = config.hop_length.unwrap_or(n_fft / 4);
    let win_length = config.win_length.unwrap_or(n_fft);

    let mut x = input.clone();
    if config.center {
        let pad = n_fft / 2;
        x = pad_1d(&x, pad, pad, config.pad_mode)?;
    }

    // 2. Framing & Windowing
    // x is (B, T) or (T,)
    let dims = x.dims();
    let (batch_size, time_len) = if dims.len() == 1 {
        (1, dims[0])
    } else if dims.len() == 2 {
        (dims[0], dims[1])
    } else {
        return Err(candle_core::Error::Msg(format!(
            "STFT input must be (B, T) or (T,), got {:?}",
            dims
        )));
    };

    let n_frames = (time_len - win_length) / hop_length + 1;

    // Move to CPU for FFT if on GPU
    let x_cpu = x.to_device(&candle_core::Device::Cpu)?;
    let x_vec = x_cpu.flatten_all()?.to_vec1::<f32>()?;

    let window_vec = if let Some(w) = window {
        w.to_device(&candle_core::Device::Cpu)?.to_vec1::<f32>()?
    } else {
        vec![1.0; win_length]
    };

    // 3. FFT Setup
    let mut planner = realfft::RealFftPlanner::<f32>::new();
    let r2c = planner.plan_fft_forward(n_fft);

    let n_bins = n_fft / 2 + 1;
    let mut output_vec = vec![0.0f32; batch_size * n_frames * n_bins * 2];

    for b in 0..batch_size {
        let batch_offset = b * time_len;
        let out_batch_offset = b * n_bins * n_frames * 2;
        for f in 0..n_frames {
            let start = batch_offset + f * hop_length;
            let mut frame = vec![0.0f32; n_fft];

            // Apply window and copy into frame (handling win_length < n_fft with zero padding)
            for i in 0..win_length {
                frame[i] = x_vec[start + i] * window_vec[i];
            }

            let mut spectrum = r2c.make_output_vec();
            r2c.process(&mut frame, &mut spectrum)
                .map_err(|e| candle_core::Error::Msg(format!("FFT error: {:?}", e)))?;

            for (i, c) in spectrum.iter().enumerate() {
                let out_idx = out_batch_offset + (i * n_frames + f) * 2;
                output_vec[out_idx] = c.re;
                output_vec[out_idx + 1] = c.im;
            }
        }
    }

    // 4. Result Formatting
    let shape = if batch_size == 1 && dims.len() == 1 {
        candle_core::Shape::from((n_bins, n_frames, 2usize))
    } else {
        candle_core::Shape::from((batch_size, n_bins, n_frames, 2usize))
    };

    let result = Tensor::from_vec(output_vec, shape, &candle_core::Device::Cpu)?;

    // Move back to original device and dtype
    result.to_device(device)?.to_dtype(dtype)
}

/// Inverse Short-time Fourier Transform (iSTFT)
pub fn istft(input: &Tensor, config: &StftConfig, window: Option<&Tensor>) -> Result<Tensor> {
    let device = input.device();
    let dtype = input.dtype();

    let n_fft = config.n_fft;
    let hop_length = config.hop_length.unwrap_or(n_fft / 4);
    let win_length = config.win_length.unwrap_or(n_fft);

    // input is (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2)
    let dims = input.dims();
    let (batch_size, n_bins, n_frames) = if dims.len() == 3 {
        (1, dims[0], dims[1])
    } else if dims.len() == 4 {
        (dims[0], dims[1], dims[2])
    } else {
        return Err(candle_core::Error::Msg(format!(
            "iSTFT input must be (B, n_bins, n_frames, 2) or (n_bins, n_frames, 2), got {:?}",
            dims
        )));
    };

    if n_bins != n_fft / 2 + 1 {
        return Err(candle_core::Error::Msg(format!(
            "Expected {} bins for n_fft={}, got {}",
            n_fft / 2 + 1,
            n_fft,
            n_bins
        )));
    }

    // Move to CPU
    let input_cpu = input.to_device(&candle_core::Device::Cpu)?;
    let input_vec = input_cpu.flatten_all()?.to_vec1::<f32>()?;

    let window_vec = if let Some(w) = window {
        w.to_device(&candle_core::Device::Cpu)?.to_vec1::<f32>()?
    } else {
        vec![1.0; win_length]
    };

    // 2. Inverse FFT Setup
    let mut planner = realfft::RealFftPlanner::<f32>::new();
    let c2r = planner.plan_fft_inverse(n_fft);

    let mut output_audio = Vec::with_capacity(batch_size * (n_frames * hop_length + n_fft));

    for b in 0..batch_size {
        let batch_offset = b * n_bins * n_frames * 2;
        let expected_len = n_frames * hop_length + n_fft;
        let mut reconstructed = vec![0.0f32; expected_len];
        let mut window_sum = vec![0.0f32; expected_len];

        for f in 0..n_frames {
            let start = f * hop_length;
            let mut spectrum = Vec::with_capacity(n_bins);
            for i in 0..n_bins {
                let idx = batch_offset + (i * n_frames + f) * 2;
                spectrum.push(num_complex::Complex::new(
                    input_vec[idx],
                    input_vec[idx + 1],
                ));
            }

            let mut frame = c2r.make_output_vec();
            c2r.process(&mut spectrum, &mut frame)
                .map_err(|e| candle_core::Error::Msg(format!("iFFT error: {:?}", e)))?;

            // Normalize iFFT (realfft doesn't normalize by default)
            let norm = 1.0 / n_fft as f32;
            for i in 0..win_length {
                reconstructed[start + i] += frame[i] * norm * window_vec[i];
                window_sum[start + i] += window_vec[i] * window_vec[i];
            }
        }

        // Apply OLA normalization (Over-Lap Add)
        for i in 0..reconstructed.len() {
            if window_sum[i] > 1e-10 {
                reconstructed[i] /= window_sum[i];
            }
        }

        output_audio.extend_from_slice(&reconstructed);
    }

    // 3. Finalize and Crop
    let mut result = Tensor::from_vec(
        output_audio,
        (batch_size, n_frames * hop_length + n_fft),
        &candle_core::Device::Cpu,
    )?;

    if config.center {
        let pad = n_fft / 2;
        let total_len = result.dim(1)?;
        result = result.narrow(1, pad, total_len - 2 * pad)?;
    }

    if batch_size == 1 && dims.len() == 3 {
        result = result.squeeze(0)?;
    }

    result.to_device(device)?.to_dtype(dtype)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hann_window() {
        let device = candle_core::Device::Cpu;
        let window = hann_window(4, &device).unwrap();
        let data: Vec<f32> = window.to_vec1().unwrap();
        // Hann window for n=4: [0, 0.5, 1, 0.5]
        assert!((data[0] - 0.0).abs() < 1e-5);
        assert!((data[1] - 0.5).abs() < 1e-5);
        assert!((data[2] - 1.0).abs() < 1e-5);
        assert!((data[3] - 0.5).abs() < 1e-5);
    }

    #[test]
    fn test_stft_istft_roundtrip() {
        let device = candle_core::Device::Cpu;
        let config = StftConfig {
            n_fft: 16,
            hop_length: Some(4),
            win_length: Some(16),
            center: true,
            pad_mode: PadMode::Reflect,
            ..Default::default()
        };

        // Create a simple signal: sum of sines
        let mut signal = vec![0.0f32; 64];
        for i in 0..64 {
            signal[i] = (2.0 * std::f32::consts::PI * 440.0 * i as f32 / 16000.0).sin();
        }
        let x = Tensor::from_vec(signal, (64,), &device).unwrap();
        let window = hann_window(config.win_length.unwrap(), &device).unwrap();

        let spec = stft(&x, &config, Some(&window)).unwrap();
        let x_hat = istft(&spec, &config, Some(&window)).unwrap();

        let x_vec = x.to_vec1::<f32>().unwrap();
        let x_hat_vec = x_hat.to_vec1::<f32>().unwrap();

        // Roundtrip should be reasonably close
        // Note: OLA with Hann window and hop=4 (n_fft/4) meets COLA
        for i in 4..60 {
            // Avoid edges due to OLA ramp-up/down if not perfectly handled by padding
            assert!(
                (x_vec[i] - x_hat_vec[i]).abs() < 1e-3,
                "At index {}: {} != {}",
                i,
                x_vec[i],
                x_hat_vec[i]
            );
        }
    }

    #[test]
    fn test_mel_scales() {
        // HTK parity
        let hz = 1000.0;
        let mel = hz_to_mel(hz, MelScale::Htk);
        assert!((mel - 1000.0).abs() < 0.1);
        let hz_back = mel_to_hz(mel, MelScale::Htk);
        assert!((hz_back - hz).abs() < 1e-5);

        // Slaney parity
        let hz_s = 2000.0;
        let mel_s = hz_to_mel(hz_s, MelScale::Slaney);
        // Slaney: 15 + log(2000/1000) / log_step
        assert!((mel_s - 25.0).abs() < 0.1);
        let hz_back_s = mel_to_hz(mel_s, MelScale::Slaney);
        assert!((hz_back_s - hz_s).abs() < 1e-5);
    }

    #[test]
    fn test_get_mel_banks() {
        let n_mels = 40;
        let n_fft = 1024;
        let sample_rate = 16000;
        let f_min = 0.0;
        let f_max = 8000.0;

        let banks = get_mel_banks(
            n_mels,
            n_fft,
            sample_rate,
            f_min,
            f_max,
            MelScale::Htk,
            MelNorm::None,
        )
        .unwrap();

        assert_eq!(banks.dims(), &[40, 513]);
    }
}
</file>

<file path="crates/pycandle-core/src/lib.rs">
//! PyCandle Core Library
//!
//! Core functionality for PyTorch ‚Üí Candle porting with parity verification.
//!
//! # Features
//! - `PyChecker` for layer-wise verification against golden tensors
//! - `py_check!` macro for embedded verification in generated code
//! - Layer implementations: BatchNorm, LSTM, activations
//! - Code generation from manifests

mod checker;
pub mod codegen;
pub mod gpt2;
pub mod layers;
pub mod samplers;
pub mod weights;

pub use checker::{ComparisonResult, LayerMeta, PyChecker, VerificationMode};
pub use layers::*;
pub use samplers::*;
pub use weights::{WeightExtractor, WeightMapper};

/// Verify tensor against golden record, panics on mismatch
#[macro_export]
macro_rules! py_check {
    ($checker:expr, $name:expr, $tensor:expr) => {
        if let Some(ref c) = $checker {
            c.verify($name, $tensor).expect("Parity Check Failed");
        }
    };
}
</file>

<file path="crates/pycandle/Cargo.toml">
[package]
name = "pycandle"
description = "CLI for PyTorch ‚Üí Candle porting with layer-wise parity verification"
version.workspace = true
edition.workspace = true
license.workspace = true

[[bin]]
name = "pycandle"
path = "src/main.rs"

[dependencies]
pycandle-core.workspace = true
candle-core.workspace = true
clap.workspace = true
colored.workspace = true
anyhow.workspace = true
serde.workspace = true
serde_json.workspace = true
walkdir.workspace = true
regex.workspace = true
ratatui = "0.29.0"
crossterm = "0.28.1"
toml = "0.8"
</file>

<file path="Cargo.toml">
[workspace]
members = ["crates/pycandle", "crates/pycandle-core", "crates/pycandle-audio"]
resolver = "2"

[workspace.package]
version = "0.1.0"
edition = "2024"
license = "MIT"

[workspace.dependencies]
# Candle ML framework
candle-core = "0.9.1"
candle-nn = "0.9.1"
candle-transformers = "0.9.1"

# Serialization
safetensors = "0.5.2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# CLI & utilities
clap = { version = "4.4", features = ["derive"] }
colored = "2.0"
thiserror = "1.0"
anyhow = "1.0"
walkdir = "2.4"
regex = "1.10"
realfft = "3.3"
num-complex = "0.4"
hf-hub = "0.4.0"
tokenizers = "0.20"
hound = "3.5"

# Internal crates
pycandle-core = { path = "crates/pycandle-core" }
pycandle-audio = { path = "crates/pycandle-audio" }
</file>

<file path="crates/pycandle-core/src/codegen/mod.rs">
//! Code generation from PyTorch manifests to Candle Rust code
//!
//! This module generates idiomatic Rust code from recorded PyTorch model manifests.

pub mod gpt2;

use crate::LayerMeta;
use serde::Serialize;
use std::collections::HashMap;

// ============================================================================
// Internal Types for Codegen
// ============================================================================

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ReturnType {
    Tensor,
    Tuple,
    Vec,
}

// ============================================================================
// JSON Output Structs for Analysis
// ============================================================================

#[derive(Serialize, Debug)]
pub struct AnalysisResult {
    pub supported: usize,
    pub unsupported: usize,
    pub total: usize,
    pub coverage_percent: f32,
    pub gaps: Vec<GapInfo>,
    pub layers: Vec<LayerInfo>,
}

#[derive(Serialize, Debug)]
pub struct GapInfo {
    pub module_type: String,
    pub count: usize,
    pub suggestion: String,
}

#[derive(Serialize, serde::Deserialize, Debug, Clone)]
pub struct LayerInfo {
    pub name: String,
    pub module_type: String,
    pub supported: bool,
    pub input_shapes: Vec<Vec<usize>>,
    pub output_shapes: Vec<Vec<usize>>,
}

#[derive(serde::Deserialize, Debug, Clone)]
pub struct GraphNode {
    pub name: String,
    pub op: String,
    pub target: String,
    pub args: Vec<serde_json::Value>,
    pub module_type: Option<String>,
}

#[derive(Serialize, serde::Deserialize, Debug, Clone, Default)]
pub struct SymbolicConfig {
    pub dims: HashMap<String, usize>,
}

/// Code generator that converts manifests to Rust code
pub struct Codegen {
    pub manifest: HashMap<String, LayerMeta>,
    pub hints: Option<HashMap<String, usize>>,
    pub graph_nodes: Vec<GraphNode>,
    pub stateful: bool,
    config: SymbolicConfig,
}

impl Codegen {
    pub fn new(
        manifest: HashMap<String, LayerMeta>,
        hints: Option<HashMap<String, usize>>,
    ) -> Self {
        let mut slf = Self {
            manifest,
            hints: hints.clone(),
            graph_nodes: Vec::new(),
            stateful: false,
            config: SymbolicConfig::default(),
        };
        slf.config = slf.extract_symbolic_config();
        slf
    }

    pub fn with_graph(mut self, graph_nodes: Vec<GraphNode>) -> Self {
        self.graph_nodes = graph_nodes;
        self
    }

    pub fn with_stateful(mut self, stateful: bool) -> Self {
        self.stateful = stateful;
        self
    }

    pub fn extract_symbolic_config(&self) -> SymbolicConfig {
        let mut dims = self.hints.clone().unwrap_or_default();

        for meta in self.manifest.values() {
            // GPT2 specific extraction
            if let Some(v) = meta.config.get("vocab_size").and_then(|v| v.as_u64()) {
                dims.entry("vocab_size".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_embd").and_then(|v| v.as_u64()) {
                dims.entry("hidden_dim".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_head").and_then(|v| v.as_u64()) {
                dims.entry("n_head".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_layer").and_then(|v| v.as_u64()) {
                dims.entry("n_layers".to_string()).or_insert(v as usize);
            }
            if let Some(v) = meta.config.get("n_positions").and_then(|v| v.as_u64()) {
                dims.entry("context_length".to_string())
                    .or_insert(v as usize);
            }

            // Generic extraction
            match meta.module_type.as_str() {
                "Embedding" => {
                    if let Some(n) = meta.config.get("num_embeddings").and_then(|v| v.as_u64()) {
                        dims.entry("vocab_size".to_string()).or_insert(n as usize);
                    }
                    if let Some(d) = meta.config.get("embedding_dim").and_then(|v| v.as_u64()) {
                        dims.entry("hidden_dim".to_string()).or_insert(d as usize);
                    }
                }
                "Linear" | "LoRACompatibleLinear" => {
                    // If out_features is high (like 30000+), it's likely a vocab_size (lm_head)
                    if let Some(out_f) = meta.config.get("out_features").and_then(|v| v.as_u64()) {
                        if out_f > 30000 {
                            dims.entry("vocab_size".to_string())
                                .or_insert(out_f as usize);
                        }
                    }
                }
                _ => {}
            }
        }

        SymbolicConfig { dims }
    }

    fn render_dim(&self, value: usize, preferred_name: &str) -> String {
        // Try preferred name first
        if !preferred_name.is_empty() {
            if let Some(&v) = self.config.dims.get(preferred_name) {
                if v == value {
                    return format!("config.{}", preferred_name);
                }
            }
        }

        // Try exact value match in any dim
        for (name, &v) in &self.config.dims {
            if v == value {
                return format!("config.{}", name);
            }
        }

        value.to_string()
    }

    fn sanitize_name(&self, name: &str) -> String {
        // Handle ONNX paths like /layers/0/Gather_1_output -> gather_1
        if name.contains('/') {
            let clean = name.trim_start_matches('/');
            let parts: Vec<&str> = clean.split('/').collect();
            let last = parts.last().unwrap_or(&name);
            let mut sanitized = last.to_lowercase().replace("_output", "");

            if sanitized.starts_with("node_") {
                sanitized = sanitized.replace("node_", "x_");
            }

            // Ensure valid identifier
            if sanitized
                .chars()
                .next()
                .map(|c| !c.is_alphabetic())
                .unwrap_or(true)
            {
                return format!("x_{}", sanitized);
            }
            return sanitized;
        }

        // Standard PyTorch names: encoder.layers.0 -> encoder_layers_0
        let mut sanitized = name.replace(".", "_").replace("-", "_");
        if sanitized.starts_with("node_") {
            sanitized = sanitized.replace("node_", "x_");
        }

        // Ensure valid identifier
        if sanitized
            .chars()
            .next()
            .map(|c| !c.is_alphabetic() && c != '_')
            .unwrap_or(true)
        {
            return format!("x_{}", sanitized);
        }

        sanitized
    }

    /// Analyze the manifest and return a structured result for JSON output
    pub fn analyze(&self) -> AnalysisResult {
        let mut supported = 0;
        let mut unsupported = 0;
        let mut gap_counts: HashMap<String, usize> = HashMap::new();
        let mut layers = Vec::new();

        for (name, meta) in &self.manifest {
            if !meta.is_leaf {
                continue;
            }

            let is_supported = self.is_supported(&meta.module_type);
            if is_supported {
                supported += 1;
            } else {
                unsupported += 1;
                *gap_counts.entry(meta.module_type.clone()).or_default() += 1;
            }

            layers.push(LayerInfo {
                name: name.clone(),
                module_type: meta.module_type.clone(),
                supported: is_supported,
                input_shapes: meta.input_shapes.clone(),
                output_shapes: meta.output_shapes.clone(),
            });
        }

        // Sort layers by name for consistent output
        layers.sort_by(|a, b| a.name.cmp(&b.name));

        let gaps: Vec<GapInfo> = gap_counts
            .into_iter()
            .map(|(t, c)| GapInfo {
                suggestion: self.get_suggestion(&t),
                module_type: t,
                count: c,
            })
            .collect();

        let total = supported + unsupported;
        let coverage_percent = if total > 0 {
            (supported as f32 / total as f32) * 100.0
        } else {
            100.0
        };

        AnalysisResult {
            supported,
            unsupported,
            total,
            coverage_percent,
            gaps,
            layers,
        }
    }

    /// Check if a module type is supported by the codegen
    pub fn is_supported(&self, module_type: &str) -> bool {
        // Check GPT2 helper first
        if gpt2::map_type(module_type).is_some() {
            return true;
        }

        matches!(
            module_type,
            "Linear"
                | "Conv1d"
                | "LayerNorm"
                | "Embedding"
                | "ReLU"
                | "GELU"
                | "Sigmoid"
                | "Tanh"
                | "ELU"
                | "LeakyReLU"
                | "Snake"
                | "BatchNorm1d"
                | "BatchNorm2d"
                | "LSTM"
                | "Mish"
                | "SiLU"
                | "CausalConv1d"
                | "Transpose"
                | "Conv1D"
                | "Dropout"
                | "NewGELUActivation"
                | "LoRACompatibleLinear"
                | "SinusoidalPosEmb"
        )
    }

    /// Get implementation suggestion for an unsupported module type
    pub fn get_suggestion(&self, module_type: &str) -> String {
        match module_type {
            "LSTM" => "Use /add-lstm workflow".to_string(),
            "BatchNorm1d" | "BatchNorm2d" => "Use /add-batchnorm workflow".to_string(),
            "Snake" | "ELU" => "Use /add-activations workflow".to_string(),
            _ => format!("Implement {} manually", module_type),
        }
    }

    pub fn generate_model_rs(&self, model_name: &str) -> String {
        let mut code = String::new();
        code.push_str("use candle_core::{Result, Tensor, IndexOp, Shape};\n");
        code.push_str("use candle_nn::{Module, VarBuilder};\n");
        code.push_str("use pycandle_core::{PyChecker, py_check, VerificationMode, layers::*};\n\n");

        if self.stateful {
            code.push_str(
                r#"#[derive(Debug, Clone)]
pub struct KVCache {
    pub k: Tensor,
    pub v: Tensor,
}

"#,
            );
        }

        code.push_str(&self.generate_config_struct());
        code.push_str("\n");
        code.push_str(&self.generate_struct(model_name));
        code.push_str("\n");
        code.push_str(&self.generate_impl(model_name));

        code
    }

    fn generate_config_struct(&self) -> String {
        let mut lines = vec!["pub struct Config {".to_string()];
        let mut dims: Vec<_> = self.config.dims.iter().collect();
        dims.sort_by_key(|(k, _)| *k);

        for (name, value) in dims {
            lines.push(format!("    pub {}: usize, // {}", name, value));
        }
        lines.push("}".to_string());
        lines.join("\n")
    }

    fn has_gpt2_types(&self) -> bool {
        self.manifest
            .values()
            .any(|meta| gpt2::is_gpt2_type(&meta.module_type))
    }

    pub fn generate_struct(&self, model_name: &str) -> String {
        let mut code = format!("pub struct {} {{\n", model_name);

        let mut sorted_keys: Vec<_> = self.manifest.keys().collect();
        sorted_keys.sort();

        for name in sorted_keys {
            let meta = &self.manifest[name];
            if meta.is_leaf {
                let rust_type = self.map_type(&meta.module_type);
                code.push_str(&format!(
                    "    pub {}: {},\n",
                    self.sanitize_name(name),
                    rust_type
                ));
            }
        }

        code.push_str("    pub checker: Option<PyChecker>,\n");
        if self.stateful {
            code.push_str("    pub cache: std::cell::RefCell<Vec<Option<KVCache>>>,\n");
        }
        code.push_str("}\n");
        code
    }

    fn map_type(&self, py_type: &str) -> String {
        // Check GPT2 helper first
        if let Some(t) = gpt2::map_type(py_type) {
            return t;
        }

        // Core types
        match py_type {
            "Linear" => "candle_nn::Linear".to_string(),
            "Conv1d" => "candle_nn::Conv1d".to_string(),
            "LayerNorm" => "candle_nn::LayerNorm".to_string(),
            "Embedding" => "candle_nn::Embedding".to_string(),
            // Activations
            "ReLU" => "ReLU".to_string(),
            "GELU" => "GELU".to_string(),
            "Sigmoid" => "Sigmoid".to_string(),
            "Tanh" => "Tanh".to_string(),
            "ELU" => "ELU".to_string(),
            "LeakyReLU" => "LeakyReLU".to_string(),
            "Snake" => "Snake".to_string(),
            "BatchNorm1d" => "BatchNorm1d".to_string(),
            "BatchNorm2d" => "BatchNorm2d".to_string(),
            "LSTM" => "LSTM".to_string(),
            "Mish" => "Mish".to_string(),
            "SiLU" => "SiLU".to_string(),
            "CausalConv1d" => "CausalConv1d".to_string(),
            "Transpose" => "Transpose".to_string(),
            "Conv1D" => "candle_nn::Linear".to_string(),
            "Dropout" => "Dropout".to_string(),
            "NewGELUActivation" => "candle_nn::Activation".to_string(),
            "LoRACompatibleLinear" => "candle_nn::Linear".to_string(),
            "SinusoidalPosEmb" => "SinusoidalPosEmb".to_string(),
            _ => format!("() /* TODO: {} */", py_type),
        }
    }

    pub fn generate_impl(&self, model_name: &str) -> String {
        let mut code = format!("impl {} {{\n", model_name);

        // Load method
        code.push_str("    #[allow(unused_variables)]\n");
        code.push_str(&format!(
            "    pub fn load(config: Config, vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {{\n"
        ));

        if self.has_gpt2_types() {
            code.push_str(
                r#"        let gpt2_cfg = pycandle_core::gpt2::Config {
            vocab_size: config.vocab_size,
            context_length: config.context_length,
            emb_dim: config.hidden_dim,
            n_heads: config.n_head,
            n_layers: config.n_layers,
            ..Default::default()
        };
"#,
            );
        }

        let mut sorted_keys: Vec<_> = self.manifest.keys().collect();
        sorted_keys.sort();

        for name in &sorted_keys {
            let meta = self.manifest.get(*name).unwrap();
            if meta.is_leaf {
                let field = self.sanitize_name(name);
                let init = self.generate_init(name, meta);
                code.push_str(&format!("        let {} = {};\n", field, init));
            }
        }

        let mut fields = vec![];
        for name in &sorted_keys {
            if self.manifest.get(*name).unwrap().is_leaf {
                fields.push(self.sanitize_name(name));
            }
        }
        fields.push("checker".to_string());

        code.push_str(&format!("        Ok(Self {{ {}", fields.join(", ")));
        if self.stateful {
            code.push_str(", cache: std::cell::RefCell::new(Vec::new())");
        }
        code.push_str(" })\n");
        code.push_str("    }\n\n");

        // Forward methods
        if self.graph_nodes.is_empty() {
            // Sequential fallback
            code.push_str("    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {\n");
            if self.stateful {
                code.push_str(
                    "        self.forward_with_cache(xs, &mut self.cache.borrow_mut())\n",
                );
                code.push_str("    }\n\n");
                code.push_str("    pub fn forward_with_cache(&self, xs: &Tensor, cache: &mut Vec<Option<KVCache>>) -> Result<Tensor> {\n");
            }
            code.push_str("        let mut x = xs.clone();\n");

            for name in &sorted_keys {
                let meta = self.manifest.get(*name).unwrap();
                if !meta.is_leaf {
                    continue;
                }
                let clean_name = self.sanitize_name(name);
                let forward_call = if self.map_type(&meta.module_type) == "LSTM" {
                    format!("self.{}.forward(&x)?.0", clean_name)
                } else {
                    format!("self.{}.forward(&x)?", clean_name)
                };
                code.push_str(&format!("\n        // Layer: {}\n", name));
                code.push_str(&format!("        x = {};\n", forward_call));
                code.push_str(&format!(
                    "        py_check!(self.checker, \"{}\", &x);\n",
                    name
                ));
            }

            code.push_str("\n        Ok(x)\n");
            code.push_str("    }\n");
        } else {
            // DAG / torch.fx based forward
            code.push_str(&self.generate_forward_dag(&self.graph_nodes));
        }
        code.push_str("}\n");

        code
    }

    fn generate_forward_dag(&self, nodes: &[GraphNode]) -> String {
        let mut placeholders = Vec::new();
        for node in nodes {
            if node.op == "placeholder" {
                placeholders.push(node.name.clone());
            }
        }

        let mut code = String::new();
        let ret_type = self.get_forward_return_type();

        let inputs = if placeholders.len() <= 1 {
            "xs: &Tensor".to_string()
        } else {
            placeholders
                .iter()
                .enumerate()
                .map(|(i, _)| format!("xs{}: &Tensor", i))
                .collect::<Vec<_>>()
                .join(", ")
        };

        code.push_str(&format!(
            "    pub fn forward(&self, {}) -> {} {{\n",
            inputs, ret_type
        ));

        // Map python placeholder names to Rust input variables
        let mut var_map = HashMap::new();
        let mut node_types = HashMap::new();

        let mut placeholder_idx = 0;
        for node in nodes {
            match node.op.as_str() {
                "placeholder" => {
                    let var_name = if placeholders.len() <= 1 {
                        "xs".to_string()
                    } else {
                        format!("xs{}", placeholder_idx)
                    };
                    var_map.insert(node.name.clone(), var_name);
                    node_types.insert(node.name.clone(), ReturnType::Tensor);
                    placeholder_idx += 1;
                }
                "call_module" => {
                    let clean_name = self.sanitize_name(&node.target);
                    let var_name = self.sanitize_name(&node.name);
                    let input_var = if let Some(arg0) = node.args.get(0) {
                        match arg0 {
                            serde_json::Value::String(s) => {
                                var_map.get(s).cloned().unwrap_or(s.clone())
                            }
                            _ => {
                                let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                                first_p.cloned().unwrap_or("xs".to_string())
                            }
                        }
                    } else {
                        let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                        first_p.cloned().unwrap_or("xs".to_string())
                    };

                    let module_type = node.module_type.clone().unwrap_or_default();
                    let is_lstm = self.map_type(&module_type) == "LSTM";

                    let (forward_call, return_type) = if is_lstm {
                        // LSTM returns (output, (h, c))
                        (
                            format!("self.{}.forward(&{})?", clean_name, input_var),
                            ReturnType::Tuple,
                        )
                    } else {
                        let meta = self.manifest.get(&node.target);
                        let multi = meta.map(|m| m.output_shapes.len() > 1).unwrap_or(false);
                        if multi {
                            (
                                format!("self.{}.forward(&{})?", clean_name, input_var),
                                ReturnType::Tuple,
                            )
                        } else {
                            (
                                format!("self.{}.forward(&{})?", clean_name, input_var),
                                ReturnType::Tensor,
                            )
                        }
                    };

                    code.push_str(&format!("        let {} = {};\n", var_name, forward_call));

                    // Only do parity check on single Tensors for now to avoid tuple mismatch in py_check!
                    if return_type == ReturnType::Tensor {
                        code.push_str(&format!(
                            "        py_check!(self.checker, \"{}\", &{});\n",
                            node.target, var_name
                        ));
                    }

                    var_map.insert(node.name.clone(), var_name.clone());
                    node_types.insert(node.name.clone(), return_type);
                }
                "call_function" => {
                    let var_name = self.sanitize_name(&node.name);
                    let mut resolved_args = Vec::new();
                    for arg in &node.args {
                        // Strictly resolve args, handling literals
                        resolved_args.push(self.resolve_fx_arg(arg, &var_map));
                    }

                    // Pass node_types to map_fx_op for type-aware generation
                    let (expr, return_type) = self.map_fx_op(
                        &node.target,
                        &resolved_args,
                        &var_map,
                        &node_types,
                        &node.args,
                    );
                    code.push_str(&format!("        let {} = {};\n", var_name, expr));
                    var_map.insert(node.name.clone(), var_name);
                    node_types.insert(node.name.clone(), return_type);
                }
                "call_method" => {
                    let var_name = self.sanitize_name(&node.name);
                    let mut resolved_args = Vec::new();
                    for arg in &node.args {
                        resolved_args.push(self.resolve_fx_arg(arg, &var_map));
                    }

                    if !resolved_args.is_empty() {
                        let self_var_name = match &node.args[0] {
                            serde_json::Value::String(s) => s.clone(),
                            _ => "".to_string(),
                        };
                        let self_var = &resolved_args[0];
                        let method_args = &resolved_args[1..];
                        let (expr, return_type) = self.map_fx_method(
                            &node.target,
                            self_var,
                            &self_var_name,
                            method_args,
                            &node_types,
                        );
                        code.push_str(&format!("        let {} = {};\n", var_name, expr));
                        var_map.insert(node.name.clone(), var_name);
                        node_types.insert(node.name.clone(), return_type);
                    }
                }
                "output" => {
                    let out_var = if let Some(arg0) = node.args.get(0) {
                        match arg0 {
                            serde_json::Value::String(s) => {
                                var_map.get(s).cloned().unwrap_or(s.clone())
                            }
                            serde_json::Value::Array(arr) => {
                                let items: Vec<String> = arr
                                    .iter()
                                    .map(|v| self.resolve_fx_arg(v, &var_map))
                                    .collect();
                                format!("({})", items.join(", "))
                            }
                            _ => {
                                let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                                first_p.cloned().unwrap_or("xs".to_string())
                            }
                        }
                    } else {
                        let first_p = placeholders.get(0).and_then(|p| var_map.get(p));
                        first_p.cloned().unwrap_or("xs".to_string())
                    };
                    code.push_str(&format!("        Ok({})\n", out_var));
                }
                _ => {}
            }
        }

        code.push_str("    }\n");
        code
    }

    fn resolve_fx_arg(&self, arg: &serde_json::Value, var_map: &HashMap<String, String>) -> String {
        match arg {
            serde_json::Value::String(s) => var_map.get(s).cloned().unwrap_or(s.clone()),
            serde_json::Value::Number(n) => n.to_string(), // Literal numbers
            serde_json::Value::Bool(b) => b.to_string(),   // Literal bools
            serde_json::Value::Array(arr) => {
                let items: Vec<String> = arr
                    .iter()
                    .map(|v| self.resolve_fx_arg(v, var_map))
                    .collect();
                format!("&[{}]", items.join(", "))
            }
            _ => arg.to_string(),
        }
    }

    fn map_fx_op(
        &self,
        target: &str,
        args: &[String],
        var_map: &HashMap<String, String>,
        node_types: &HashMap<String, ReturnType>,
        raw_args: &[serde_json::Value], // Needed to check for literals
    ) -> (String, ReturnType) {
        let target_lower = target.to_lowercase();

        // Common binary ops
        if target_lower.contains("add") && args.len() >= 2 {
            return (
                format!("(&{} + &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if target_lower.contains("sub") && args.len() >= 2 {
            return (
                format!("(&{} - &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if target_lower.contains("mul") && args.len() >= 2 {
            return (
                format!("(&{} * &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }
        if (target_lower.contains("div") || target_lower.contains("truediv")) && args.len() >= 2 {
            return (
                format!("(&{} / &{})?", args[0], args[1]),
                ReturnType::Tensor,
            );
        }

        // Functional ops
        match target {
            "torch.cat" | "cat" => {
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                let mut tensors = args[0].clone();
                if tensors.starts_with("&[") {
                    // Convert &[x, y] to &[&x, &y] for Candle's cat
                    let content = &tensors[2..tensors.len() - 1];
                    let items: Vec<String> =
                        content.split(", ").map(|s| format!("&{}", s)).collect();
                    tensors = format!("&[{}]", items.join(", "));
                }
                (
                    format!("Tensor::cat({}, {})?", tensors, dim),
                    ReturnType::Tensor,
                )
            }
            "torch.chunk" | "chunk" => {
                let chunks = args.get(1).map(|s| s.as_str()).unwrap_or("2");
                let dim = args.get(2).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.chunk({}, {})?", args[0], chunks, dim),
                    ReturnType::Vec,
                )
            }
            "torch.split" | "split" => {
                let split_size = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                let dim = args.get(2).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.split({}, {})?", args[0], split_size, dim),
                    ReturnType::Vec,
                )
            }
            "torch.relu" | "relu" => (format!("{}.relu()?", args[0]), ReturnType::Tensor),
            "torch.sigmoid" | "sigmoid" => (
                format!("candle_nn::ops::sigmoid(&{})?", args[0]),
                ReturnType::Tensor,
            ),
            "torch.tanh" | "tanh" => (format!("{}.tanh()?", args[0]), ReturnType::Tensor),
            "torch.squeeze" | "squeeze" => {
                if args.len() > 1 {
                    (
                        format!("{}.squeeze({})?", args[0], args[1]),
                        ReturnType::Tensor,
                    )
                } else {
                    (format!("{}.squeeze(0)?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.unsqueeze" | "unsqueeze" => (
                format!("{}.unsqueeze({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.pow" | "pow" => (
                format!("{}.powf({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.sqrt" | "sqrt" => (format!("{}.sqrt()?", args[0]), ReturnType::Tensor),
            "torch.exp" | "exp" => (format!("{}.exp()?", args[0]), ReturnType::Tensor),
            "torch.log" | "log" => (format!("{}.log()?", args[0]), ReturnType::Tensor),
            "torch.abs" | "abs" => (format!("{}.abs()?", args[0]), ReturnType::Tensor),
            "torch.sum" | "sum" => {
                if args.len() > 1 {
                    (format!("{}.sum({})?", args[0], args[1]), ReturnType::Tensor)
                } else {
                    (format!("{}.sum_all()?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.mean" | "mean" => {
                if args.len() > 1 {
                    (
                        format!("{}.mean({})?", args[0], args[1]),
                        ReturnType::Tensor,
                    )
                } else {
                    (format!("{}.mean_all()?", args[0]), ReturnType::Tensor)
                }
            }
            "torch.transpose" => (
                format!("{}.transpose({}, {})?", args[0], args[1], args[2]),
                ReturnType::Tensor,
            ),
            "torch.reshape" => (
                format!("{}.reshape({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            "torch.permute" => (
                format!("{}.permute({})?", args[0], args[1]),
                ReturnType::Tensor,
            ),
            // FEATURE: Comparisons
            "torch.lt" | "lt" => (format!("{}.lt(&{})?", args[0], args[1]), ReturnType::Tensor),
            "torch.gt" | "gt" => (format!("{}.gt(&{})?", args[0], args[1]), ReturnType::Tensor),
            // FEATURE: SDPA
            "torch.nn.functional.scaled_dot_product_attention" | "scaled_dot_product_attention" => {
                (
                    format!(
                        "// TODO: SDPA(q={}, k={}, v={}) - Implement custom kernel or simplified attention\ntodo!(\"SDPA\")",
                        args[0], args[1], args[2]
                    ),
                    ReturnType::Tensor,
                )
            }
            // FEATURE: Functional Tensor Creation
            "torch.ones" | "ones" => {
                let shape = args[0].clone();
                // Infer device from a previous tensor if available, else default?
                // We will try to find the first available tensor variable in scope to steal its device/dtype
                // or default to arbitrary choice if none (which might fail compile, but better than nothing)
                let device_hint = var_map
                    .values()
                    .next()
                    .map(|v| format!("{}.device()", v))
                    .unwrap_or("Device::Cpu".to_string());
                let dtype_hint = var_map
                    .values()
                    .next()
                    .map(|v| format!("{}.dtype()", v))
                    .unwrap_or("DType::F32".to_string());

                // If the graph has inputs, use the first one (usually 'xs')
                let (dev, dt) = if !var_map.is_empty() {
                    // Try to find a variable that is a Tensor
                    let tensor_var = var_map
                        .iter()
                        .find(|(k, _)| node_types.get(*k) == Some(&ReturnType::Tensor));
                    if let Some((_, v)) = tensor_var {
                        (format!("{}.device()", v), format!("{}.dtype()", v))
                    } else {
                        (device_hint, dtype_hint)
                    }
                } else {
                    ("Device::Cpu".to_string(), "DType::F32".to_string())
                };

                (
                    format!("Tensor::ones({}, {}, {})?", shape, dt, dev),
                    ReturnType::Tensor,
                )
            }
            "torch.zeros" | "zeros" => {
                let shape = args[0].clone();
                // Heuristic: Use first variable found for device/dtype
                let tensor_var = var_map
                    .iter()
                    .find(|(k, _)| node_types.get(*k) == Some(&ReturnType::Tensor));
                let (dev, dt) = if let Some((_, v)) = tensor_var {
                    (format!("{}.device()", v), format!("{}.dtype()", v))
                } else {
                    ("Device::Cpu".to_string(), "DType::F32".to_string())
                };
                (
                    format!("Tensor::zeros({}, {}, {})?", shape, dt, dev),
                    ReturnType::Tensor,
                )
            }
            "torch.arange" | "arange" => {
                // arange(start, end, step) or arange(end)
                // We need to check arg count
                let tensor_var = var_map
                    .iter()
                    .find(|(k, _)| node_types.get(*k) == Some(&ReturnType::Tensor));
                let dev = if let Some((_, v)) = tensor_var {
                    format!("{}.device()", v)
                } else {
                    "Device::Cpu".to_string()
                };

                if args.len() == 1 {
                    (
                        format!("Tensor::arange(0u32, {}, {})?", args[0], dev),
                        ReturnType::Tensor,
                    )
                } else if args.len() >= 2 {
                    (
                        format!("Tensor::arange({}, {}, {})?", args[0], args[1], dev),
                        ReturnType::Tensor,
                    )
                } else {
                    (
                        "Tensor::arange(0u32, 1u32, Device::Cpu)?".to_string(),
                        ReturnType::Tensor,
                    )
                }
            }
            "torch.full" => {
                let shape = args[0].clone();
                let fill_value = args[1].clone();
                let tensor_var = var_map
                    .iter()
                    .find(|(k, _)| node_types.get(*k) == Some(&ReturnType::Tensor));
                let (dev, dt) = if let Some((_, v)) = tensor_var {
                    (format!("{}.device()", v), format!("{}.dtype()", v))
                } else {
                    ("Device::Cpu".to_string(), "DType::F32".to_string())
                };
                (
                    format!(
                        "Tensor::full({}, {}, {})?.to_dtype({})?",
                        fill_value, shape, dev, dt
                    ),
                    ReturnType::Tensor,
                )
            }

            "operator.getitem" => {
                let idx = &args[1];

                // Find the source variable name to check its type
                let src_name = var_map
                    .iter()
                    .find(|(_, v)| *v == &args[0])
                    .map(|(k, _)| k.as_str())
                    .unwrap_or(&args[0]);

                let src_type = node_types
                    .get(src_name)
                    .cloned()
                    .unwrap_or(ReturnType::Tensor);

                match src_type {
                    ReturnType::Tuple => {
                        // Tuple indexing: x.0, x.1
                        if let Ok(i) = idx.parse::<usize>() {
                            (format!("{}.{}", args[0], i), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                    ReturnType::Vec => {
                        // Vec indexing: x[0]
                        if let Ok(i) = idx.parse::<usize>() {
                            (format!("{}[{}].clone()", args[0], i), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                    ReturnType::Tensor => {
                        if idx.contains("slice(") || idx == "None" || idx.starts_with("&[") {
                            // Map to .i() for indexing/slicing
                            let mut cleaned = idx.clone();
                            if cleaned.starts_with("&[") {
                                cleaned = cleaned[2..cleaned.len() - 1].to_string();
                            }

                            let items: Vec<String> = cleaned
                                .split(",")
                                .enumerate()
                                .map(|(i, s)| self.parse_slice_item(s.trim(), &args[0], i))
                                .collect();

                            let final_idx = if items.len() == 1 {
                                items[0].clone()
                            } else {
                                format!("({})", items.join(", "))
                            };

                            (format!("{}.i({})?", args[0], final_idx), ReturnType::Tensor)
                        } else {
                            (format!("{}.get({})?", args[0], args[1]), ReturnType::Tensor)
                        }
                    }
                }
            }
            _ => {
                if target_lower.contains("add") && args.len() >= 2 {
                    return (
                        format!("(&{} + &{})?", args[0], args[1]),
                        ReturnType::Tensor,
                    );
                }
                // FEATURE: In-Place Operations
                if target_lower.contains("add_") && args.len() >= 2 {
                    return (
                        format!("(&{} + &{})?", args[0], args[1]),
                        ReturnType::Tensor,
                    );
                }

                // FEATURE: Non-Tensor Literals handling
                // We need to wrap numerical args in Tensor if they are being used in a tensor operation
                // This is a bit tricky without full type inference, but we can try to wrap obvious ones
                // or just rely on Candle's impl which often accepts f64 for some scalar ops.
                // However, things like `conv1d` expect tensors.
                // For now, let's just make sure we print unknown ops nicely.
                (
                    format!("todo!(/* function: {} */)", target),
                    ReturnType::Tensor,
                )
            }
        }
    }

    fn parse_slice_item(&self, item: &str, tensor_name: &str, dim_idx: usize) -> String {
        if item == "None" {
            return "..".to_string();
        }

        // Handle single negative index
        if let Ok(val) = item.parse::<isize>() {
            if val < 0 {
                return format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs());
            }
            return item.to_string();
        }

        if !item.contains("slice(") {
            return item.to_string();
        }

        // Parse slice(start, stop, step)
        let content = item
            .strip_prefix("slice(")
            .and_then(|s| s.strip_suffix(")"))
            .unwrap_or(item);
        let parts: Vec<&str> = content.split(",").map(|s| s.trim()).collect();

        let start_str = parts.get(0).copied().unwrap_or("None");
        let stop_str = parts.get(1).copied().unwrap_or("None");

        let start = if let Ok(val) = start_str.parse::<isize>() {
            if val < 0 {
                format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs())
            } else {
                start_str.to_string()
            }
        } else {
            start_str.to_string()
        };

        let stop = if let Ok(val) = stop_str.parse::<isize>() {
            if val < 0 {
                format!("{}.dim({})? - {}", tensor_name, dim_idx, val.abs())
            } else {
                stop_str.to_string()
            }
        } else {
            stop_str.to_string()
        };

        match (start.as_str(), stop.as_str()) {
            ("None", "None") => "..".to_string(),
            ("None", stop) => format!("..{}", stop),
            (start, "None") => format!("{}..", start),
            (start, stop) => format!("{}..{}", start, stop),
        }
    }

    fn map_fx_method(
        &self,
        method: &str,
        self_var: &str,
        self_var_name: &str,
        args: &[String],
        node_types: &HashMap<String, ReturnType>,
    ) -> (String, ReturnType) {
        let _src_type = node_types
            .get(self_var_name)
            .cloned()
            .unwrap_or(ReturnType::Tensor);

        match method {
            "view" | "reshape" => {
                if args.len() == 1 && args[0].starts_with("&[") {
                    (
                        format!("{}.reshape({})?", self_var, args[0]),
                        ReturnType::Tensor,
                    )
                } else {
                    (
                        format!("{}.reshape(vec![{}])?", self_var, args.join(", ")),
                        ReturnType::Tensor,
                    )
                }
            }
            "flatten" => (format!("{}.flatten_all()?", self_var), ReturnType::Tensor),
            "transpose" => (
                format!("{}.transpose({}, {})?", self_var, args[0], args[1]),
                ReturnType::Tensor,
            ),
            "permute" => (
                format!("{}.permute({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "unsqueeze" => (
                format!("{}.unsqueeze({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "squeeze" => (
                format!("{}.squeeze({})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "chunk" => {
                let chunks = args.get(0).map(|s| s.as_str()).unwrap_or("2");
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("0");
                (
                    format!("{}.chunk({}, {})?", self_var, chunks, dim),
                    ReturnType::Vec,
                )
            }
            "split" => {
                let split_size = args.get(0).map(|s| s.as_str()).unwrap_or("1");
                let dim = args.get(1).map(|s| s.as_str()).unwrap_or("1");
                (
                    format!("{}.split({}, {})?", self_var, split_size, dim),
                    ReturnType::Vec,
                )
            }
            "t" => (format!("{}.t()?", self_var), ReturnType::Tensor),
            "contiguous" => (format!("{}.contiguous()?", self_var), ReturnType::Tensor),
            "size" => {
                if args.is_empty() {
                    (format!("{}.dims().to_vec()", self_var), ReturnType::Tensor)
                } else {
                    (
                        format!("{}.dim({})?", self_var, args[0]),
                        ReturnType::Tensor,
                    )
                }
            }
            // FEATURE: In-Place Operations (Method calls)
            "add_" => (
                format!("(&{} + &{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "sub_" => (
                format!("(&{} - &{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "mul_" => (
                format!("(&{} * &{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "div_" => (
                format!("(&{} / &{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            // Comparisons
            "lt" => (
                format!("{}.lt(&{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            "gt" => (
                format!("{}.gt(&{})?", self_var, args[0]),
                ReturnType::Tensor,
            ),
            // Ops
            "scaled_dot_product_attention" => (
                // Naive mapping to a potential ops::sdpa or just a placeholder if not existing
                // Since this is a method call on a module usually, but here it might be functional.
                // If functional: F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)
                // We'll map to a custom helper or todo with clearer message if candle doesn't have it directly.
                // Candle has candle_nn::ops::softmax usually used.
                // Let's assume we map it to a custom function or just todo for now but CLEANLY.
                // actually candle-nn has it? No.
                // We'll emit a TODO but with the arguments.
                format!(
                    "todo!(\"scaled_dot_product_attention({}, {}, {}, ...)\")",
                    args.get(0).unwrap_or(&"".to_string()),
                    args.get(1).unwrap_or(&"".to_string()),
                    args.get(2).unwrap_or(&"".to_string())
                ),
                ReturnType::Tensor,
            ),
            "masked_fill_" => {
                // In-place masked_fill_: tensor.masked_fill_(mask, value)
                // Candle out-of-place: tensor.masked_fill(mask, value)
                let mask = &args[0];
                let value = &args[1];
                (
                    format!("{}.masked_fill(&{}, {})?", self_var, mask, value),
                    ReturnType::Tensor,
                )
            }
            _ => (
                format!("todo!(/* method: {} on {} */)", method, self_var),
                ReturnType::Tensor,
            ),
        }
    }

    fn infer_linear_dims(&self, meta: &LayerMeta) -> (usize, usize) {
        let in_f = meta.config.get("in_features").and_then(|v| v.as_u64());
        let out_f = meta.config.get("out_features").and_then(|v| v.as_u64());

        if let (Some(i), Some(o)) = (in_f, out_f) {
            return (i as usize, o as usize);
        }

        // Fallback to shapes if config is missing (common in ONNX or custom layers)
        let in_shape = meta
            .input_shapes
            .first()
            .and_then(|s| s.last())
            .copied()
            .unwrap_or(0);
        let out_shape = meta
            .output_shapes
            .first()
            .and_then(|s| s.last())
            .copied()
            .unwrap_or(0);

        (in_shape, out_shape)
    }

    fn generate_init(&self, layer_name: &str, meta: &LayerMeta) -> String {
        // Check GPT2 helper first
        if let Some(init) = gpt2::generate_init(layer_name, meta, &self.config.dims) {
            return init;
        }

        // Core types
        // NEW: Check for weight norm flag
        let is_weight_norm = meta
            .config
            .get("weight_norm")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);

        match meta.module_type.as_str() {
            "Linear" | "LoRACompatibleLinear" | "Conv1D" => {
                let (in_f_val, out_f_val) = self.infer_linear_dims(meta);
                let in_f = self.render_dim(in_f_val, "hidden_dim");
                let out_f = self.render_dim(out_f_val, "");
                let bias = meta.config["bias"].as_bool().unwrap_or(true);

                if is_weight_norm {
                    // Generate call to the new helper
                    return format!(
                        "pycandle_core::layers::load_weight_norm_linear(vb.pp(\"{}\"), {}, {}, {})?",
                        layer_name, in_f, out_f, bias
                    );
                }

                // Check for weight shape to detect transpose needs
                let needs_transpose = meta
                    .config
                    .get("weight_shape")
                    .and_then(|v| v.as_array())
                    .map(|arr| {
                        let dims: Vec<u64> = arr.iter().filter_map(|x| x.as_u64()).collect();
                        // PyTorch Linear stores (out, in), if we see (in, out) we need transpose
                        dims.len() == 2 && dims[0] == in_f_val as u64 && dims[1] == out_f_val as u64
                    })
                    .unwrap_or(false);

                if needs_transpose {
                    format!(
                        "{{ let w = vb.pp(\"{}\").get(({}, {}), \"weight\")?.t()?; \
                         let b = {}; candle_nn::Linear::new(w, b) }}",
                        layer_name,
                        in_f,
                        out_f,
                        if bias {
                            format!("Some(vb.pp(\"{}\").get({}, \"bias\")?)", layer_name, out_f)
                        } else {
                            "None".to_string()
                        }
                    )
                } else if bias {
                    format!(
                        "candle_nn::linear({}, {}, vb.pp(\"{}\"))?",
                        in_f, out_f, layer_name
                    )
                } else {
                    format!(
                        "candle_nn::linear_no_bias({}, {}, vb.pp(\"{}\"))?",
                        in_f, out_f, layer_name
                    )
                }
            }
            "Conv1d" => {
                let in_c = self.render_dim(
                    meta.config["in_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let out_c = self.render_dim(
                    meta.config["out_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let k = meta.config["kernel_size"].as_u64().unwrap_or(0);
                let s = meta.config["stride"].as_u64().unwrap_or(1);
                let p = meta.config["padding"].as_u64().unwrap_or(0);
                format!(
                    "candle_nn::conv1d({}, {}, {}, candle_nn::Conv1dConfig {{ stride: {}, padding: {}, ..Default::default() }}, vb.pp(\"{}\"))?",
                    in_c, out_c, k, s, p, layer_name
                )
            }
            "LayerNorm" => {
                let shape: Vec<usize> =
                    serde_json::from_value(meta.config["normalized_shape"].clone())
                        .unwrap_or_default();
                let eps = meta.config["eps"].as_f64().unwrap_or(1e-5);
                // Use a single value if shape is [N], otherwise use a Slice
                let shape_str = if shape.len() == 1 {
                    format!("{}", shape[0])
                } else {
                    format!("Shape::from(vec!{:?})", shape)
                };
                format!(
                    "candle_nn::layer_norm({}, candle_nn::LayerNormConfig {{ eps: {:.1e}, ..Default::default() }}, vb.pp(\"{}\"))?",
                    shape_str, eps, layer_name
                )
            }
            "Embedding" => {
                let n = self.render_dim(
                    meta.config["num_embeddings"].as_u64().unwrap_or(0) as usize,
                    "vocab_size",
                );
                let d = self.render_dim(
                    meta.config["embedding_dim"].as_u64().unwrap_or(0) as usize,
                    "hidden_dim",
                );
                format!(
                    "candle_nn::embedding({}, {}, vb.pp(\"{}\"))?",
                    n, d, layer_name
                )
            }
            // Activations - stateless
            "ReLU" => "ReLU".to_string(),
            "GELU" => "GELU".to_string(),
            "Sigmoid" => "Sigmoid".to_string(),
            "Tanh" => "Tanh".to_string(),
            // Activations - parameterized
            "ELU" => {
                let alpha = meta
                    .config
                    .get("alpha")
                    .and_then(|v| v.as_f64())
                    .unwrap_or(1.0);
                format!("ELU::new({})", alpha)
            }
            "LeakyReLU" => {
                let slope = meta
                    .config
                    .get("negative_slope")
                    .and_then(|v| v.as_f64())
                    .unwrap_or(0.01);
                format!("LeakyReLU::new({})", slope)
            }
            "Snake" => {
                let in_features = meta
                    .config
                    .get("in_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0);
                format!("Snake::load(vb.pp(\"{}\"), {})?", layer_name, in_features)
            }
            "BatchNorm1d" => {
                let num_features = meta
                    .config
                    .get("num_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as usize;
                format!(
                    "BatchNorm1d::load(vb.pp(\"{}\"), {})?",
                    layer_name, num_features
                )
            }
            "BatchNorm2d" => {
                let num_features = meta
                    .config
                    .get("num_features")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as usize;
                format!(
                    "BatchNorm2d::load(vb.pp(\"{}\"), {})?",
                    layer_name, num_features
                )
            }
            "LSTM" => {
                let input_size = self.render_dim(
                    meta.config
                        .get("input_size")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(0) as usize,
                    "hidden_dim",
                );
                let hidden_size = self.render_dim(
                    meta.config
                        .get("hidden_size")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(0) as usize,
                    "",
                );
                let num_layers = self.render_dim(
                    meta.config
                        .get("num_layers")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(1) as usize,
                    "n_layers",
                );
                format!(
                    "LSTM::load(vb.pp(\"{}\"), {}, {}, {})?",
                    layer_name, input_size, hidden_size, num_layers
                )
            }
            "CausalConv1d" => {
                let in_c = self.render_dim(
                    meta.config["in_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let out_c = self.render_dim(
                    meta.config["out_channels"].as_u64().unwrap_or(0) as usize,
                    "",
                );
                let k = meta.config["kernel_size"].as_u64().unwrap_or(0);
                let s = meta.config["stride"].as_u64().unwrap_or(1);
                let bias = meta.config["bias"].as_bool().unwrap_or(true);
                format!(
                    "CausalConv1d::load(vb.pp(\"{}\"), {}, {}, {}, {}, {})?",
                    layer_name, in_c, out_c, k, s, bias
                )
            }
            "Mish" => "Mish".to_string(),
            "SiLU" => "SiLU".to_string(),
            // GPT2 specific
            "NewGELUActivation" => "candle_nn::Activation::NewGelu".to_string(),
            "Dropout" => "Dropout::new()".to_string(),
            "Transpose" => {
                let d0 = meta.config["dim0"].as_u64().unwrap_or(1);
                let d1 = meta.config["dim1"].as_u64().unwrap_or(2);
                format!("Transpose::new({}, {})", d0, d1)
            }
            "SinusoidalPosEmb" => {
                let dim = meta.output_shapes[0][1];
                format!("SinusoidalPosEmb::new({})", dim)
            }
            _ => format!(
                "todo!(\"Implement initialization for {}\")",
                meta.module_type
            ),
        }
    }

    fn get_forward_return_type(&self) -> String {
        if !self.graph_nodes.is_empty() {
            for node in &self.graph_nodes {
                if node.op == "output" {
                    if let Some(arg0) = node.args.get(0) {
                        if let Some(arr) = arg0.as_array() {
                            if arr.len() > 1 {
                                let tensors = vec!["Tensor"; arr.len()];
                                return format!("Result<({})>", tensors.join(", "));
                            }
                        }
                    }
                }
            }
        }
        "Result<Tensor>".to_string()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;

    #[test]
    fn test_sanitize_name() {
        let codegen = Codegen::new(HashMap::new(), None);

        // PyTorch names
        assert_eq!(
            codegen.sanitize_name("encoder.layers.0"),
            "encoder_layers_0"
        );
        assert_eq!(codegen.sanitize_name("node_123"), "x_123");
        assert_eq!(codegen.sanitize_name("123_invalid"), "x_123_invalid");

        // ONNX names
        assert_eq!(codegen.sanitize_name("/layers/0/Gemm_output"), "gemm");
        assert_eq!(codegen.sanitize_name("/node_456"), "x_456");
        assert_eq!(codegen.sanitize_name("/Gather_1_output"), "gather_1");
    }
}
</file>

<file path="crates/pycandle/src/main.rs">
//! PyCandle CLI
//!
//! Command-line interface for PyTorch ‚Üí Candle porting.

mod dashboard;
mod init;
mod report;
mod test_gen;
mod todos;

use anyhow::{Context, Result};
use clap::{Parser, Subcommand};
use pycandle_core::LayerMeta;
use pycandle_core::codegen::Codegen;
use report::ReportGenerator;
use std::collections::HashMap;
use std::path::PathBuf;
use std::process::Command;

#[derive(Parser)]
#[command(name = "pycandle")]
#[command(about = "A tool for bit-perfect parity checking between PyTorch and Candle", long_about = None)]
struct Cli {
    /// Output in JSON format for agent consumption
    #[arg(long, global = true)]
    json: bool,

    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Record activations from a PyTorch model
    Record {
        /// Path to the Python script that defines and runs the model
        #[arg(short, long)]
        script: PathBuf,

        /// Project name for the trace
        #[arg(short, long)]
        name: String,

        /// Output directory for the trace and manifest
        #[arg(short, long, default_value = "pycandle_trace")]
        out: PathBuf,
    },
    /// Generate Candle code from a manifest
    Codegen {
        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output path for the generated Rust file or directory
        #[arg(short, long)]
        out: PathBuf,

        /// Name of the model struct to generate
        #[arg(long, default_value = "MyModel")]
        model: String,

        /// Analyze without generating code
        #[arg(long)]
        analyze_only: bool,

        /// Generate stateful code with KV-caching support
        #[arg(long)]
        stateful: bool,
    },
    /// Extract and manage TODO markers in generated code
    Todos {
        /// Path to generated Rust file or directory
        #[arg(short, long)]
        path: PathBuf,

        /// Just check if TODOs remain (exit code 1 if any)
        #[arg(long)]
        check: bool,
    },
    /// Generate an HTML coverage report
    Report {
        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output HTML file path
        #[arg(short, long, default_value = "pycandle_report.html")]
        out: PathBuf,
    },
    Weights {
        #[command(subcommand)]
        action: WeightActions,
    },
    /// Launch the TUI Parity Dashboard
    Dashboard {
        // Optional arguments if we want to pass filter to cargo test
        #[arg(last = true)]
        args: Vec<String>,
    },
    /// Initialize a new project with boilerplate
    Init {
        /// Optional project name
        #[arg(short, long)]
        name: Option<String>,
    },
    /// Generate automated parity test
    GenTest {
        /// Name of the model struct (must match generated code)
        #[arg(long, default_value = "MyModel")]
        model: String,

        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output path for the generated test file
        #[arg(short, long, default_value = "tests/parity.rs")]
        out: PathBuf,
    },
    /// Convert ONNX model to PyCandle manifest
    OnnxConvert {
        /// Path to the ONNX model file
        #[arg(short = 'i', long)]
        onnx: PathBuf,

        /// Project name
        #[arg(short, long)]
        name: String,

        /// Output directory for the manifest
        #[arg(short, long, default_value = "pycandle_trace")]
        out: PathBuf,
    },
}

#[derive(Subcommand)]
enum WeightActions {
    /// Surgically extract weights used in a manifest
    Extract {
        /// Path to PyTorch checkpoint (.bin, .pt, .safetensors)
        #[arg(short, long)]
        checkpoint: PathBuf,

        /// Path to the manifest JSON file
        #[arg(short, long)]
        manifest: PathBuf,

        /// Output .safetensors path
        #[arg(short, long)]
        out: PathBuf,

        /// Optional JSON mapping file for renaming
        #[arg(long)]
        map: Option<PathBuf>,
    },
    /// Rename keys in a safetensors file using a mapping
    Map {
        /// Input .safetensors file
        #[arg(short, long)]
        input: PathBuf,

        /// Output .safetensors file
        #[arg(short, long)]
        out: PathBuf,

        /// JSON mapping file
        #[arg(short, long)]
        map: PathBuf,
    },
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Record { script, name, out } => {
            println!(
                "üöÄ Recording trace for project '{}' using script '{:?}'...",
                name, script
            );

            let status = Command::new("uv")
                .arg("run")
                .arg("python")
                .arg(script)
                .spawn()
                .context("Failed to spawn uv run")?
                .wait()
                .context("Failed to wait for python process")?;

            if status.success() {
                println!("‚úÖ Recording complete. Files should be in {:?}", out);
            } else {
                eprintln!("‚ùå Recording failed.");
            }
        }
        Commands::Codegen {
            manifest: start_path,
            out: out_path,
            model,
            analyze_only,
            stateful,
        } => {
            // Find manifests: if directory, glob *.json, else use file
            let manifest_files = if start_path.is_dir() {
                std::fs::read_dir(&start_path)?
                    .filter_map(|entry| {
                        let path = entry.ok()?.path();
                        if path.extension()?.to_str()? == "json"
                            && path.file_name()?.to_str()?.ends_with("_manifest.json")
                        {
                            Some(path)
                        } else {
                            None
                        }
                    })
                    .collect::<Vec<_>>()
            } else {
                vec![start_path]
            };

            if manifest_files.is_empty() {
                eprintln!("‚ùå No manifest files found.");
                return Ok(());
            }

            for manifest_path in manifest_files {
                let manifest_content = std::fs::read_to_string(&manifest_path)
                    .with_context(|| format!("Failed to read manifest at {:?}", manifest_path))?;

                // Full manifest structure including optional graph
                #[derive(serde::Deserialize)]
                struct Manifest {
                    #[serde(flatten)]
                    layers: HashMap<String, serde_json::Value>,
                    #[serde(rename = "_graph_nodes")]
                    graph_nodes: Option<Vec<pycandle_core::codegen::GraphNode>>,
                    #[serde(rename = "_graph_code")]
                    graph_code: Option<String>,
                    #[serde(rename = "_symbolic_hints")]
                    symbolic_hints: Option<HashMap<String, usize>>,
                }

                let full_manifest: Manifest = serde_json::from_str(&manifest_content)
                    .context("Failed to parse manifest JSON")?;

                // Filter out internal keys starting with "_"
                let layers: HashMap<String, LayerMeta> = full_manifest
                    .layers
                    .into_iter()
                    .filter(|(k, _)| !k.starts_with('_'))
                    .map(|(k, v)| {
                        let meta: LayerMeta = serde_json::from_value(v)
                            .with_context(|| format!("Failed to parse LayerMeta for {}", k))?;
                        Ok((k, meta))
                    })
                    .collect::<Result<_>>()?;

                let mut generator =
                    Codegen::new(layers, full_manifest.symbolic_hints).with_stateful(stateful);
                if let Some(nodes) = full_manifest.graph_nodes {
                    generator = generator.with_graph(nodes);
                }

                if analyze_only || cli.json {
                    let analysis = generator.analyze();

                    if cli.json {
                        println!(
                            "{}",
                            serde_json::to_string_pretty(&analysis)
                                .context("Failed to serialize analysis")?
                        );
                    } else {
                        println!("üìä Analysis of {:?}:", manifest_path);
                        println!(
                            "  Supported: {}/{} ({:.1}%)",
                            analysis.supported, analysis.total, analysis.coverage_percent
                        );
                        println!("  Unsupported: {}", analysis.unsupported);
                        if !analysis.gaps.is_empty() {
                            println!("\n  Gaps:");
                            for gap in &analysis.gaps {
                                println!(
                                    "    - {}: {} occurrence(s) ‚Üí {}",
                                    gap.module_type, gap.count, gap.suggestion
                                );
                            }
                        }
                    }
                }

                if !analyze_only {
                    // Determine output path
                    let final_out = if out_path.is_dir() {
                        let stem = manifest_path.file_stem().unwrap().to_str().unwrap();
                        let name = stem.replace("_manifest", "");
                        out_path.join(format!("generated_{}.rs", name))
                    } else {
                        // If multiple manifests but one output file, this is ambiguous/wrong unless overwriting.
                        // We'll enforce directory output if input is directory.
                        out_path.clone()
                    };

                    println!(
                        "üèóÔ∏è Generating Candle code from manifest '{:?}'...",
                        manifest_path
                    );

                    // Use model name from CLI args for struct name.
                    // TODO: maybe derive struct name from manifest filename too?
                    let code = generator.generate_model_rs(&model);

                    std::fs::write(&final_out, code).with_context(|| {
                        format!("Failed to write generated code to {:?}", final_out)
                    })?;

                    println!("‚úÖ Code generated successfully at {:?}", final_out);
                }
            }
        }
        Commands::Todos { path, check } => {
            let files = if path.is_dir() {
                // simple recursion or flat for now? Let's just do one level or walkdir if needed.
                // For now, let's use fs::read_dir and filter .rs
                std::fs::read_dir(&path)?
                    .filter_map(|entry| {
                        let p = entry.ok()?.path();
                        if p.extension()?.to_str()? == "rs" {
                            Some(p)
                        } else {
                            None
                        }
                    })
                    .collect::<Vec<_>>()
            } else {
                vec![path]
            };

            let mut any_todos = false;

            for file_path in files {
                let content = std::fs::read_to_string(&file_path)
                    .with_context(|| format!("Failed to read file at {:?}", file_path))?;

                let todos = todos::extract_todos(&content);
                if !todos.is_empty() {
                    any_todos = true;
                }

                let report = todos::generate_report(file_path.to_str().unwrap_or("unknown"), todos);

                if cli.json {
                    println!(
                        "{}",
                        serde_json::to_string_pretty(&report)
                            .context("Failed to serialize report")?
                    );
                } else {
                    println!("üìã TODOs in {:?}:", file_path);
                    println!("   Total: {}", report.total);
                    if !report.by_type.is_empty() {
                        println!("\n   By type:");
                        let mut types: Vec<_> = report.by_type.iter().collect();
                        types.sort_by_key(|(_, count)| std::cmp::Reverse(*count));
                        for (t, c) in types {
                            println!("   - {}: {}", t, c);
                        }
                    }
                    if !report.todos.is_empty() {
                        println!("\n   Details:");
                        for todo in &report.todos {
                            println!(
                                "   L{}: {} ({}) ‚Üí {}",
                                todo.line, todo.field_name, todo.module_type, todo.suggestion
                            );
                        }
                    }
                }
            }

            if check && any_todos {
                std::process::exit(1);
            }
        }
        Commands::Report { manifest, out } => {
            let manifest_content = std::fs::read_to_string(&manifest)
                .with_context(|| format!("Failed to read manifest at {:?}", manifest))?;

            let manifest_data: HashMap<String, LayerMeta> =
                serde_json::from_str(&manifest_content).context("Failed to parse manifest JSON")?;

            let generator = ReportGenerator::new(manifest_data);
            let data = generator.analyze();
            let html = generator.generate_html(&data);

            std::fs::write(&out, html)
                .with_context(|| format!("Failed to write report to {:?}", out))?;

            println!("üìä Report generated: {:?}", out);
        }
        Commands::Weights { action } => match action {
            WeightActions::Extract {
                checkpoint,
                manifest,
                out,
                map,
            } => {
                println!("üî™ Performing surgical weight extraction...");
                let mut cmd = Command::new("uv");
                cmd.arg("run")
                    .arg("python")
                    .arg("py/weight_extractor.py")
                    .arg("--checkpoint")
                    .arg(&checkpoint)
                    .arg("--manifest")
                    .arg(&manifest)
                    .arg("--out")
                    .arg(&out);

                if let Some(m) = map {
                    cmd.arg("--map").arg(m);
                }

                let status = cmd.spawn()?.wait()?;
                if status.success() {
                    println!("‚úÖ Extraction complete: {:?}", out);
                } else {
                    anyhow::bail!("Weight extraction failed");
                }
            }
            WeightActions::Map { input, out, map } => {
                println!("üîÑ Renaming weights using map {:?}...", map);
                let map_content = std::fs::read_to_string(&map)?;
                let mapper = pycandle_core::WeightMapper::from_json(&map_content)?;

                let weights = candle_core::safetensors::load(&input, &candle_core::Device::Cpu)?;
                let mut renamed_weights = HashMap::new();
                for (name, tensor) in weights {
                    renamed_weights.insert(mapper.map_key(&name), tensor);
                }

                candle_core::safetensors::save(&renamed_weights, &out)?;
                println!("‚úÖ Renaming complete: {:?}", out);
            }
        },
        Commands::Dashboard { args } => {
            dashboard::run_dashboard(&args)?;
        }
        Commands::Init { name } => {
            init::run_init(name)?;
        }
        Commands::GenTest {
            model,
            manifest,
            out,
        } => {
            println!("üß™ Generating test harness for model '{}'...", model);
            let generator = test_gen::TestGenerator::new(model, manifest)?;
            let code = generator.generate_test_file();

            if let Some(parent) = out.parent() {
                std::fs::create_dir_all(parent)
                    .with_context(|| format!("Failed to create directory {:?}", parent))?;
            }

            std::fs::write(&out, code)
                .with_context(|| format!("Failed to write test file to {:?}", out))?;

            println!("‚úÖ Test generated at {:?}", out);
        }
        Commands::OnnxConvert { onnx, name, out } => {
            println!(
                "üì¶ Converting ONNX model '{:?}' to PyCandle manifest...",
                onnx
            );

            let status = Command::new("uv")
                .arg("run")
                .arg("--project")
                .arg("py")
                .env("PYTHONPATH", "py")
                .arg("python")
                .arg("py/onnx_to_fx.py")
                .arg("--onnx")
                .arg(&onnx)
                .arg("--name")
                .arg(&name)
                .arg("--out")
                .arg(&out)
                .spawn()
                .context("Failed to spawn uv run")?
                .wait()
                .context("Failed to wait for python process")?;

            if status.success() {
                println!("‚úÖ Conversion complete. Manifest saved in {:?}", out);
            } else {
                eprintln!("‚ùå Conversion failed.");
            }
        }
    }

    Ok(())
}
</file>

<file path="py/spy.py">
import os
import json
import torch
import torch.nn as nn
from safetensors.torch import save_file
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict
from collections import defaultdict

@dataclass
class LayerMeta:
    name: str
    module_type: str
    input_shapes: List[List[int]]
    output_shapes: List[List[int]]
    parameters: List[str]
    buffers: List[str]
    is_leaf: bool
    config: Dict[str, Any]

class GoldenRecorder:
    def __init__(self, output_dir: str = "pycandle_trace", keep_dtype: bool = False):
        self.output_dir = output_dir
        self.keep_dtype = keep_dtype
        self.records: Dict[str, torch.Tensor] = {}
        self.manifest: Dict[str, LayerMeta] = {}
        self.call_counts = defaultdict(int)
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def _get_module_config(self, module: nn.Module) -> Dict[str, Any]:
        cfg = {}
        
        # --- NEW BLOCK: Detect Weight Normalization ---
        is_weight_norm = False
        # Check PyTorch 2.0+ parametrizations
        if hasattr(module, 'parametrizations') and 'weight' in module.parametrizations:
             # Check if it's actually WeightNorm
             for p in module.parametrizations.weight:
                 if type(p).__name__ == "WeightNorm":
                     is_weight_norm = True
        # Check Legacy PyTorch weight_norm
        elif hasattr(module, 'weight_g') and hasattr(module, 'weight_v'):
            is_weight_norm = True
        
        if is_weight_norm:
            cfg['weight_norm'] = True
        # ---------------------------------------------

        if isinstance(module, nn.Linear):
            cfg = {
                "in_features": module.in_features,
                "out_features": module.out_features,
                "bias": module.bias is not None,
                # Record actual weight shape for transpose detection
                "weight_shape": list(module.weight.shape),  # [out, in] in PyTorch
            }
        elif isinstance(module, nn.Conv1d) or type(module).__name__ == "Conv1d":
            cfg = {
                "in_channels": module.in_channels,
                "out_channels": module.out_channels,
                "kernel_size": module.kernel_size[0] if isinstance(module.kernel_size, (list, tuple)) else module.kernel_size,
                "stride": module.stride[0] if isinstance(module.stride, (list, tuple)) else module.stride,
                "padding": module.padding[0] if isinstance(module.padding, (list, tuple)) else module.padding,
                "bias": module.bias is not None
            }
        # HF GPT2 often uses Conv1D
        elif type(module).__name__ == "Conv1D":
            cfg = {
                "in_features": module.weight.shape[0],
                "out_features": module.weight.shape[1],
                "bias": module.bias is not None,
                "weight_shape": list(module.weight.shape)
            }
        elif isinstance(module, nn.LayerNorm):
            cfg = {"normalized_shape": list(module.normalized_shape), "eps": module.eps}
        elif isinstance(module, nn.Embedding):
            cfg = {"num_embeddings": module.num_embeddings, "embedding_dim": module.embedding_dim}
        # Activation functions
        elif isinstance(module, nn.ELU):
            cfg = {"alpha": module.alpha}
        elif isinstance(module, nn.LeakyReLU):
            cfg = {"negative_slope": module.negative_slope}
        elif isinstance(module, nn.BatchNorm1d):
            cfg = {"num_features": module.num_features, "eps": module.eps}
        elif isinstance(module, nn.BatchNorm2d):
            cfg = {"num_features": module.num_features, "eps": module.eps}
        elif isinstance(module, nn.LSTM):
            cfg = {
                "input_size": module.input_size,
                "hidden_size": module.hidden_size,
                "num_layers": module.num_layers,
                "batch_first": module.batch_first,
                "bidirectional": module.bidirectional
            }
        # Snake activation (custom) - check for alpha parameter and in_features
        elif hasattr(module, 'alpha') and isinstance(getattr(module, 'alpha', None), torch.nn.Parameter):
            # Snake: alpha is a learnable parameter, extract in_features from its shape
            alpha = module.alpha
            if alpha.dim() >= 2:
                cfg = {"in_features": alpha.shape[1]}
            elif alpha.dim() == 1:
                cfg = {"in_features": alpha.shape[0]}
        # Custom Transpose
        elif type(module).__name__ == "Transpose":
             cfg = {
                "dim0": getattr(module, "dim0", 1),
                "dim1": getattr(module, "dim1", 2),
            }
        
        # GPT2 from HuggingFace transformers
        if hasattr(module, 'config') and hasattr(module.config, 'n_embd'):
            cfg['vocab_size'] = module.config.vocab_size
            cfg['n_positions'] = module.config.n_positions  # context_length
            cfg['n_embd'] = module.config.n_embd  # emb_dim
            cfg['n_head'] = module.config.n_head  # n_heads
            cfg['n_layer'] = module.config.n_layer  # n_layers
            cfg['resid_pdrop'] = module.config.resid_pdrop  # drop_rate
        
        # --- Universal Fallback ---
        # Capture all scalar/list attributes to support generic loading
        # This allows us to handle layers we haven't explicitly mapped yet
        base_attrs = dir(torch.nn.Module())
        for k in dir(module):
            if k.startswith('_') or k in base_attrs: continue
            try:
                v = getattr(module, k)
                if isinstance(v, (int, float, bool, str)):
                    if k not in cfg:
                        cfg[k] = v
                elif isinstance(v, (tuple, list)):
                    if all(isinstance(x, (int, float, bool, str)) for x in v):
                         if k not in cfg:
                            cfg[k] = v
            except:
                pass
        
        return cfg

    def _tensor_to_cpu(self, t: Any) -> Optional[torch.Tensor]:
        if isinstance(t, torch.Tensor):
            if t.device.type == 'meta':
                # Return placeholder or None
                return None
            t = t.detach().clone().contiguous().cpu()
            if not self.keep_dtype:
                t = t.float()
            return t
        return None

    def _extract_shapes(self, data: Any) -> List[List[int]]:
        shapes = []
        if isinstance(data, torch.Tensor):
            shapes.append(list(data.shape))
        elif isinstance(data, (tuple, list)):
            for item in data:
                shapes.extend(self._extract_shapes(item))
        return shapes

    def hook_factory(self, name: str):
        def hook(m, inp, out):
            call_idx = self.call_counts[name]
            self.call_counts[name] += 1
            
            trace_key = f"{name}.{call_idx}" if call_idx > 0 else name
            
            # Record Inputs
            if isinstance(inp, tuple):
                for i, x in enumerate(inp):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.in.{i}"] = cpu_x
            
            # Record Outputs
            if isinstance(out, torch.Tensor):
                self.records[f"{trace_key}.out.0"] = self._tensor_to_cpu(out)
            elif isinstance(out, (tuple, list)):
                for i, x in enumerate(out):
                    cpu_x = self._tensor_to_cpu(x)
                    if cpu_x is not None:
                        self.records[f"{trace_key}.out.{i}"] = cpu_x

            # Record Metadata
            self.manifest[trace_key] = LayerMeta(
                name=name,
                module_type=type(m).__name__,
                input_shapes=self._extract_shapes(inp),
                output_shapes=self._extract_shapes(out),
                parameters=[n for n, _ in m.named_parameters(recurse=False)],
                buffers=[n for n, _ in m.named_buffers(recurse=False)],
                is_leaf=len(list(m.children())) == 0,
                config=self._get_module_config(m)
            )
        return hook

    def record(self, model: nn.Module, *args, trace_fx: bool = False, fx_concrete_args: Optional[Dict[str, Any]] = None, **kwargs):
        model.eval()
        
        # Record model inputs
        for i, arg in enumerate(args):
            cpu_arg = self._tensor_to_cpu(arg)
            if cpu_arg is not None:
                self.records[f"model_input.{i}"] = cpu_arg

        hooks = []
        for name, module in model.named_modules():
            if name == "": continue 
            hooks.append(module.register_forward_hook(self.hook_factory(name)))
        
        try:
            with torch.no_grad():
                output = model(*args, **kwargs)
        finally:
            for h in hooks:
                h.remove()
        
        if trace_fx:
            self.trace_fx(model, concrete_args=fx_concrete_args, *args, **kwargs)
            
        return output

    def trace_fx(self, model: nn.Module, *example_inputs, concrete_args=None, **kwargs):
        """Use torch.fx to capture the computation graph."""
        import torch.fx as fx
        
        # Try to use transformers.utils.fx if available for HF models
        # This handles control flow and other HF-specific quirks better than vanilla fx
        try:
            from transformers.utils.fx import symbolic_trace as hf_symbolic_trace
            # Check if it's a transformers model (heuristic)
            is_hf = any(c.__module__.startswith("transformers") for c in model.__class__.__mro__)
            if is_hf:
                print("üïµÔ∏è Using transformers.utils.fx.symbolic_trace for robustness...")
                # HF tracer requires input_names usually, but let's try basic first
                # Note: HF tracer might expect 'input_names' kwarg if we want to be safe, 
                # but let's see if it works with just concrete_args.
                traced = hf_symbolic_trace(model, input_names=["input_ids"], disable_check=True)
            else:
                traced = fx.symbolic_trace(model, concrete_args=concrete_args)
        except ImportError:
            traced = fx.symbolic_trace(model, concrete_args=concrete_args)
        except Exception as e:
            print(f"‚ö†Ô∏è transformers trace failed ({e}), falling back to torch.fx...")
            traced = fx.symbolic_trace(model, concrete_args=concrete_args)
        
        def serialize_arg(arg):
            if isinstance(arg, (list, tuple)):
                return [serialize_arg(a) for a in arg]
            if hasattr(arg, "name"):
                return arg.name
            return str(arg)

        def serialize_target(target):
            if isinstance(target, str):
                return target
            if hasattr(target, "__module__") and hasattr(target, "__name__"):
                 # e.g. torch.arange, operator.getitem
                 return f"{target.__module__}.{target.__name__}"
            if hasattr(target, "__name__"):
                return target.__name__
            return str(target)

        graph_nodes = []
        for node in traced.graph.nodes:
            # We want to map these nodes to the modules or functions
            node_info = {
                "name": node.name,
                "op": node.op,
                "target": serialize_target(node.target),
                "args": [serialize_arg(arg) for arg in node.args],
            }
            
            if node.op == "call_module":
                try:
                    submod = traced.get_submodule(node.target)
                    node_info["module_type"] = type(submod).__name__
                except:
                    pass
            
            graph_nodes.append(node_info)
            
        self._fx_graph = {
            "graph_nodes": graph_nodes,
            "graph_code": traced.code
        }
        return self._fx_graph

    def save(self, project_name: str, use_fx: bool = False, hints: Optional[Dict[str, int]] = None):
        tensor_path = os.path.join(self.output_dir, f"{project_name}_trace.safetensors")
        # Filter out None values (meta placeholders)
        real_records = {k: v for k, v in self.records.items() if v is not None}
        if real_records:
            save_file(real_records, tensor_path)
        else:
            print("‚ö†Ô∏è No real tensors recorded (Meta-only mode)")
        
        manifest_path = os.path.join(self.output_dir, f"{project_name}_manifest.json")
        manifest_data = {k: asdict(v) for k, v in self.manifest.items()}
        
        if use_fx and hasattr(self, '_fx_graph'):
            manifest_data["_graph_nodes"] = self._fx_graph["graph_nodes"]
            manifest_data["_graph_code"] = self._fx_graph["graph_code"]

        if hints:
            manifest_data["_symbolic_hints"] = hints

        with open(manifest_path, "w") as f:
            json.dump(manifest_data, f, indent=4)
        print(f"‚úÖ Trace and Manifest saved for {project_name}")
</file>

<file path="README.md">
# PyCandle

**Automated PyTorch ‚Üí Candle porting with layer-wise parity verification.**

PyCandle captures activation traces from PyTorch models to generate verified Rust Candle code. Acting as "Chrome DevTools" for neural networks, it provides full transparency into the internal state of complex models, ensuring seamless parity across Python and Rust.

## Quick Start

### 1. Record a PyTorch model

```python
# your_model_script.py
import sys
sys.path.insert(0, "path/to/pycandle/py")

import torch
from spy import GoldenRecorder

# Your PyTorch model
model = MyModel()
model.eval()

# Create dummy input
x = torch.randn(1, 128)

# Record
recorder = GoldenRecorder(output_dir="traces")
recorder.record(model, x)
recorder.save("my_model")
```

### 2. Generate Candle code

```bash
cargo run -p pycandle -- codegen \
    --manifest traces/my_model_manifest.json \
    --out generated_my_model.rs \
    --model MyModel
```

### 3. Use generated code with parity checking

```rust
use pycandle_core::{PyChecker, py_check};

// Load golden records for verification
let checker = PyChecker::load("my_model", "traces/", &device)?;

// Use the generated model
let model = MyModel::load(vb, Some(checker))?;
let output = model.forward(&input)?;  // py_check! runs at each layer
```

## CLI Commands

### `pycandle record`
Run a Python script that uses `GoldenRecorder`.

```bash
pycandle record --script model_script.py --name my_model --out traces/
```

### `pycandle codegen`
Generate Candle Rust code from a manifest.

```bash
pycandle codegen --manifest traces/manifest.json --out generated.rs --model ModelName
```

**Flags:**
- `--analyze-only` - Show analysis without generating code
- `--json` - Output as JSON (works with all commands)

**Analysis mode example:**
```bash
# Human-readable analysis
pycandle codegen --manifest m.json --out NUL --analyze-only

# JSON output for scripting
pycandle codegen --manifest m.json --out NUL --analyze-only --json
```

### `pycandle todos`
Extract and manage TODO markers in generated code.

```bash
# List all TODOs with suggestions
pycandle todos --file generated_model.rs

# JSON output
pycandle todos --file generated_model.rs --json

# Check mode (exit code 1 if TODOs remain)
pycandle todos --file generated_model.rs --check
```

**Agent workflow example:**
```bash
# 1. Generate code
pycandle codegen --manifest m.json --out model.rs --model MyModel

# 2. Check for TODOs
if ! pycandle todos --file model.rs --check; then
    # 3. Get gaps as JSON and implement
    pycandle todos --file model.rs --json | jq '.by_type'
fi
```

## Python Spy API

```python
from spy import GoldenRecorder

recorder = GoldenRecorder(output_dir="traces")
recorder.record(model, *inputs, **kwargs)  # Runs forward pass with hooks
recorder.save("project_name")  # Saves .safetensors + _manifest.json
```

**Output files:**
- `{name}_trace.safetensors` - Activation tensors for each layer
- `{name}_manifest.json` - Module metadata (types, shapes, configs)

### Advanced: Resolving Symbolic Ambiguity (Hints)

If your model has multiple dimensions with the same size (e.g., `hidden_dim=1024` and `context_length=1024`), the symbolic propagator might pick the wrong one. You can resolve this by passing `hints` to `recorder.save()`:

```python
recorder.save("my_model", hints={
    "vocab_size": 50257,
    "hidden_dim": 768,
    "context_length": 1024
})
```

The codegen will prioritize these hints when generating the `Config` struct and mapping layer dimensions.

## Rust Verification API

```rust
use pycandle_core::{PyChecker, py_check};

// Load checker
let checker = PyChecker::load("model_name", "traces/", &device)?;

// Verify a tensor against golden record
let result = checker.verify("layer_name", &tensor)?;
println!("MSE: {}", result.mse);

// Or use the macro (embedded in generated code)
py_check!(checker, "layer_name", &tensor);
```

## Generated Code Structure

```rust
pub struct Config {
    pub vocab_size: usize,
    pub hidden_dim: usize,
}

pub struct MyModel {
    pub linear1: Linear,
    pub linear2: Linear,
    pub checker: Option<PyChecker>,
}

impl MyModel {
    pub fn load(cfg: Config, vb: VarBuilder, checker: Option<PyChecker>) -> Result<Self> {
        let linear1 = candle_nn::linear(cfg.hidden_dim, 256, vb.pp("linear1"))?;
        let linear2 = candle_nn::linear(256, cfg.vocab_size, vb.pp("linear2"))?;
        Ok(Self { linear1, linear2, checker })
    }

    pub fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let mut x = xs.clone();
        x = self.linear1.forward(&x)?;
        py_check!(self.checker, "linear1", &x);
        x = self.linear2.forward(&x)?;
        py_check!(self.checker, "linear2", &x);
        Ok(x)
    }
}
```

## Supported Module Types

| PyTorch | Candle | Status |
|---------|--------|--------|
| `nn.Linear` | `candle_nn::linear` | ‚úÖ Auto (with smart transpose) |
| `nn.Conv1d` | `candle_nn::conv1d` | ‚úÖ Auto |
| `nn.Embedding` | `candle_nn::embedding` | ‚úÖ Auto |
| `nn.LayerNorm` | `candle_nn::layer_norm` | ‚úÖ Auto |
| `nn.BatchNorm1d` | `BatchNorm1d` | ‚úÖ Auto |
| `nn.BatchNorm2d` | `BatchNorm2d` | ‚úÖ Auto |
| `nn.LSTM` | `LSTM` | ‚úÖ Auto |
| `nn.ReLU/GELU/Sigmoid/Tanh` | Activations | ‚úÖ Auto |
| `nn.ELU/LeakyReLU` | Parameterized activations | ‚úÖ Auto |
| `Snake` (BigVGAN) | `Snake` | ‚úÖ Auto |
| Custom modules | - | ‚ö†Ô∏è TODO marker |

## Workspace Structure

```
pycandle/
‚îú‚îÄ‚îÄ Cargo.toml              # Workspace root
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ pycandle/           # CLI binary
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/main.rs
‚îÇ   ‚îú‚îÄ‚îÄ pycandle-core/      # Library (PyChecker, layers, codegen)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lib.rs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ checker.rs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ layers.rs
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ codegen/
‚îÇ   ‚îî‚îÄ‚îÄ pycandle-audio/     # Audio ops (STFT, padding)
‚îÇ       ‚îî‚îÄ‚îÄ src/lib.rs
‚îî‚îÄ‚îÄ py/
    ‚îî‚îÄ‚îÄ spy.py              # GoldenRecorder
```

**Using as a library:**
```toml
[dependencies]
pycandle-core = { git = "https://github.com/user/pycandle" }
# Optional audio support:
pycandle-audio = { git = "https://github.com/user/pycandle" }
```

---

## Roadmap

PyCandle has evolved into a high-fidelity transpilation framework. The following items track the transition from v0.1 to a production-grade v1.0.

### üîÑ DAG Resolver (torch.fx Tracing)
**Status:** Complete ‚úÖ

Handle non-sequential models with skip connections and branches:
- Use `torch.fx` to trace computation graphs automatically.
- Generate named variables based on FX graph nodes (e.g., `let x_conv1 = ...`).
- Automatic residual detection and mapping of functional ops (`add`, `cat`, `mul`).
- Mapping of `operator.getitem` to Candle `.i()` for complex slicing logic.

### üìê Symbolic Shape Propagation
**Status:** Complete ‚úÖ

Generate `Config` structs instead of hardcoded dimensions to decouple model logic from specific input sizes:
- Automatic detection of `vocab_size`, `hidden_dim`, and `context_length`.
- Config-driven initialization in the generated Rust `load` functions.

### üìä Visual Drift Analysis (Mechanistic Diagnostics)
**Status:** Complete ‚úÖ

Enhanced diagnostics for numerical drift using real-time verification data:
- **D3.js Coverage Report:** Standalone HTML report with MSE/Cosine Similarity charts.
- **Divergence Detection:** Automatic identification of the "Point of Failure" where math starts to drift.
- **TUI Dashboard:** Real-time terminal dashboard with 8x8 error heatmaps for instant visual feedback.

### üéµ Audio-Specific Ops (pycandle-audio)
**Status:** Complete ‚úÖ

Bit-perfect PyTorch parity for audio preprocessing and specialized layers:
- **MelSpectrogram:** Full parity with `torchaudio` (including Slaney-scale area normalization).
- **STFT/iSTFT:** High-precision CPU-based transforms using `realfft`.
- **Specialized Layers:** Native support for `Snake` (BigVGAN), `CausalConv1d`, and `Mish`.

### üî¨ Interactive Debugger (Lock-Step)
**Status:** Complete ‚úÖ

Automated post-mortem tools for failed parity checks:
- **Snippet Generation:** Automatically saves `.safetensors` containing the failing Rust tensor and the Golden reference.
- **Python Debug Scripts:** Generates a ready-to-run `.py` script that loads the failure snippet into Matplotlib for visual histogram/heatmap comparison.

### üì¶ Surgical Weight Management
**Status:** Complete ‚úÖ

Tools to handle the "Integration Gap" between PyTorch checkpoints and Rust structs:
- **Checkpoint Mapper:** Regex-based renaming engine to map PyTorch keys to Rust fields.
- **Meta-Extractor:** CLI tool to surgically extract only the weights used in a manifest, significantly reducing checkpoint size.

### üß™ Automated Test Generation
**Status:** Complete ‚úÖ

Eliminate manual test writing by generating the full Rust test harness:
- **Auto-Test CLI:** `pycandle gen-test` to generate `tests/parity.rs` automatically.
- **Data-Driven Harness:** The test will automatically load the input/output tensors from the recorded trace and run the verification loop.

### üìê Symbolic Ambiguity Hints
**Status:** Complete ‚úÖ

Refining the symbolic propagator for complex models:
- **Hint System:** Allow users to provide a `hints.json` to resolve ambiguous dimensions (e.g., when `hidden_dim` and `context_length` are both 1024).
- **Manual Overrides:** Use the Python `recorder.save(..., hints={"vocab_size": 50000})` to guide the codegen when heuristics fail.

### üß© Multi-Input & Complex Slicing
**Status:** Complete ‚úÖ

- **Multi-Input Support:** Models with multiple placeholders (e.g., TTS models) correctly generate `forward(&self, xs0: &Tensor, xs1: &Tensor, ...)` signatures.
- **Robust Slicing:** Improved handling of complex `torch.fx` slicing nodes, mapping `operator.getitem` to Candle's `.i()` with support for multi-dimensional ranges (e.g., `x.i((.., ..-1))?`).

### üìâ Quantization Parity (GGUF/AWQ)
**Status:** Complete ‚úÖ

Extending the verification engine to quantized models:
- **Quantization Drift Tracking:** Measure MSE drift introduced specifically by `Q4_0`, `Q8_0`, or `AWQ` compared to the `f32` Golden Record.
- **Parity-Aware Quantization:** Identify which specific layers are most sensitive to quantization to help guide mixed-precision strategies.

---

### Summary of the "Powerful" PyCandle Vision:
1.  **Python Spy:** Captures Graph (FX) + Config + Activations + Weights.
2.  **Transpiler:** Converts FX Graph to idiomatic Rust DAG (with residuals).
3.  **Verifiable Crate:** Generated code with `py_check!` macros that "lights up" green as you implement layers.
4.  **Diagnostics:** A visual report and Python debug scripts showing exactly where the "Math Leak" is happening.
---
### üåê Universal ONNX Transpilation
**Status: Prototype (via `onnx2torch`) üõ†Ô∏è**

- **Bridge Strategy:** Automatically converts ONNX models to PyTorch in-memory, then traces them with `torch.fx` to generate idiomatic Rust.
- **CLI Integration:** `pycandle onnx-convert --onnx model.onnx --name my_model` handles the conversion pipeline automatically.
- **Dynamic Shape Inference:** Automatically detects input dimensions from the ONNX graph definition, defaulting dynamic axes to `1`.

### Technical Challenges to Watch For:
1.  **The "Opset" Nightmare:** ONNX has many versions (Opsets). You‚Äôll need to focus on the most common ones (Opset 17+).
2.  **Naming Conventions:** ONNX often renames layers to generic IDs (like `node_1`, `node_2`). v1.1 will include a "Sanitizer" pass in `codegen/mod.rs` to keep generated Rust code readable.
3.  **Complex Ops:** Some ONNX ops (like `EinsteinSum` or complex `Loop` nodes) are very hard to map to Candle.

### Should you do it?
**Yes, but as an alternative input.** 
Keep the `torch.fx` path as the "Primary" because it produces the most readable, idiomatic Rust code. Use ONNX as the "Emergency/Universal" path for when the original source code isn't available.

**PyCandle would then be:**
*   **Input:** PyTorch Code OR ONNX File.
*   **Process:** Trace + Analyze + Codegen.
*   **Output:** Verified, Production-Grade Rust.

---

## License

MIT
</file>

</files>
